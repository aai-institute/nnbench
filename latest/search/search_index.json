{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to nnbench, a framework for reproducibly benchmarking machine learning models. The main goals of this project are portable and customizable benchmarking for ML models, and easy integration into existing ML pipelines.</p> <p>Highlights:</p> <ul> <li>Easy definition, bookkeeping and organization of machine learning benchmarks,</li> <li>Enriching benchmark results with context to properly track and annotate results,</li> <li>Streaming results to a variety of data sinks.</li> </ul> <p>Quickstart</p><p>Step-by-step installation and first operations</p> <p>Examples</p><p>Examples on how to use nnbench</p> <p>API Reference</p><p>Full documentation of the Python API</p> <p>User Guide</p><p>Solving specific tasks with nnbench</p> <p>Contributing</p><p>How to contribute to the project</p>"},{"location":"CONTRIBUTING/","title":"Contributing to nnbench","text":"<p>Thank you for your interest in contributing to this project!</p> <p>We appreciate issue reports, pull requests for code and documentation, as well as any project-related communication through GitHub Discussions.</p>"},{"location":"CONTRIBUTING/#getting-started","title":"Getting Started","text":"<p>To get started with development, you can follow these steps:</p> <ol> <li> <p>Clone this repository:</p> <pre><code>git clone https://github.com/aai-institute/nnbench.git\n</code></pre> </li> <li> <p>Navigate to the directory and install the development dependencies into a virtual environment, e.g. using <code>uv</code>:</p> <pre><code>cd nnbench\nuv venv --seed -p 3.11\nsource .venv/bin/activate\n</code></pre> </li> <li> <p>After making your changes, verify they adhere to our Python code style by running <code>pre-commit</code>:</p> <pre><code>uvx pre-commit run --all-files --verbose --show-diff-on-failure\n</code></pre> <p>You can also set up Git hooks through <code>pre-commit</code> to perform these checks automatically:</p> <pre><code>pre-commit install\n</code></pre> </li> <li> <p>To run the tests, just invoke <code>pytest</code> from the package root directory:     <pre><code>uv run pytest -s\n</code></pre></p> </li> </ol>"},{"location":"CONTRIBUTING/#updating-dependencies","title":"Updating dependencies","text":"<p>Dependencies should stay locked for as long as possible, ideally for a whole release. If you have to update a dependency during development, you should do the following:</p> <ol> <li>If it is a core dependency needed for the package, add it to the <code>dependencies</code> section in the <code>pyproject.toml</code> via <code>uv add &lt;dep&gt;</code>.</li> <li>In case of a development dependency, add it to the <code>dev</code> section of the <code>project.dependency-groups</code> table instead (<code>uv add --group dev &lt;dep&gt;</code>).</li> <li>Dependencies needed for documentation generation are found in the <code>docs</code> sections of <code>project.dependency-groups</code> (<code>uv add --group docs &lt;dep&gt;</code>).</li> </ol> <p>After adding the dependency in either of these sections, use <code>uv lock</code> to pin all dependencies again:</p> <pre><code>uv lock\n</code></pre> <p>Important</p> <p>Since the official development version is Python 3.11, please run the above commands in a virtual environment with Python 3.11.</p>"},{"location":"quickstart/","title":"Quickstart","text":"<p>Welcome! This quickstart guide will convey the basics needed to use nnbench. You will define a benchmark, initialize a runner and reporter, and execute the benchmark, obtaining the results in the console in tabular format.</p>"},{"location":"quickstart/#a-short-scikit-learn-model-benchmark","title":"A short scikit-learn model benchmark","text":"<p>In the following simple example, we put the training and benchmarking logic in the same file. For more complex workloads, we recommend structuring your code into multiple files to improve project organization, similarly to unit tests. Check the user guide on structuring your benchmarks for inspiration.</p> <pre><code>from sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\ndata = load_iris()\nX, y = data.data, data.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\n</code></pre> <p>To benchmark your model, you encapsulate the benchmark code into a function and apply the <code>@nnbench.benchmark</code> decorator.  This marks the function for collection to our benchmark runner later.</p> <pre><code>import nnbench\nimport numpy as np\nfrom sklearn import base, metrics\n\n\n@nnbench.benchmark\ndef accuracy(model: base.BaseEstimator, X_test: np.ndarray, y_test: np.ndarray) -&gt; float:\n    y_pred = model.predict(X_test)\n    accuracy = metrics.accuracy_score(y_test, y_pred)\n    return accuracy\n</code></pre> <p>Now we can instantiate a benchmark runner to collect and run the accuracy benchmark. Then, using the <code>ConsoleReporter</code> we report the resulting accuracy metric by printing it to the terminal in a table.</p> <pre><code>import nnbench\nfrom nnbench.reporter import ConsoleReporter\n\nbenchmarks = nnbench.collect(\"__main__\")\nreporter = ConsoleReporter()\n# To collect in the current file, pass \"__main__\" as module name.\nrecord = nnbench.run(benchmarks, params={\"model\": model, \"X_test\": X_test, \"y_test\": y_test})\nreporter.write(record)\n</code></pre> <p>The resulting output might look like this:</p> <pre><code>python benchmarks.py\n\nname         value\n--------  --------\naccuracy  0.933333\n</code></pre>"},{"location":"cli/","title":"Command-line interface (CLI)","text":"<p>This subdirectory contains resources on how to run ML benchmarks in command line mode with the <code>nnbench</code> CLI.</p>"},{"location":"cli/cli/","title":"<code>nnbench</code> command reference","text":"<p>While you can always use nnbench to directly run your benchmarks in your Python code, for example as part of a workflow, there is also the option of running benchmarks from the command line. This way of using nnbench is especially useful for integrating into a machine learning pipeline as part of a continuous training/delivery scenario.</p>"},{"location":"cli/cli/#general-options","title":"General options","text":"<p>The <code>nnbench</code> CLI has the following top-level options:</p> <pre><code>$ nnbench\nusage: nnbench [-h] [--version] [--log-level &lt;level&gt;]  ...\n\noptions:\n  -h, --help           show this help message and exit\n  --version            show program's version number and exit\n  --log-level &lt;level&gt;  Log level to use for the nnbench package, defaults to NOTSET (no logging).\n\nAvailable commands:\n\n    run                Run a benchmark workload.\n    compare            Compare results from multiple benchmark runs.\n</code></pre> <p>Supported log levels are <code>\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\"</code>, and <code>\"CRITICAL\"</code>.</p>"},{"location":"cli/cli/#running-benchmark-workloads-on-the-command-line","title":"Running benchmark workloads on the command line","text":"<p>This is the responsibility of the <code>nnbench run</code> subcommand.</p> <pre><code>$ nnbench run -h                                                                         \nusage: nnbench run [-h] [-n &lt;name&gt;] [-j &lt;N&gt;] [--context &lt;key=value&gt;] [-t &lt;tag&gt;] [-o &lt;file&gt;] [--jsonifier &lt;classpath&gt;] [&lt;benchmarks&gt;]\n\npositional arguments:\n  &lt;benchmarks&gt;          A Python file or directory of files containing benchmarks to run.\n\noptions:\n  -h, --help            show this help message and exit\n  -n, --name &lt;name&gt;     A name to assign to the benchmark run, for example for record keeping in a database.\n  -j &lt;N&gt;                Number of processes to use for running benchmarks in parallel, default: -1 (no parallelism)\n  --context &lt;key=value&gt;\n                        Additional context values giving information about the benchmark run.\n  -t, --tag &lt;tag&gt;       Only run benchmarks marked with one or more given tag(s).\n  -o, --output-file &lt;file&gt;\n                        File or stream to write results to, defaults to stdout.\n  --jsonifier &lt;classpath&gt;\n                        Function to create a JSON representation of input parameters with, helping make runs reproducible.\n</code></pre> <p>To run a benchmark workload contained in a single <code>benchmarks.py</code> file, you would run <code>nnbench run benchmarks.py</code>. For tips on how to structure and annotate your benchmarks, refer to the organization guide.</p> <p>For injecting context values on the command line, you need to give the key-value pair explicitly by passing the <code>--context</code> switch. For example, to look up and persist the <code>pyyaml</code> version in the current environment, you could run the following:</p> <pre><code>nnbench run .sandbox/example.py --context=pyyaml=`python3 -c \"from importlib.metadata import version; print(version('pyyaml'))\"`\n</code></pre> <p>Tip</p> <p>Both <code>--context</code> and <code>--tag</code> are appending options, so you can pass multiple context values and multiple tags per run.</p> <p>Tip</p> <p>For more complex calculations of context values, it is recommended to register a custom context provider in your pyproject.toml file. An introductory example can be found in the nnbench CLI configuration guide.</p>"},{"location":"cli/cli/#comparing-results-across-multiple-benchmark-runs","title":"Comparing results across multiple benchmark runs","text":"<p>To create a comparison table between multiple benchmark runs, use the <code>nnbench compare</code> command.</p> <pre><code>$ nnbench compare -h                                            \nusage: nnbench compare [-h] [-P &lt;name&gt;] [-C &lt;name&gt;] [-E &lt;name&gt;] records [records ...]\n\npositional arguments:\n  records               Records to compare results for. Can be given as local files or remote URIs.\n\noptions:\n  -h, --help            show this help message and exit\n  -P, --include-parameter &lt;name&gt;\n                        Names of input parameters to display in the comparison table.\n  -C, --include-context &lt;name&gt;\n                        Context values to display in the comparison table. Use dotted syntax for nested context values.\n  -E, --extra-column &lt;name&gt;\n                        Additional record data to display in the comparison table.\n</code></pre> <p>Supposing we have the following records from previous runs, for a benchmark <code>add(a,b)</code> that adds two integers:</p> <pre><code>// Pretty-printed JSON, obtained as &lt;record1.json | jq\n{\n  \"run\": \"nnbench-3ff188b4\",\n  \"context\": {\n    \"foo\": \"bar\"\n  },\n  \"benchmarks\": [\n    {\n      \"name\": \"add\",\n      \"function\": \"add\",\n      \"description\": \"\",\n      \"date\": \"2024-12-02T17:41:16\",\n      \"error_occurred\": false,\n      \"error_message\": \"\",\n      \"parameters\": {\n        \"a\": 200,\n        \"b\": 100\n      },\n      \"value\": 300,\n      \"time_ns\": 1291\n    }\n  ]\n}\n</code></pre> <p>and</p> <pre><code>// &lt;record2.json | jq\n{\n  \"run\": \"nnbench-5cbb85f8\",\n  \"context\": {\n    \"foo\": \"baz\"\n  },\n  \"benchmarks\": [\n    {\n      \"name\": \"add\",\n      \"function\": \"add\",\n      \"description\": \"\",\n      \"date\": \"2024-12-02T17:42:04\",\n      \"error_occurred\": false,\n      \"error_message\": \"\",\n      \"parameters\": {\n        \"a\": 200,\n        \"b\": 100\n      },\n      \"value\": 300,\n      \"time_ns\": 1792\n    }\n  ]\n},\n</code></pre> <p>we can compare them in a table view by running <code>nnnbench compare record1.json record2.json</code>:</p> <pre><code>$ nnbench compare record1.json record2.json\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Benchmark run    \u2503 add \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 nnbench-3ff188b4 \u2502 300 \u2502\n\u2502 nnbench-5cbb85f8 \u2502 300 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>To include benchmark parameter values in the table, use the <code>-P</code> switch (you can supply this multiple times to include multiple parameters). For example, to see which values were used for <code>a</code> and <code>b</code> in our <code>add(a, b)</code> benchmark above, we supply <code>-P a</code> and <code>-P b</code>:</p> <pre><code>$ nnbench compare record1.json record2.json -P a -P b\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Benchmark run    \u2503 add \u2503 Params-&gt;a \u2503 Params-&gt;b \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 nnbench-3ff188b4 \u2502 300 \u2502 200       \u2502 100       \u2502\n\u2502 nnbench-5cbb85f8 \u2502 300 \u2502 200       \u2502 100       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>To include context values in the table - in our case, we might want to display the <code>foo</code> value - use the <code>-C</code> switch (this is also appending, same as <code>-P</code>):</p> <pre><code>$ nnbench compare record1.json record2.json -P a -P b -C foo\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Benchmark run    \u2503 add \u2503 Params-&gt;a \u2503 Params-&gt;b \u2503 foo \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 nnbench-3ff188b4 \u2502 300 \u2502 200       \u2502 100       \u2502 bar \u2502\n\u2502 nnbench-5cbb85f8 \u2502 300 \u2502 200       \u2502 100       \u2502 baz \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cli/fixtures/","title":"Using fixtures to supply parameters to benchmarks","text":"<p>One of the main problems when running <code>nnbench</code> on the command line is how to supply parameters. Default values for benchmarks are one solution, but that does not scale well, and requires frequent code changes when values change.</p> <p>Instead, nnbench borrows a bit of pytest's fixture concept to source parameters from special marker files, named <code>conf.py</code> in reference to pytest's <code>conftest.py</code>.</p>"},{"location":"cli/fixtures/#how-to-define-fixture-values-for-benchmarks","title":"How to define fixture values for benchmarks","text":"<p>Suppose you have a benchmark defined in a single file, <code>metrics.py</code>:</p> <pre><code># metrics.py\nimport nnbench\n\n\n@nnbench.benchmark\ndef accuracy(model, data):\n    ...\n</code></pre> <p>To supply <code>model</code> and <code>data</code> to the benchmark, define both values as return values of similarly named functions in a <code>conf.py</code> file in the same directory. The layout of your benchmark directory should look like this:</p> <pre><code>\ud83d\udcc2 benchmarks\n\u2523\u2501\u2501 conf.py\n\u2523\u2501\u2501 metrics.py\n\u2523\u2501\u2501 ...\n</code></pre> <p>Inside your <code>conf.py</code> file, you might define your values as shown below. Note that currently, all fixtures must be raw Python callables, and their names must match input values of benchmarks exactly.</p> <pre><code># benchmarks/conf.py\ndef model():\n    return MyModel()\n\n\ndef data():\n    return TestDataset.load(\"path/to/my/dataset\")\n</code></pre> <p>Then, nnbench will discover and auto-use these values when running this benchmark from the command line:</p> <pre><code>$ nnbench run benchmarks.py \n</code></pre> <p>Warning</p> <p>Benchmarks with default values for their arguments will unconditionally use those defaults over potential fixtures. That is, for a benchmark <code>def add(a: int, b: int = 1)</code>, only the named parameter <code>a</code> will be resolved.</p>"},{"location":"cli/fixtures/#fixtures-with-inputs","title":"Fixtures with inputs","text":"<p>Like in pytest, fixtures can consume inputs. However, in nnbench, fixtures can consume other inputs by name only within the same module scope, i.e. members within the same <code>conf.py</code>.</p> <pre><code># conf.py\n\n# Revisiting the above example, we could also write the following:\ndef path() -&gt; str:\n    return \"path/to/my/dataset\"\n\n\ndef data(path):\n    return TestDataset.load(path)\n\n# ... but not this, since `config` is not a member of the conf.py module:\ndef model(config):\n    return MyModel.load(config)\n</code></pre> <p>Warning</p> <p>nnbench fixtures cannot have cycles in them - two fixtures may never depend on each other.</p>"},{"location":"cli/fixtures/#hierarchical-confpy-files","title":"Hierarchical <code>conf.py</code> files","text":"<p>nnbench also supports sourcing fixtures from different levels in a directory hierarchy. Suppose we have a benchmark directory layout like this:</p> <pre><code>\ud83d\udcc2 benchmarks\n\u2523\u2501\u2501 \ud83d\udcc2 nested\n\u2503   \u2523\u2501\u2501 conf.py\n\u2503   \u2517\u2501\u2501 model.py\n\u2523\u2501\u2501 base.py\n\u2517\u2501\u2501 conf.py\n</code></pre> <p>Let's assume that the benchmarks in <code>nested/model.py</code> consume some fixture values specific to them, and reuse some top-level fixtures as well.</p> <pre><code># benchmarks/conf.py\n\ndef path() -&gt; str:\n    return \"path/to/my/dataset\"\n\n\ndef data(path: str):\n    \"\"\"Test dataset, to be reused by all benchmarks.\"\"\"\n    return TestDataset.load(path)\n\n# -------------------------\n# benchmarks/nested/conf.py\n# -------------------------\n\ndef model():\n    \"\"\"Model, needed only by the nested benchmarks.\"\"\"\n    return MyModel.load()\n</code></pre> <p>If we have a benchmark in <code>benchmarks/nested/model.py</code> defined like this:</p> <pre><code># benchmarks/nested/model.py\n\ndef accuracy(model, data):\n    ...\n</code></pre> <p>Now nnbench will source the <code>model</code> fixture from <code>benchmarks/nested/conf.py</code> and fall back to the top-level <code>benchmarks/conf.py</code> to obtain <code>data</code>.</p> <p>Info</p> <p>Just like pytest, nnbench collects fixture values bottom-up, starting with the benchmark file's parent directory.</p> <p>For example, if the <code>benchmarks/nested/conf.py</code> above also defined a <code>data</code> fixture, the <code>accuracy</code> benchmark would use that instead.</p>"},{"location":"cli/pyproject/","title":"Configuring the CLI experience in <code>pyproject.toml</code>","text":"<p>To create your custom CLI profile for nnbench, you can set certain options directly in your <code>pyproject.toml</code> file. Like other tools, nnbench will look for a <code>[tool.nnbench]</code> table inside the pyproject.toml file, and if found, use it to set certain values.</p> <p>Currently, you can set the log level, and register custom context provider classes.</p>"},{"location":"cli/pyproject/#general-options","title":"General options","text":"<pre><code>[tool.nnbench]\n# This sets the `nnbench` logger's level to \"DEBUG\", enabling debug log collections.\nlog-level = \"DEBUG\"\n</code></pre>"},{"location":"cli/pyproject/#registering-custom-context-providers","title":"Registering custom context providers","text":"<p>As a quick refresher, in nnbench, a context provider is a function taking no arguments, and returning a Python dictionary with string keys:</p> <pre><code>import os\n\ndef foo() -&gt; dict[str, str]:\n    \"\"\"Returns a context value named 'foo', containing the value of the FOO environment variable.\"\"\"\n    return {\"foo\": os.getenv(\"FOO\", \"\")}\n</code></pre> <p>If you would like to use a custom context provider to collect metadata before a CLI benchmark run, you can give its details in a <code>[tool.nnbench.context]</code> table.</p> <pre><code>[tool.nnbench.context.myctx]\nname = \"myctx\"\nclasspath = \"nnbench.context.PythonInfo\"\narguments = { packages = [\"rich\", \"pyyaml\"] }\n</code></pre> <p>In this case, we are augmenting <code>nnbench.context.PythonInfo</code>, a builtin provider class, to also collect the versions of the <code>rich</code> and <code>pyyaml</code> packages from the current environment, and registering it under the name \"myctx\".</p> <p>The <code>name</code> field is used to register the context provider. The <code>classpath</code> field needs to be a fully qualified Python module path to the context provider class or function. Any arguments needed to instantiate a context provider class can be given under the <code>arguments</code> key in an inline table, which will be passed to the class found under <code>classpath</code> as keyword arguments.</p> <p>Warning</p> <p>If you register a context provider function, you must leave the <code>arguments</code> key out of the above TOML table, since by definition, context providers do not take any arguments in their <code>__call__()</code> signature.</p> <p>Now we can use said provider in a benchmark run by passing just the provider name:</p> <pre><code>$ nnbench run benchmarks.py --context=myctx\nContext values:\n{\n    \"python\": {\n        \"version\": \"3.11.10\",\n        \"implementation\": \"CPython\",\n        \"buildno\": \"main\",\n        \"buildtime\": \"Sep  7 2024 01:03:31\",\n        \"packages\": {\n            \"rich\": \"13.9.3\",\n            \"pyyaml\": \"6.0.2\"\n        }\n    }\n}\n\n&lt;tabular result output&gt;\n</code></pre> <p>Tip</p> <p>This feature is a work in progress. The ability to register custom IO and comparison classes will be implemented in future releases of nnbench.</p>"},{"location":"guides/","title":"User Guide","text":"<p>The nnbench user guide provides documentation for users of the library looking to solve specific tasks. See the Quickstart guide for an introductory tutorial.</p> <p>The following guides are available, covering the core concepts of nnbench:</p> <ul> <li>Defining benchmarks and benchmark families with decorators</li> <li>Adding context, customizing benchmarks, and supplying parameters</li> <li>Organizing benchmark code efficiently</li> <li>Collecting and running benchmarks by hand</li> </ul>"},{"location":"guides/benchmarks/","title":"Defining benchmarks with decorators","text":"<p>To benchmark your machine learning code in nnbench, define your key metrics in Python functions and apply one of the provided decorators. The available decorators are  - <code>@nnbench.benchmark</code>, which runs a benchmark with supplied parameters, - <code>@nnbench.parametrize</code>, which runs several benchmarks with the supplied parameter configurations, - <code>@nnbench.product</code>, which runs benchmarks with all parameter combinations that arise from the supplied values. </p> <p>First we introduce a small machine learning example which we will subsequently use to motivate the use of the three benchmark decorators.</p> <p>We recommend to split the model training, benchmark definition, and benchmark running into different files. In this guide, these are called <code>training.py</code>, <code>benchmarks.py</code>, and <code>main.py</code>.</p>"},{"location":"guides/benchmarks/#example","title":"Example","text":"<p>Let us consider an example where we want to evaluate a <code>scikit-learn</code> random forest classifier on the Iris dataset. For this purpose, we will define several helper functions inside a file, <code>training.py</code>. We use <code>prepare_data()</code>, to load the dataset,  <code>train_rf()</code> to train a random forest model with the specified parameters, and <code>accuracy()</code> to calculate the accuracy of the supplied model on the given dataset.</p> <pre><code># training.py\nimport numpy as np\nfrom sklearn import base, metrics\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\n\ndef prepare_data() -&gt; tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    data = load_iris()\n    X, y = data.data, data.target\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n    return X_train, X_test, y_train, y_test\n\n\ndef train_rf(X_train: np.ndarray, y_train: np.ndarray, n_estimators: int, max_depth: int, random_state: int = 42) -&gt; RandomForestClassifier:\n    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=random_state)\n    model.fit(X_train, y_train)\n    return model\n\n\ndef accuracy(model: base.BaseEstimator, y_test: np.ndarray, y_pred: np.ndarray) -&gt; float:\n    accuracy = metrics.accuracy_score(y_test, y_pred)\n    return accuracy\n</code></pre>"},{"location":"guides/benchmarks/#benchmark-for-single-benchmarks","title":"<code>@benchmark</code> for single benchmarks","text":"<p>Now, we define our benchmarks in a new file called <code>benchmarks.py</code>. We first encapsulate the benchmark logic into a function, <code>benchmark_accuracy()</code> which prepares the data, trains a classifier, and lastly, obtains the accuracy. To mark such a function as a benchmark, we apply the <code>@benchmark</code> decorator.</p> <pre><code># benchmarks.py\nimport nnbench\nfrom training import prepare_data, train_rf, accuracy as _accuracy\n\n@nnbench.benchmark\ndef accuracy(n_estimators: int, max_depth: int, random_state: int) -&gt; float:\n    X_train, X_test, y_train, y_test = prepare_data()\n    rf = train_rf(X_train=X_train, y_train=y_train, n_estimators=n_estimators,\n                  max_depth=max_depth, random_state=random_state)\n    y_pred = rf.predict(X_test)\n    acc = _accuracy(model=rf, y_test=y_test, y_pred=y_pred)\n    return acc\n</code></pre> <p>Warning</p> <p>This training benchmark is designed as a local, simple, and self-contained example to showcase nnbench.  In a real world scenario, to follow best practices, you may want to separate the data preparation and model training steps from the benchmarking logic and pass the corresponding artifacts as a parameter to the benchmark. See the user guide for more information.</p> <p>Lastly, we set up a benchmark runner in <code>main.py</code>. There, we supply the parameters (<code>n_estimators</code>, <code>max_depth</code>, <code>random_state</code>) necessary in the function definition as a dictionary to the <code>params</code> keyword argument.</p> <pre><code># main.py\nimport nnbench\nfrom nnbench.reporter import ConsoleReporter\n\nreporter = ConsoleReporter()\nbenchmarks = nnbench.collect(\"benchmarks.py\")\nresult = nnbench.run(benchmarks, params={\"n_estimators\": 100, \"max_depth\": 5, \"random_state\": 42})\nreporter.write(result)\n</code></pre> <p>When we execute <code>main.py</code>, we get the following output:</p> <pre><code>python main.py\n\n# \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n# \u2503 Benchmark \u2503 Value              \u2503 Wall time (ns) \u2503 Parameters                                                \u2503\n# \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n# \u2502 accuracy  \u2502 0.9555555555555556 \u2502 49305875       \u2502 {'n_estimators': 100, 'max_depth': 5, 'random_state': 42} \u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/benchmarks/#nnbenchparametrize-for-multiple-configurations-of-the-same-benchmark","title":"<code>@nnbench.parametrize</code> for multiple configurations of the same benchmark","text":"<p>Sometimes, we are not only interested in the performance of a model for given parameters but want to compare the performance for different configurations.  To achieve this, we can turn our single accuracy benchmark in the <code>benchmarks.py</code> file into a parametrized benchmark. To do this, replace the decorator with <code>@nnbench.parametrize</code> and supply the parameter combinations of choice as dictionaries in the first argument.</p> <pre><code># benchmarks.py\nimport nnbench\nfrom training import prepare_data, train_rf, accuracy as _accuracy\n\n@nnbench.parametrize(\n    ({\"n_estimators\": 10, \"max_depth\": 2},\n    {\"n_estimators\": 50, \"max_depth\": 5},\n    {\"n_estimators\": 100, \"max_depth\": 10})\n)\ndef accuracy(n_estimators: int, max_depth: int, random_state: int) -&gt; float:\n    X_train, X_test, y_train, y_test = prepare_data()\n    rf = train_rf(X_train=X_train, y_train=y_train, n_estimators=n_estimators,\n                  max_depth=max_depth, random_state=random_state)\n    y_pred = rf.predict(X_test)\n    acc = _accuracy(model=rf, y_test=y_test, y_pred=y_pred)\n    return acc\n</code></pre> <p>Notice that the parametrization is still incomplete, as we did not supply a <code>random_state</code> argument. The unfilled arguments are given in <code>nnbench.run()</code> via a dictionary passed as the <code>params</code> keyword argument.</p> <pre><code># main.py\nimport nnbench\nfrom nnbench.reporter import ConsoleReporter\n\nreporter = ConsoleReporter()\nbenchmarks = nnbench.collect(\"benchmarks.py\")\nresult = nnbench.run(benchmarks, params={\"random_state\": 42})\nreporter.write(result)\n</code></pre> <p>Executing the parametrized benchmark, we get an output similar to this:</p> <pre><code>python main.py\n\n# \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n# \u2503 Benchmark                              \u2503 Value              \u2503 Wall time (ns) \u2503 Parameters                                                 \u2503\n# \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n# \u2502 accuracy_n_estimators=10_max_depth=2   \u2502 0.9555555555555556 \u2502 10600833       \u2502 {'n_estimators': 10, 'max_depth': 2, 'random_state': 42}   \u2502\n# \u2502 accuracy_n_estimators=50_max_depth=5   \u2502 0.9555555555555556 \u2502 22033000       \u2502 {'n_estimators': 50, 'max_depth': 5, 'random_state': 42}   \u2502\n# \u2502 accuracy_n_estimators=100_max_depth=10 \u2502 0.9333333333333333 \u2502 43839917       \u2502 {'n_estimators': 100, 'max_depth': 10, 'random_state': 42} \u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/benchmarks/#nnbenchproduct-for-benchmarks-over-parameter-grids","title":"<code>@nnbench.product</code> for benchmarks over parameter grids","text":"<p>In case we want to run a benchmark for all possible combinations of a set of parameters, we can use the <code>@nnbench.product</code> decorator to supply the different values for each parameter.</p> <pre><code># benchmarks.py\nimport nnbench\nfrom training import prepare_data, train_rf, accuracy as _accuracy\n\n@nnbench.product(n_estimators=[10, 50, 100], max_depth=[2, 5, 10])\ndef benchmark_accuracy_product(n_estimators: int, max_depth: int, random_state: int) -&gt; float:\n    X_train, X_test, y_train, y_test = prepare_data()\n    rf = train_rf(X_train=X_train, y_train=y_train, n_estimators=n_estimators,\n                  max_depth=max_depth, random_state=random_state)\n    y_pred = rf.predict(X_test)\n    acc = _accuracy(model=rf, y_test=y_test, y_pred=y_pred)\n    return acc\n</code></pre> <p>We still provide the <code>random_state</code> parameter to the runner directly, like we did with the <code>@nnbench.parametrize</code> decorator. By executing the benchmark, we get results for all combinations of <code>n_estimators</code> and <code>max_depth</code>. It looks similar to this:</p> <pre><code>python main.py\n\n# \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n# \u2503 Benchmark                              \u2503 Value              \u2503 Wall time (ns) \u2503 Parameters                                                 \u2503\n# \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n# \u2502 accuracy_n_estimators=10_max_depth=2   \u2502 0.9111111111111111 \u2502 10516875       \u2502 {'n_estimators': 10, 'max_depth': 2, 'random_state': 42}   \u2502\n# \u2502 accuracy_n_estimators=10_max_depth=5   \u2502 1.0                \u2502 5783791        \u2502 {'n_estimators': 10, 'max_depth': 5, 'random_state': 42}   \u2502\n# \u2502 accuracy_n_estimators=10_max_depth=10  \u2502 0.8888888888888888 \u2502 5350000        \u2502 {'n_estimators': 10, 'max_depth': 10, 'random_state': 42}  \u2502\n# \u2502 accuracy_n_estimators=50_max_depth=2   \u2502 0.9555555555555556 \u2502 21473084       \u2502 {'n_estimators': 50, 'max_depth': 2, 'random_state': 42}   \u2502\n# \u2502 accuracy_n_estimators=50_max_depth=5   \u2502 0.9777777777777777 \u2502 21978583       \u2502 {'n_estimators': 50, 'max_depth': 5, 'random_state': 42}   \u2502\n# \u2502 accuracy_n_estimators=50_max_depth=10  \u2502 0.9777777777777777 \u2502 21687166       \u2502 {'n_estimators': 50, 'max_depth': 10, 'random_state': 42}  \u2502\n# \u2502 accuracy_n_estimators=100_max_depth=2  \u2502 0.9111111111111111 \u2502 42262792       \u2502 {'n_estimators': 100, 'max_depth': 2, 'random_state': 42}  \u2502\n# \u2502 accuracy_n_estimators=100_max_depth=5  \u2502 0.9555555555555556 \u2502 43785958       \u2502 {'n_estimators': 100, 'max_depth': 5, 'random_state': 42}  \u2502\n# \u2502 accuracy_n_estimators=100_max_depth=10 \u2502 0.9111111111111111 \u2502 43720709       \u2502 {'n_estimators': 100, 'max_depth': 10, 'random_state': 42} \u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/customization/","title":"Defining setup/teardown tasks, context, and <code>nnbench.Parameters</code>","text":"<p>This page introduces some customization options for benchmark runs. These options can be helpful for tasks surrounding benchmark state management, such as automatic setup and cleanup, contextualizing results with context values, and defining typed parameters with the <code>nnbench.Parameters</code> class.</p>"},{"location":"guides/customization/#defining-setup-and-teardown-tasks","title":"Defining setup and teardown tasks","text":"<p>For some benchmarks, it is important to set certain configuration values and prepare the execution environment before running. To do this, you can pass a setup task to all of the nnbench decorators via the <code>setUp</code> keyword:</p> <pre><code>import os\n\nimport nnbench\n\n\ndef set_envvar(**params):\n    os.environ[\"MY_ENV\"] = \"MY_VALUE\"\n\n\n@nnbench.benchmark(setUp=set_envvar)\ndef prod(a: int, b: int) -&gt; int:\n    return a * b\n</code></pre> <p>Similarly, to revert the environment state back to its previous form (or clean up any created resources), you can supply a finalization task with the <code>tearDown</code> keyword:</p> <pre><code>import os\n\nimport nnbench\n\n\ndef set_envvar(**params):\n    os.environ[\"MY_ENV\"] = \"MY_VALUE\"\n\n\ndef pop_envvar(**params):\n    os.environ.pop(\"MY_ENV\")\n\n\n@nnbench.benchmark(setUp=set_envvar, tearDown=pop_envvar)\ndef prod(a: int, b: int) -&gt; int:\n    return a * b\n</code></pre> <p>Both the setup and teardown task must take the exact same set of parameters as the benchmark function. To simplify function declaration, it is easiest to use a variadic keyword-only interface, i.e. <code>setup(**kwargs)</code>, as shown.</p> <p>Tip</p> <p>This facility works exactly the same for the <code>@nnbench.parametrize</code> and <code>@nnbench.product</code> decorators. There, the specified setup and teardown tasks are run once before or after each of the resulting benchmarks respectively.</p>"},{"location":"guides/customization/#enriching-benchmark-metadata-with-context-values","title":"Enriching benchmark metadata with context values","text":"<p>It is often useful to log specific environment metadata in addition to the benchmark's target metrics. Such metadata can give a clearer picture of how certain models perform on a given hardware, how model architectures compare in performance, and much more. In <code>nnbench</code>, you can give additional metadata to your benchmarks as context values.</p> <p>A context value is defined here as a key-value pair where <code>key</code> is a string, and <code>value</code> is any valid JSON value holding the desired information. As an example, the context value <code>{\"cpuarch\": \"arm64\"}</code> gives information about the CPU architecture of the host machine running the benchmark.</p> <p>A context provider is a function taking no arguments and returning a Python dictionary of context values. The following is a basic example of a context provider:</p> <pre><code>import platform\n\ndef platinfo() -&gt; dict[str, str]:\n    \"\"\"Returns CPU arch, system name (Windows/Linux/Darwin), and Python version.\"\"\"\n    return {\n        \"system\": platform.system(),\n        \"cpuarch\": platform.machine(),\n        \"python_version\": platform.python_version(),\n    }\n</code></pre> <p>To supply context to your benchmarks, you can give a sequence of context providers to <code>nnbench.run()</code>:</p> <pre><code>import nnbench\n\n# uses the `platinfo` context provider from above to log platform metadata.\nbenchmarks = nnbench.collect(__name__)\nresult = nnbench.run(benchmarks, params={}, context=[platinfo])\n</code></pre>"},{"location":"guides/customization/#being-type-safe-by-using-nnbenchparameters","title":"Being type safe by using <code>nnbench.Parameters</code>","text":"<p>Instead of specifying your benchmark's parameters by using a raw Python dictionary, you can define a custom subclass of <code>nnbench.Parameters</code>:</p> <pre><code>import nnbench\nfrom dataclasses import dataclass\n\n\n@dataclass(frozen=True)\nclass MyParams(nnbench.Parameters):\n    a: int\n    b: int\n\n\n@nnbench.benchmark\ndef prod(a: int, b: int) -&gt; int:\n    return a * b\n\n\nparams = MyParams(a=1, b=2)\nbenchmarks = nnbench.collect(__name__)\nresult = nnbench.run(benchmarks, params=params)\n</code></pre> <p>While this does not have a concrete advantage in terms of type safety over a raw dictionary, it guards against accidental modification of parameters breaking reproducibility.</p>"},{"location":"guides/organization/","title":"How to efficiently organize benchmark code","text":"<p>To efficiently organize benchmarks and keeping your setup modular, you can follow a few guidelines.</p>"},{"location":"guides/organization/#tip-1-separate-benchmarks-from-project-code","title":"Tip 1: Separate benchmarks from project code","text":"<p>This tip is well known from other software development practices such as unit testing. To improve project organization, consider splitting off your benchmarks into their own modules or even directories, if you have multiple benchmark workloads.</p> <p>An example project layout can look like this, with benchmarks as a separate directory at the top-level:</p> <pre><code>my-project/\n\u251c\u2500\u2500 benchmarks/ # &lt;- contains all benchmarking Python files.\n\u251c\u2500\u2500 docs/\n\u251c\u2500\u2500 src/\n\u251c\u2500\u2500 .pre-commit-config.yaml\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 ...\n</code></pre> <p>This keeps the benchmarks neatly grouped together while siloing them away from the actual project code. Since you will most likely not run your benchmarks in a production setting, this is also advantageous for packaging, as the <code>benchmarks/</code> directory does not ship by default in this configuration.</p>"},{"location":"guides/organization/#tip-2-group-benchmarks-by-common-attributes","title":"Tip 2: Group benchmarks by common attributes","text":"<p>To maintain good organization within your benchmark directory, you can group similar benchmarks into their own Python files. As an example, if you have a set of benchmarks to establish data quality, and benchmarks for scoring trained models on curated data, you could structure them as follows:</p> <pre><code>benchmarks/\n\u251c\u2500\u2500 data_quality.py\n\u251c\u2500\u2500 model_perf.py\n\u2514\u2500\u2500 ...\n</code></pre> <p>This is helpful when running multiple benchmark workloads separately, as you can just point your benchmark runner to each of these separate files:</p> <pre><code>import nnbench\n\ndata_quality_benchmarks = nnbench.collect(\"benchmarks/data_quality.py\")\ndata_metrics = nnbench.run(data_quality_benchmarks, params=...)\n# same for model metrics, where instead you pass benchmarks/model_perf.py.\nmodel_perf_benchmarks = nnbench.collect(\"benchmarks/model_perf.py\")\nmodel_metrics = nnbench.run(model_perf_benchmarks, params=...)\n</code></pre>"},{"location":"guides/organization/#tip-3-attach-tags-to-benchmarks-for-selective-filtering","title":"Tip 3: Attach tags to benchmarks for selective filtering","text":"<p>For structuring benchmarks within files, you can also use tags, which are tuples of strings attached to a benchmark:</p> <pre><code># benchmarks/data_quality.py\nimport nnbench\n\n\n@nnbench.benchmark(tags=(\"foo\",))\ndef foo1(data) -&gt; float:\n    ...\n\n\n@nnbench.benchmark(tags=(\"foo\",))\ndef foo2(data) -&gt; int:\n    ...\n\n\n@nnbench.benchmark(tags=(\"bar\",))\ndef bar(data) -&gt; int:\n    ...\n</code></pre> <p>Now, to only run data quality benchmarks marked \"foo\", pass the corresponding tag to <code>nnbench.run()</code>:</p> <pre><code>import nnbench\n\nbenchmarks = nnbench.collect(\"benchmarks/data_quality.py\", tags=(\"foo\",))\nfoo_data_metrics = nnbench.run(benchmarks, params=..., )\n</code></pre> <p>Tip</p> <p>This concept works exactly the same when creating benchmarks with the <code>@nnbench.parametrize</code> and <code>@nnbench.product</code> decorators.</p>"},{"location":"guides/runners/","title":"Collecting and running benchmarks","text":"<p>nnbench provides the <code>nnbench.collect</code> and <code>nnbench.run</code> APIs as a compact interface to collect and run benchmarks selectively.</p> <p>Use the <code>nnbench.collect()</code> method to collect benchmarks from files or directories. Assume we have the following benchmark setup: <pre><code># dir_a/bm1.py\nimport nnbench\n\n@nnbench.benchmark\ndef dummy_benchmark(a: int) -&gt; int:\n    return a\n</code></pre></p> <pre><code># dir_b/bm2.py\nimport nnbench\n\n@nnbench.benchmark(tags=(\"tag\",))\ndef another_benchmark(b: int) -&gt; int:\n    return b\n\n@nnbench.benchmark\ndef yet_another_benchmark(c: int) -&gt; int:\n    return c\n</code></pre> <pre><code># dir_b/bm3.py\nimport nnbench\n@nnbench.benchmark(tags=(\"tag\",))\ndef the_last_benchmark(d: int) -&gt; int:\n    return d\n</code></pre> <p>Now we can collect benchmarks from files:</p> <p><pre><code>import nnbench\n\n\nbenchmarks = nnbench.collect('dir_a/bm1.py')\n</code></pre> Or directories:</p> <pre><code>benchmarks = nnbench.collect('dir_b')\n</code></pre> <p>You can also supply tags to the runner to selectively collect only benchmarks with the appropriate tag. For example, after clearing the runner again, you can collect all benchmarks with the <code>\"tag\"</code> tag as such:</p> <pre><code>import nnbench\n\n\ntagged_benchmarks = nnbench.collect('dir_b', tags=(\"tag\",))\n</code></pre> <p>To run the benchmarks, call the <code>nnbench.run()</code> method and supply the necessary parameters required by the collected benchmarks.</p> <pre><code>result = nnbench.run(benchmarks, params={\"b\": 1, \"c\": 2, \"d\": 3})\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>nnbench<ul> <li>cli</li> <li>compare</li> <li>config</li> <li>context</li> <li>core</li> <li>fixtures</li> <li>reporter<ul> <li>console</li> <li>file</li> <li>service</li> <li>util</li> </ul> </li> <li>runner</li> <li>types</li> <li>util</li> </ul> </li> </ul>"},{"location":"reference/nnbench/","title":"nnbench","text":"<p>A framework for organizing and running benchmark workloads on machine learning models.</p>"},{"location":"reference/nnbench/#modules","title":"Modules","text":""},{"location":"reference/nnbench/#cli","title":"cli","text":"<p>The <code>nnbench</code> command line interface.</p>"},{"location":"reference/nnbench/#compare","title":"compare","text":"<p>Contains machinery to compare multiple benchmark records side by side.</p>"},{"location":"reference/nnbench/#config","title":"config","text":"<p>Utilities for parsing a <code>[tool.nnbench]</code> config block out of a pyproject.toml file.</p>"},{"location":"reference/nnbench/#context","title":"context","text":"<p>Utilities for collecting context key-value pairs as metadata in benchmark runs.</p>"},{"location":"reference/nnbench/#core","title":"core","text":"<p>Data model, registration, and parametrization facilities for defining benchmarks.</p>"},{"location":"reference/nnbench/#fixtures","title":"fixtures","text":"<p>Collect values ('fixtures') by name for benchmark runs from certain files,</p>"},{"location":"reference/nnbench/#reporter","title":"reporter","text":"<p>An interface for displaying, writing, or streaming benchmark results to</p>"},{"location":"reference/nnbench/#runner","title":"runner","text":"<p>The abstract benchmark runner interface, which can be overridden for custom benchmark workloads.</p>"},{"location":"reference/nnbench/#types","title":"types","text":"<p>Types for benchmarks and records holding results of a run.</p>"},{"location":"reference/nnbench/#util","title":"util","text":"<p>Various utilities related to benchmark collection, filtering, and more.</p>"},{"location":"reference/nnbench/cli/","title":"cli","text":"<p>The <code>nnbench</code> command line interface.</p>"},{"location":"reference/nnbench/cli/#nnbench.cli.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger('nnbench')\n</code></pre>"},{"location":"reference/nnbench/cli/#nnbench.cli.CustomFormatter","title":"CustomFormatter","text":"<p>               Bases: <code>RawDescriptionHelpFormatter</code></p> Source code in <code>src/nnbench/cli.py</code> <pre><code>class CustomFormatter(argparse.RawDescriptionHelpFormatter):\n    def _format_action_invocation(self, action):\n        if not action.option_strings:\n            (metavar,) = self._metavar_formatter(action, action.dest)(1)\n            return metavar\n        elif isinstance(action, argparse.BooleanOptionalAction):\n            if len(action.option_strings) == 2:\n                true_opt, false_opt = action.option_strings\n                return \"--[no-]\" + true_opt[2:]\n        else:\n            parts = []\n            # if the Optional doesn't take a value, format is:\n            #    -s, --long\n            if action.nargs == 0:\n                parts.extend(action.option_strings)\n\n            # if the Optional takes a value, format is:\n            #    -s, --long ARGS\n            else:\n                default = action.dest.upper()\n                args_string = self._format_args(action, default)\n                parts.extend(action.option_strings)\n                parts[-1] += f\" {args_string}\"\n            return \", \".join(parts)\n</code></pre>"},{"location":"reference/nnbench/cli/#nnbench.cli.collect_and_run","title":"collect_and_run","text":"<pre><code>collect_and_run(\n    path: str | PathLike[str],\n    name: str | None = None,\n    tags: tuple[str, ...] = (),\n    context: Context | Iterable[ContextProvider] = (),\n    jsonifier: Callable = jsonify,\n) -&gt; BenchmarkRecord\n</code></pre> Source code in <code>src/nnbench/cli.py</code> <pre><code>def collect_and_run(\n    path: str | PathLike[str],\n    name: str | None = None,\n    tags: tuple[str, ...] = (),\n    context: Context | Iterable[ContextProvider] = (),\n    jsonifier: Callable = jsonify,\n) -&gt; BenchmarkRecord:\n    benchmarks = collect(path, tags=tags)\n    record = run(\n        benchmarks,\n        name=name,\n        context=context,\n        jsonifier=jsonifier,\n    )\n    return record\n</code></pre>"},{"location":"reference/nnbench/cli/#nnbench.cli.construct_parser","title":"construct_parser","text":"<pre><code>construct_parser(config: NNBenchConfig) -&gt; ArgumentParser\n</code></pre> Source code in <code>src/nnbench/cli.py</code> <pre><code>def construct_parser(config: NNBenchConfig) -&gt; argparse.ArgumentParser:\n    parser = argparse.ArgumentParser(\"nnbench\", formatter_class=CustomFormatter)\n    parser.add_argument(\"--version\", action=\"version\", version=_VERSION)\n    parser.add_argument(\n        \"--log-level\",\n        default=config.log_level,\n        type=_log_level,\n        metavar=\"&lt;level&gt;\",\n        help=\"Log level to use for the nnbench package, defaults to NOTSET (no logging).\",\n    )\n    subparsers = parser.add_subparsers(\n        title=\"Available commands\",\n        required=False,\n        dest=\"command\",\n        metavar=\"\",\n    )\n    run_parser = subparsers.add_parser(\n        \"run\", help=\"Run a benchmark workload.\", formatter_class=CustomFormatter\n    )\n    # can be a directory, single file, or glob\n    run_parser.add_argument(\n        \"benchmarks\",\n        nargs=\"?\",\n        metavar=\"&lt;benchmarks&gt;\",\n        default=\"benchmarks\",\n        help=\"A Python file or directory of files containing benchmarks to run.\",\n    )\n    run_parser.add_argument(\n        \"-n\",\n        \"--name\",\n        type=str,\n        default=f\"nnbench-{time.time_ns()}\",\n        metavar=\"&lt;name&gt;\",\n        help=\"A name to assign to the benchmark run, for example for record keeping in a database.\",\n    )\n    run_parser.add_argument(\n        \"-j\",\n        type=int,\n        default=-1,\n        dest=\"jobs\",\n        metavar=\"&lt;N&gt;\",\n        help=\"Number of processes to use for running benchmarks in parallel, default: -1 (no parallelism)\",\n    )\n    run_parser.add_argument(\n        \"--context\",\n        action=\"append\",\n        metavar=\"&lt;key=value&gt;\",\n        help=\"Additional context values giving information about the benchmark run.\",\n        default=list(),\n    )\n    run_parser.add_argument(\n        \"-t\",\n        \"--tag\",\n        action=\"append\",\n        metavar=\"&lt;tag&gt;\",\n        dest=\"tags\",\n        help=\"Only run benchmarks marked with one or more given tag(s).\",\n        default=tuple(),\n    )\n    run_parser.add_argument(\n        \"-o\",\n        \"--output-file\",\n        metavar=\"&lt;file&gt;\",\n        dest=\"outfile\",\n        help=\"File or stream to write results to, defaults to stdout.\",\n        default=sys.stdout,\n    )\n    run_parser.add_argument(\n        \"--jsonifier\",\n        metavar=\"&lt;classpath&gt;\",\n        default=config.jsonifier,\n        help=\"Function to create a JSON representation of input parameters with, helping make runs reproducible.\",\n    )\n\n    compare_parser = subparsers.add_parser(\n        \"compare\",\n        help=\"Compare results from multiple benchmark runs.\",\n        formatter_class=CustomFormatter,\n    )\n    compare_parser.add_argument(\n        \"records\",\n        nargs=\"+\",\n        help=\"Records to compare results for. Can be given as local files or remote URIs.\",\n    )\n    compare_parser.add_argument(\n        \"-P\",\n        \"--include-parameter\",\n        action=\"append\",\n        metavar=\"&lt;name&gt;\",\n        dest=\"parameters\",\n        default=list(),\n        help=\"Names of input parameters to display in the comparison table.\",\n    )\n    compare_parser.add_argument(\n        \"-C\",\n        \"--include-context\",\n        action=\"append\",\n        metavar=\"&lt;name&gt;\",\n        dest=\"contextvals\",\n        default=list(),\n        help=\"Context values to display in the comparison table. Use dotted syntax for nested context values.\",\n    )\n    compare_parser.add_argument(\n        \"-E\",\n        \"--extra-column\",\n        action=\"append\",\n        metavar=\"&lt;name&gt;\",\n        dest=\"extra_cols\",\n        default=list(),\n        help=\"Additional record data to display in the comparison table.\",\n    )\n    return parser\n</code></pre>"},{"location":"reference/nnbench/cli/#nnbench.cli.main","title":"main","text":"<pre><code>main(argv: list[str] | None = None) -&gt; int\n</code></pre> <p>The main <code>nnbench</code> CLI entry point.</p> Source code in <code>src/nnbench/cli.py</code> <pre><code>def main(argv: list[str] | None = None) -&gt; int:\n    \"\"\"The main ``nnbench`` CLI entry point.\"\"\"\n    config = parse_nnbench_config()\n    parser = construct_parser(config)\n    try:\n        args = parser.parse_args(argv)\n        if args.command is None:\n            parser.print_help()\n            return 1\n        elif args.command == \"run\":\n            from nnbench.context import builtin_providers, register_context_provider\n\n            context: dict[str, Any] = {}\n            for p in config.context:\n                logger.debug(f\"Registering context provider {p.name!r}\")\n                klass = import_(p.classpath)\n                register_context_provider(p.name, klass, p.arguments)\n\n            for val in args.context:\n                if val in builtin_providers:\n                    context.update(builtin_providers[val]())\n                else:\n                    try:\n                        k, v = val.split(\"=\", 1)\n                        context[k] = v\n                    except ValueError:\n                        raise ValueError(\"context values need to be of the form &lt;key&gt;=&lt;value&gt;\")\n\n            n_jobs: int = args.jobs\n            jsonifier = import_(args.jsonifier)\n            if n_jobs &lt; 2:\n                record = collect_and_run(\n                    args.benchmarks,\n                    name=args.name,\n                    tags=tuple(args.tags),\n                    context=context,\n                    jsonifier=jsonifier,\n                )\n            else:\n                compute_fn = partial(\n                    collect_and_run,\n                    name=args.name,\n                    tags=tuple(args.tags),\n                    context=context,\n                    jsonifier=jsonifier,\n                )\n                with multiprocessing.Pool(n_jobs) as p:\n                    bm_path = Path(args.benchmarks)\n                    # unroll paths in case a directory is passed.\n                    if bm_path.is_dir():\n                        benchmarks = all_python_files(bm_path)\n                    else:\n                        benchmarks = [bm_path]\n                    res: list[BenchmarkRecord] = p.map(compute_fn, benchmarks)\n                    benchmarks = sum([r.benchmarks for r in res], start=list())\n                    # Assumes the context and run name to be consistent across workers.\n                    record = BenchmarkRecord(\n                        run=res[0].run,\n                        context=res[0].context,\n                        benchmarks=benchmarks,\n                    )\n\n            outfile = args.outfile\n            io = get_io_implementation(outfile)\n            io.write(record, outfile, {})\n        elif args.command == \"compare\":\n            from nnbench.compare import compare\n\n            f = FileReporter()\n            records = [f.read(file) for file in args.records]\n            compare(\n                records=records,\n                parameters=args.parameters,\n                contextvals=args.contextvals,\n            )\n\n        return 0\n    except Exception as e:\n        sys.stderr.write(f\"error: {e}\")\n        return 1\n</code></pre>"},{"location":"reference/nnbench/compare/","title":"compare","text":"<p>Contains machinery to compare multiple benchmark records side by side.</p>"},{"location":"reference/nnbench/compare/#nnbench.compare.get_value_by_name","title":"get_value_by_name","text":"<pre><code>get_value_by_name(record: BenchmarkRecord, name: str, missing: str) -&gt; str\n</code></pre> <p>Get the value of a metric by name from a benchmark record, or a placeholder if the metric name is not present in the record.</p> <p>If the name is found, but the benchmark did not complete successfully (i.e. the <code>error_occurred</code> value is set to <code>True</code>), the returned value will be set to the value of the <code>error_message</code> field.</p> PARAMETER DESCRIPTION <code>record</code> <p>The benchmark record to extract a metric value from.</p> <p> TYPE: <code>BenchmarkRecord</code> </p> <code>name</code> <p>The name of the target metric.</p> <p> TYPE: <code>str</code> </p> <code>missing</code> <p>A placeholder string to return in the event of a missing metric.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <p>A string containing the metric value (or error message) formatted as rich text.</p> Source code in <code>src/nnbench/compare.py</code> <pre><code>def get_value_by_name(record: BenchmarkRecord, name: str, missing: str) -&gt; str:\n    \"\"\"\n    Get the value of a metric by name from a benchmark record, or a placeholder\n    if the metric name is not present in the record.\n\n    If the name is found, but the benchmark did not complete successfully\n    (i.e. the ``error_occurred`` value is set to ``True``), the returned value\n    will be set to the value of the ``error_message`` field.\n\n    Parameters\n    ----------\n    record: BenchmarkRecord\n        The benchmark record to extract a metric value from.\n    name: str\n        The name of the target metric.\n    missing: str\n        A placeholder string to return in the event of a missing metric.\n\n    Returns\n    -------\n    str\n        A string containing the metric value (or error message) formatted\n        as rich text.\n\n    \"\"\"\n    metric_names = [b[\"name\"] for b in record.benchmarks]\n    if name not in metric_names:\n        return missing\n\n    res = record.benchmarks[metric_names.index(name)]\n    if res.get(\"error_occurred\", False):\n        errmsg = res.get(\"error_message\", \"&lt;unknown&gt;\")\n        return \"[red]ERROR: [/red]\" + errmsg\n    return str(res.get(\"value\", missing))\n</code></pre>"},{"location":"reference/nnbench/compare/#nnbench.compare.compare","title":"compare","text":"<pre><code>compare(\n    records: Sequence[BenchmarkRecord],\n    parameters: Sequence[str] | None = None,\n    contextvals: Sequence[str] | None = None,\n    missing: str = _MISSING,\n) -&gt; None\n</code></pre> <p>Compare a series of benchmark records, displaying their results in a table side by side.</p> PARAMETER DESCRIPTION <code>records</code> <p>The benchmark records to compare.</p> <p> TYPE: <code>Sequence[BenchmarkRecord]</code> </p> <code>parameters</code> <p>Names of parameters to display as extra columns.</p> <p> TYPE: <code>Sequence[str] | None</code> DEFAULT: <code>None</code> </p> <code>contextvals</code> <p>Names of context values to display as extra columns. Supports nested access via dotted syntax.</p> <p> TYPE: <code>Sequence[str] | None</code> DEFAULT: <code>None</code> </p> <code>missing</code> <p>A placeholder string to show in the event of a missing metric.</p> <p> TYPE: <code>str</code> DEFAULT: <code>_MISSING</code> </p> Source code in <code>src/nnbench/compare.py</code> <pre><code>def compare(\n    records: Sequence[BenchmarkRecord],\n    parameters: Sequence[str] | None = None,\n    contextvals: Sequence[str] | None = None,\n    missing: str = _MISSING,\n) -&gt; None:\n    \"\"\"\n    Compare a series of benchmark records, displaying their results in a table\n    side by side.\n\n    Parameters\n    ----------\n    records: Sequence[BenchmarkRecord]\n        The benchmark records to compare.\n    parameters: Sequence[str] | None\n        Names of parameters to display as extra columns.\n    contextvals: Sequence[str] | None\n        Names of context values to display as extra columns. Supports nested access\n        via dotted syntax.\n    missing: str\n        A placeholder string to show in the event of a missing metric.\n    \"\"\"\n    t = Table()\n\n    rows: list[list[str]] = []\n    columns: list[str] = [\"Benchmark run\"]\n\n    # Add metric names first, without duplicates.\n    for record in records:\n        names = [b[\"name\"] for b in record.benchmarks]\n        for name in names:\n            if name not in set(columns):\n                columns.append(name)\n\n    names = copy.deepcopy(columns[1:])\n\n    # Then parameters, if any\n    if parameters is not None:\n        columns += [f\"Params-&gt;{p}\" for p in parameters]\n\n    if contextvals is not None:\n        columns += contextvals\n\n    # Main loop, extracts values from the individual records,\n    # or a placeholder if there are any.\n    for record in records:\n        # flatten facilitates dotted access to nested context values, e.g. git.branch\n        ctx = flatten(record.context)\n        row = [record.run]\n        row += [get_value_by_name(record, name, _MISSING) for name in names]\n        # hacky, extra cols is likely now broken\n        b = record.benchmarks[0]\n        # TODO: Add record-level parameters struct as the union of all benchmark inputs\n        if parameters is not None:\n            params = b.get(\"parameters\", {})\n            row += [str(params.get(p)) for p in parameters]\n        if contextvals is not None:\n            row += [str(ctx.get(cval, missing)) for cval in contextvals]\n        rows.append(row)\n\n    for column in columns:\n        t.add_column(column)\n    for row in rows:\n        t.add_row(*row)\n\n    c = Console()\n    c.print(t)\n</code></pre>"},{"location":"reference/nnbench/config/","title":"config","text":"<p>Utilities for parsing a <code>[tool.nnbench]</code> config block out of a pyproject.toml file.</p>"},{"location":"reference/nnbench/config/#nnbench.config.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger('nnbench.config')\n</code></pre>"},{"location":"reference/nnbench/config/#nnbench.config.DEFAULT_JSONIFIER","title":"DEFAULT_JSONIFIER  <code>module-attribute</code>","text":"<pre><code>DEFAULT_JSONIFIER = 'nnbench.runner.jsonify'\n</code></pre>"},{"location":"reference/nnbench/config/#nnbench.config.ContextProviderDef","title":"ContextProviderDef  <code>dataclass</code>","text":"<p>A POD struct representing a custom context provider definition in a pyproject.toml table.</p> Source code in <code>src/nnbench/config.py</code> <pre><code>@dataclass\nclass ContextProviderDef:\n    \"\"\"\n    A POD struct representing a custom context provider definition in a\n    pyproject.toml table.\n    \"\"\"\n\n    # TODO: Extend this def to a generic typedef, reusable by context\n    #  providers, IOs, and comparisons (with a type enum).\n\n    name: str\n    \"\"\"Name under which the provider should be registered by nnbench.\"\"\"\n    classpath: str\n    \"\"\"Full path to the class or callable returning the context dict.\"\"\"\n    arguments: dict[str, Any] = field(default_factory=dict)\n    \"\"\"\n    Arguments needed to instantiate the context provider class,\n    given as key-value pairs in the table.\n    If the class path points to a function, no arguments may be given.\"\"\"\n</code></pre>"},{"location":"reference/nnbench/config/#nnbench.config.ContextProviderDef.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>Name under which the provider should be registered by nnbench.</p>"},{"location":"reference/nnbench/config/#nnbench.config.ContextProviderDef.classpath","title":"classpath  <code>instance-attribute</code>","text":"<pre><code>classpath: str\n</code></pre> <p>Full path to the class or callable returning the context dict.</p>"},{"location":"reference/nnbench/config/#nnbench.config.ContextProviderDef.arguments","title":"arguments  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arguments: dict[str, Any] = field(default_factory=dict)\n</code></pre> <p>Arguments needed to instantiate the context provider class, given as key-value pairs in the table. If the class path points to a function, no arguments may be given.</p>"},{"location":"reference/nnbench/config/#nnbench.config.NNBenchConfig","title":"NNBenchConfig  <code>dataclass</code>","text":"Source code in <code>src/nnbench/config.py</code> <pre><code>@dataclass(frozen=True)\nclass NNBenchConfig:\n    log_level: str\n    \"\"\"Log level to use for the ``nnbench`` module root logger.\"\"\"\n    context: list[ContextProviderDef]\n    \"\"\"A list of context provider definitions found in pyproject.toml.\"\"\"\n    # TODO: Move this down one level to [tool.nnbench.run]\n    jsonifier: str | Callable[[dict[str, Any]], dict[str, Any]] = DEFAULT_JSONIFIER\n\n    @classmethod\n    def from_toml(cls, d: dict[str, Any]) -&gt; Self:\n        \"\"\"\n        Returns an nnbench CLI config by processing fields obtained from\n        parsing a [tool.nnbench] block in a pyproject.toml file.\n\n        Parameters\n        ----------\n        d: dict[str, Any]\n            Mapping containing the [tool.nnbench] table contents,\n            as obtained by ``tomllib.load()``.\n\n        Returns\n        -------\n        Self\n            An nnbench config instance with the values from pyproject.toml,\n            and defaults for values that were not set explicitly.\n        \"\"\"\n        log_level = d.get(\"log-level\", \"NOTSET\")\n        provider_map = d.get(\"context\", {})\n        jsonifier = d.get(\"jsonifier\", DEFAULT_JSONIFIER)\n        context = [ContextProviderDef(**cpd) for cpd in provider_map.values()]\n        return cls(log_level=log_level, context=context, jsonifier=jsonifier)\n</code></pre>"},{"location":"reference/nnbench/config/#nnbench.config.NNBenchConfig.log_level","title":"log_level  <code>instance-attribute</code>","text":"<pre><code>log_level: str\n</code></pre> <p>Log level to use for the <code>nnbench</code> module root logger.</p>"},{"location":"reference/nnbench/config/#nnbench.config.NNBenchConfig.context","title":"context  <code>instance-attribute</code>","text":"<pre><code>context: list[ContextProviderDef]\n</code></pre> <p>A list of context provider definitions found in pyproject.toml.</p>"},{"location":"reference/nnbench/config/#nnbench.config.NNBenchConfig.jsonifier","title":"jsonifier  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>jsonifier: str | Callable[[dict[str, Any]], dict[str, Any]] = DEFAULT_JSONIFIER\n</code></pre>"},{"location":"reference/nnbench/config/#nnbench.config.NNBenchConfig.from_toml","title":"from_toml  <code>classmethod</code>","text":"<pre><code>from_toml(d: dict[str, Any]) -&gt; Self\n</code></pre> <p>Returns an nnbench CLI config by processing fields obtained from parsing a [tool.nnbench] block in a pyproject.toml file.</p> PARAMETER DESCRIPTION <code>d</code> <p>Mapping containing the [tool.nnbench] table contents, as obtained by <code>tomllib.load()</code>.</p> <p> TYPE: <code>dict[str, Any]</code> </p> RETURNS DESCRIPTION <code>Self</code> <p>An nnbench config instance with the values from pyproject.toml, and defaults for values that were not set explicitly.</p> Source code in <code>src/nnbench/config.py</code> <pre><code>@classmethod\ndef from_toml(cls, d: dict[str, Any]) -&gt; Self:\n    \"\"\"\n    Returns an nnbench CLI config by processing fields obtained from\n    parsing a [tool.nnbench] block in a pyproject.toml file.\n\n    Parameters\n    ----------\n    d: dict[str, Any]\n        Mapping containing the [tool.nnbench] table contents,\n        as obtained by ``tomllib.load()``.\n\n    Returns\n    -------\n    Self\n        An nnbench config instance with the values from pyproject.toml,\n        and defaults for values that were not set explicitly.\n    \"\"\"\n    log_level = d.get(\"log-level\", \"NOTSET\")\n    provider_map = d.get(\"context\", {})\n    jsonifier = d.get(\"jsonifier\", DEFAULT_JSONIFIER)\n    context = [ContextProviderDef(**cpd) for cpd in provider_map.values()]\n    return cls(log_level=log_level, context=context, jsonifier=jsonifier)\n</code></pre>"},{"location":"reference/nnbench/config/#nnbench.config.locate_pyproject","title":"locate_pyproject","text":"<pre><code>locate_pyproject(stop: PathLike[str] = home()) -&gt; PathLike[str] | None\n</code></pre> <p>Locate a pyproject.toml file by walking up from the current directory, and checking for file existence, stopping at <code>stop</code> (by default, the current user home directory).</p> <p>If no pyproject.toml file can be found at any level, returns None.</p> RETURNS DESCRIPTION <code>PathLike[str] | None</code> <p>The path to pyproject.toml.</p> Source code in <code>src/nnbench/config.py</code> <pre><code>def locate_pyproject(stop: os.PathLike[str] = Path.home()) -&gt; os.PathLike[str] | None:\n    \"\"\"\n    Locate a pyproject.toml file by walking up from the current directory,\n    and checking for file existence, stopping at ``stop`` (by default, the\n    current user home directory).\n\n    If no pyproject.toml file can be found at any level, returns None.\n\n    Returns\n    -------\n    os.PathLike[str] | None\n        The path to pyproject.toml.\n    \"\"\"\n    cwd = Path.cwd()\n    for p in (cwd, *cwd.parents):\n        if (pyproject_cand := (p / \"pyproject.toml\")).exists():\n            return pyproject_cand\n        if p == stop:\n            break\n    logger.debug(f\"could not locate pyproject.toml in directory {cwd}\")\n    return None\n</code></pre>"},{"location":"reference/nnbench/config/#nnbench.config.parse_nnbench_config","title":"parse_nnbench_config","text":"<pre><code>parse_nnbench_config(pyproject_path: str | PathLike[str] | None = None) -&gt; NNBenchConfig\n</code></pre> <p>Load an nnbench config from a given pyproject.toml file.</p> <p>If no path to the pyproject.toml file is given, an attempt at autodiscovery will be made. If that is unsuccessful, an empty config is returned.</p> PARAMETER DESCRIPTION <code>pyproject_path</code> <p>Path to the current project's pyproject.toml file, optional.</p> <p> TYPE: <code>str | PathLike[str] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>NNBenchConfig</code> <p>The loaded config if found, or a default config.</p> Source code in <code>src/nnbench/config.py</code> <pre><code>def parse_nnbench_config(pyproject_path: str | os.PathLike[str] | None = None) -&gt; NNBenchConfig:\n    \"\"\"\n    Load an nnbench config from a given pyproject.toml file.\n\n    If no path to the pyproject.toml file is given, an attempt at autodiscovery\n    will be made. If that is unsuccessful, an empty config is returned.\n\n    Parameters\n    ----------\n    pyproject_path: str | os.PathLike[str] | None\n        Path to the current project's pyproject.toml file, optional.\n\n    Returns\n    -------\n    NNBenchConfig\n        The loaded config if found, or a default config.\n    \"\"\"\n    pyproject_path = pyproject_path or locate_pyproject()\n    if pyproject_path is None:\n        # pyproject.toml could not be found, so return an empty config.\n        return NNBenchConfig.from_toml({})\n\n    with open(pyproject_path, \"rb\") as fp:\n        pyproject = tomllib.load(fp)\n        return NNBenchConfig.from_toml(pyproject.get(\"tool\", {}).get(\"nnbench\", {}))\n</code></pre>"},{"location":"reference/nnbench/config/#nnbench.config.import_","title":"import_","text":"<pre><code>import_(resource: str) -&gt; Any\n</code></pre> Source code in <code>src/nnbench/config.py</code> <pre><code>def import_(resource: str) -&gt; Any:\n    # If the current directory is not on sys.path, insert it in front.\n    if \"\" not in sys.path and \".\" not in sys.path:\n        sys.path.insert(0, \"\")\n\n    # NB: This assumes that every resource is a top-level member of the\n    # target module, and not nested in a class or other construct.\n    modname, classname = resource.rsplit(\".\", 1)\n    klass = getattr(importlib.import_module(modname), classname)\n    return klass\n</code></pre>"},{"location":"reference/nnbench/context/","title":"context","text":"<p>Utilities for collecting context key-value pairs as metadata in benchmark runs.</p>"},{"location":"reference/nnbench/context/#nnbench.context.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger('nnbench.context')\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.Context","title":"Context  <code>module-attribute</code>","text":"<pre><code>Context = dict[str, Any]\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.ContextProvider","title":"ContextProvider  <code>module-attribute</code>","text":"<pre><code>ContextProvider = Callable[[], Context]\n</code></pre> <p>A function providing a dictionary of context values.</p>"},{"location":"reference/nnbench/context/#nnbench.context.builtin_providers","title":"builtin_providers  <code>module-attribute</code>","text":"<pre><code>builtin_providers: dict[str, ContextProvider] = {\n    \"cpu\": CPUInfo(),\n    \"git\": GitEnvironmentInfo(),\n    \"python\": PythonInfo(),\n}\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.PythonInfo","title":"PythonInfo","text":"<p>A context helper returning version info for requested installed packages.</p> <p>If a requested package is not installed, an empty string is returned instead.</p> PARAMETER DESCRIPTION <code>packages</code> <p>Names of the requested packages under which they exist in the current environment. For packages installed through <code>pip</code>, this equals the PyPI package name.</p> <p> TYPE: <code>Iterable[str]</code> DEFAULT: <code>()</code> </p> Source code in <code>src/nnbench/context.py</code> <pre><code>class PythonInfo:\n    \"\"\"\n    A context helper returning version info for requested installed packages.\n\n    If a requested package is not installed, an empty string is returned instead.\n\n    Parameters\n    ----------\n    packages: Iterable[str]\n        Names of the requested packages under which they exist in the current environment.\n        For packages installed through ``pip``, this equals the PyPI package name.\n    \"\"\"\n\n    key = \"python\"\n\n    def __init__(self, packages: Iterable[str] = ()):\n        self.packages = tuple(packages)\n\n    def __call__(self) -&gt; dict[str, Any]:\n        from importlib.metadata import PackageNotFoundError, version\n\n        result: dict[str, Any] = dict()\n\n        result[\"version\"] = platform.python_version()\n        result[\"implementation\"] = platform.python_implementation()\n        buildno, buildtime = platform.python_build()\n        result[\"buildno\"] = buildno\n        result[\"buildtime\"] = buildtime\n\n        packages: dict[str, str] = {}\n        for pkg in self.packages:\n            try:\n                packages[pkg] = version(pkg)\n            except PackageNotFoundError:\n                packages[pkg] = \"\"\n\n        result[\"packages\"] = packages\n        return {self.key: result}\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.PythonInfo.key","title":"key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>key = 'python'\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.PythonInfo.packages","title":"packages  <code>instance-attribute</code>","text":"<pre><code>packages = tuple(packages)\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.GitEnvironmentInfo","title":"GitEnvironmentInfo","text":"<p>A context helper providing the current git commit, latest tag, and upstream repository name.</p> PARAMETER DESCRIPTION <code>remote</code> <p>Remote name for which to provide info, by default <code>\"origin\"</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'origin'</code> </p> Source code in <code>src/nnbench/context.py</code> <pre><code>class GitEnvironmentInfo:\n    \"\"\"\n    A context helper providing the current git commit, latest tag, and upstream repository name.\n\n    Parameters\n    ----------\n    remote: str\n        Remote name for which to provide info, by default ``\"origin\"``.\n    \"\"\"\n\n    key = \"git\"\n\n    def __init__(self, remote: str = \"origin\"):\n        self.remote = remote\n\n    def __call__(self) -&gt; dict[str, dict[str, Any]]:\n        import subprocess\n\n        def git_subprocess(args: list[str]) -&gt; subprocess.CompletedProcess:\n            if platform.system() == \"Windows\":\n                git = \"git.exe\"\n            else:\n                git = \"git\"\n\n            return subprocess.run(\n                [git, *args],\n                capture_output=True,\n                encoding=\"utf-8\",\n            )\n\n        result: dict[str, Any] = {\n            \"commit\": \"\",\n            \"provider\": \"\",\n            \"repository\": \"\",\n            \"tag\": \"\",\n            \"dirty\": None,\n        }\n\n        # first, check if inside a repo.\n        p = git_subprocess([\"rev-parse\", \"--is-inside-work-tree\"])\n        # if not, return empty info.\n        if p.returncode:\n            return {\"git\": result}\n\n        # secondly: get the current commit.\n        p = git_subprocess([\"rev-parse\", \"HEAD\"])\n        if not p.returncode:\n            result[\"commit\"] = p.stdout.strip()\n\n        # thirdly, get the latest tag, without a short commit SHA attached.\n        p = git_subprocess([\"describe\", \"--tags\", \"--abbrev=0\"])\n        if not p.returncode:\n            result[\"tag\"] = p.stdout.strip()\n\n        # and finally, get the remote repo name pointed to by the given remote.\n        p = git_subprocess([\"remote\", \"get-url\", self.remote])\n        if not p.returncode:\n            remotename: str = p.stdout.strip()\n            if \"@\" in remotename:\n                # it's an SSH remote.\n                prefix, sep = \"git@\", \":\"\n            else:\n                # it is HTTPS.\n                prefix, sep = \"https://\", \"/\"\n\n            remotename = remotename.removeprefix(prefix)\n            provider, reponame = remotename.split(sep, 1)\n\n            result[\"provider\"] = provider\n            result[\"repository\"] = reponame.removesuffix(\".git\")\n\n        p = git_subprocess([\"status\", \"--porcelain\"])\n        if not p.returncode:\n            result[\"dirty\"] = bool(p.stdout.strip())\n\n        return {\"git\": result}\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.GitEnvironmentInfo.key","title":"key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>key = 'git'\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.GitEnvironmentInfo.remote","title":"remote  <code>instance-attribute</code>","text":"<pre><code>remote = remote\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.CPUInfo","title":"CPUInfo","text":"<p>A context helper providing information about the host machine's CPU capabilities, operating system, and amount of memory.</p> PARAMETER DESCRIPTION <code>memunit</code> <p>The unit to display memory size in (either \"kB\" for kilobytes, \"MB\" for Megabytes, or \"GB\" for Gigabytes).</p> <p> TYPE: <code>Literal['kB', 'MB', 'GB']</code> DEFAULT: <code>'MB'</code> </p> <code>frequnit</code> <p>The unit to display CPU clock speeds in (either \"kHz\" for kilohertz, \"MHz\" for Megahertz, or \"GHz\" for Gigahertz).</p> <p> TYPE: <code>Literal['kHz', 'MHz', 'GHz']</code> DEFAULT: <code>'MHz'</code> </p> Source code in <code>src/nnbench/context.py</code> <pre><code>class CPUInfo:\n    \"\"\"\n    A context helper providing information about the host machine's CPU\n    capabilities, operating system, and amount of memory.\n\n    Parameters\n    ----------\n    memunit: Literal[\"kB\", \"MB\", \"GB\"]\n        The unit to display memory size in (either \"kB\" for kilobytes,\n        \"MB\" for Megabytes, or \"GB\" for Gigabytes).\n    frequnit: Literal[\"kHz\", \"MHz\", \"GHz\"]\n        The unit to display CPU clock speeds in (either \"kHz\" for kilohertz,\n        \"MHz\" for Megahertz, or \"GHz\" for Gigahertz).\n    \"\"\"\n\n    key = \"cpu\"\n\n    def __init__(\n        self,\n        memunit: Literal[\"kB\", \"MB\", \"GB\"] = \"MB\",\n        frequnit: Literal[\"kHz\", \"MHz\", \"GHz\"] = \"MHz\",\n    ):\n        self.memunit = memunit\n        self.frequnit = frequnit\n        self.conversion_table: dict[str, float] = {\"k\": 1e3, \"M\": 1e6, \"G\": 1e9}\n\n    def __call__(self) -&gt; dict[str, Any]:\n        try:\n            import psutil\n        except ModuleNotFoundError:\n            raise ModuleNotFoundError(\n                f\"context provider {self.__class__.__name__}() needs `psutil` installed. \"\n                f\"To install, run `{sys.executable} -m pip install --upgrade psutil`.\"\n            )\n\n        result: dict[str, Any] = dict()\n\n        # first, the platform info.\n        result[\"architecture\"] = platform.machine()\n        result[\"bitness\"] = platform.architecture()[0]\n        result[\"processor\"] = platform.processor()\n        result[\"system\"] = platform.system()\n        result[\"system-version\"] = platform.release()\n\n        try:\n            # The CPU frequency is not available on some ARM devices\n            freq_struct = psutil.cpu_freq()\n            result[\"min_frequency\"] = float(freq_struct.min)\n            result[\"max_frequency\"] = float(freq_struct.max)\n            freq_conversion = self.conversion_table[self.frequnit[0]]\n            # result is in MHz, so we convert to Hz and apply the conversion factor.\n            result[\"frequency\"] = freq_struct.current * 1e6 / freq_conversion\n        except RuntimeError:\n            result[\"frequency\"] = 0.0\n            result[\"min_frequency\"] = 0.0\n            result[\"max_frequency\"] = 0.0\n\n        result[\"frequency_unit\"] = self.frequnit\n        result[\"num_cpus\"] = psutil.cpu_count(logical=False)\n        result[\"num_logical_cpus\"] = psutil.cpu_count()\n\n        mem_struct = psutil.virtual_memory()\n        mem_conversion = self.conversion_table[self.memunit[0]]\n        # result is in bytes, so no need for base conversion.\n        result[\"total_memory\"] = mem_struct.total / mem_conversion\n        result[\"memory_unit\"] = self.memunit\n        return {self.key: result}\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.CPUInfo.key","title":"key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>key = 'cpu'\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.CPUInfo.memunit","title":"memunit  <code>instance-attribute</code>","text":"<pre><code>memunit = memunit\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.CPUInfo.frequnit","title":"frequnit  <code>instance-attribute</code>","text":"<pre><code>frequnit = frequnit\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.CPUInfo.conversion_table","title":"conversion_table  <code>instance-attribute</code>","text":"<pre><code>conversion_table: dict[str, float] = {'k': 1000.0, 'M': 1000000.0, 'G': 1000000000.0}\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.system","title":"system","text":"<pre><code>system() -&gt; dict[str, str]\n</code></pre> Source code in <code>src/nnbench/context.py</code> <pre><code>def system() -&gt; dict[str, str]:\n    return {\"system\": platform.system()}\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.cpuarch","title":"cpuarch","text":"<pre><code>cpuarch() -&gt; dict[str, str]\n</code></pre> Source code in <code>src/nnbench/context.py</code> <pre><code>def cpuarch() -&gt; dict[str, str]:\n    return {\"cpuarch\": platform.machine()}\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.python_version","title":"python_version","text":"<pre><code>python_version() -&gt; dict[str, str]\n</code></pre> Source code in <code>src/nnbench/context.py</code> <pre><code>def python_version() -&gt; dict[str, str]:\n    return {\"python_version\": platform.python_version()}\n</code></pre>"},{"location":"reference/nnbench/context/#nnbench.context.register_context_provider","title":"register_context_provider","text":"<pre><code>register_context_provider(\n    name: str, typ: type[ContextProvider] | ContextProvider, kwargs: Any\n) -&gt; None\n</code></pre> Source code in <code>src/nnbench/context.py</code> <pre><code>def register_context_provider(\n    name: str, typ: type[ContextProvider] | ContextProvider, kwargs: Any\n) -&gt; None:\n    logger.debug(f\"Registering context provider {name!r}\")\n\n    if isinstance(typ, type):\n        # classes can be instantiated with arguments,\n        # while functions cannot.\n        builtin_providers[name] = typ(**kwargs)\n    else:\n        builtin_providers[name] = typ\n</code></pre>"},{"location":"reference/nnbench/core/","title":"core","text":"<p>Data model, registration, and parametrization facilities for defining benchmarks.</p>"},{"location":"reference/nnbench/core/#nnbench.core.benchmark","title":"benchmark","text":"<pre><code>benchmark(\n    func: None = None,\n    name: str = \"\",\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    tags: tuple[str, ...] = (),\n) -&gt; Callable[[Callable], Benchmark]\n</code></pre><pre><code>benchmark(\n    func: Callable[..., Any],\n    name: str = \"\",\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    tags: tuple[str, ...] = (),\n) -&gt; Benchmark\n</code></pre> <pre><code>benchmark(\n    func: Callable[..., Any] | None = None,\n    name: str = \"\",\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    tags: tuple[str, ...] = (),\n) -&gt; Benchmark | Callable[[Callable], Benchmark]\n</code></pre> <p>Define a benchmark from a function.</p> <p>The resulting benchmark can either be completely (i.e., the resulting function takes no more arguments) or incompletely parametrized. In the latter case, the remaining free parameters need to be passed in the calls to <code>nnbench.run()</code>.</p> PARAMETER DESCRIPTION <code>func</code> <p>The function to benchmark. This slot only exists to allow application of the decorator without parentheses, you should never fill it explicitly.</p> <p> TYPE: <code>Callable[..., Any] | None</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>A display name to give to the benchmark. Useful in summaries and reports.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>setUp</code> <p>A setup hook to run before the benchmark.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>tearDown</code> <p>A teardown hook to run after the benchmark.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>tags</code> <p>Additional tags to attach for bookkeeping and selective filtering during runs.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>Benchmark | Callable[[Callable], Benchmark]</code> <p>The resulting benchmark (if no arguments were given), or a parametrized decorator returning the benchmark.</p> Source code in <code>src/nnbench/core.py</code> <pre><code>def benchmark(\n    func: Callable[..., Any] | None = None,\n    name: str = \"\",\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    tags: tuple[str, ...] = (),\n) -&gt; Benchmark | Callable[[Callable], Benchmark]:\n    \"\"\"\n    Define a benchmark from a function.\n\n    The resulting benchmark can either be completely (i.e., the resulting function takes no\n    more arguments) or incompletely parametrized. In the latter case, the remaining free\n    parameters need to be passed in the calls to `nnbench.run()`.\n\n    Parameters\n    ----------\n    func: Callable[..., Any] | None\n        The function to benchmark. This slot only exists to allow application of the decorator\n        without parentheses, you should never fill it explicitly.\n    name: str\n        A display name to give to the benchmark. Useful in summaries and reports.\n    setUp: Callable[..., None]\n        A setup hook to run before the benchmark.\n    tearDown: Callable[..., None]\n        A teardown hook to run after the benchmark.\n    tags: tuple[str, ...]\n        Additional tags to attach for bookkeeping and selective filtering during runs.\n\n    Returns\n    -------\n    Benchmark | Callable[[Callable], Benchmark]\n        The resulting benchmark (if no arguments were given), or a parametrized decorator\n        returning the benchmark.\n    \"\"\"\n\n    def decorator(fun: Callable) -&gt; Benchmark:\n        return Benchmark(fun, name=name, setUp=setUp, tearDown=tearDown, tags=tags)\n\n    if func is not None:\n        return decorator(func)\n    else:\n        return decorator\n</code></pre>"},{"location":"reference/nnbench/core/#nnbench.core.parametrize","title":"parametrize","text":"<pre><code>parametrize(\n    parameters: Iterable[dict[str, Any]],\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    namegen: Callable[..., str] = _default_namegen,\n    tags: tuple[str, ...] = (),\n) -&gt; Callable[[Callable], BenchmarkFamily]\n</code></pre> <p>Define a family of benchmarks over a function with varying parameters.</p> <p>The resulting benchmarks can either be completely (i.e., the resulting function takes no more arguments) or incompletely parametrized. In the latter case, the remaining free parameters need to be passed in the call to <code>nnbench.run()</code>.</p> PARAMETER DESCRIPTION <code>parameters</code> <p>The different sets of parameters defining the benchmark family.</p> <p> TYPE: <code>Iterable[dict[str, Any]]</code> </p> <code>setUp</code> <p>A setup hook to run before each of the benchmarks.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>tearDown</code> <p>A teardown hook to run after each of the benchmarks.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>namegen</code> <p>A function taking the benchmark function and given parameters that generates a unique custom name for the benchmark. The default name generated is the benchmark function's name followed by the keyword arguments in <code>key=value</code> format separated by underscores.</p> <p> TYPE: <code>Callable[..., str]</code> DEFAULT: <code>_default_namegen</code> </p> <code>tags</code> <p>Additional tags to attach for bookkeeping and selective filtering during runs.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>Callable[[Callable], BenchmarkFamily]</code> <p>A parametrized decorator returning the benchmark family.</p> Source code in <code>src/nnbench/core.py</code> <pre><code>def parametrize(\n    parameters: Iterable[dict[str, Any]],\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    namegen: Callable[..., str] = _default_namegen,\n    tags: tuple[str, ...] = (),\n) -&gt; Callable[[Callable], BenchmarkFamily]:\n    \"\"\"\n    Define a family of benchmarks over a function with varying parameters.\n\n    The resulting benchmarks can either be completely (i.e., the resulting function takes no\n    more arguments) or incompletely parametrized. In the latter case, the remaining free\n    parameters need to be passed in the call to `nnbench.run()`.\n\n    Parameters\n    ----------\n    parameters: Iterable[dict[str, Any]]\n        The different sets of parameters defining the benchmark family.\n    setUp: Callable[..., None]\n        A setup hook to run before each of the benchmarks.\n    tearDown: Callable[..., None]\n        A teardown hook to run after each of the benchmarks.\n    namegen: Callable[..., str]\n        A function taking the benchmark function and given parameters that generates a unique\n        custom name for the benchmark. The default name generated is the benchmark function's name\n        followed by the keyword arguments in ``key=value`` format separated by underscores.\n    tags: tuple[str, ...]\n        Additional tags to attach for bookkeeping and selective filtering during runs.\n\n    Returns\n    -------\n    Callable[[Callable], BenchmarkFamily]\n        A parametrized decorator returning the benchmark family.\n    \"\"\"\n\n    def decorator(fn: Callable) -&gt; BenchmarkFamily:\n        return BenchmarkFamily(\n            fn,\n            parameters,\n            name=namegen,\n            setUp=setUp,\n            tearDown=tearDown,\n            tags=tags,\n        )\n\n    return decorator\n</code></pre>"},{"location":"reference/nnbench/core/#nnbench.core.product","title":"product","text":"<pre><code>product(\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    namegen: Callable[..., str] = _default_namegen,\n    tags: tuple[str, ...] = (),\n    **iterables: Iterable\n) -&gt; Callable[[Callable], BenchmarkFamily]\n</code></pre> <p>Define a family of benchmarks over a cartesian product of one or more iterables.</p> <p>The resulting benchmarks can either be completely (i.e., the resulting function takes no more arguments) or incompletely parametrized. In the latter case, the remaining free parameters need to be passed in the call to <code>nnbench.run()</code>.</p> PARAMETER DESCRIPTION <code>setUp</code> <p>A setup hook to run before each of the benchmarks.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>tearDown</code> <p>A teardown hook to run after each of the benchmarks.</p> <p> TYPE: <code>Callable[..., None]</code> DEFAULT: <code>NoOp</code> </p> <code>namegen</code> <p>A function taking the benchmark function and given parameters that generates a unique custom name for the benchmark. The default name generated is the benchmark function's name followed by the keyword arguments in <code>key=value</code> format separated by underscores.</p> <p> TYPE: <code>Callable[..., str]</code> DEFAULT: <code>_default_namegen</code> </p> <code>tags</code> <p>Additional tags to attach for bookkeeping and selective filtering during runs.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>()</code> </p> <code>**iterables</code> <p>The iterables parametrizing the benchmarks.</p> <p> TYPE: <code>Iterable</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Callable[[Callable], BenchmarkFamily]</code> <p>A parametrized decorator returning the benchmark family.</p> Source code in <code>src/nnbench/core.py</code> <pre><code>def product(\n    setUp: Callable[..., None] = NoOp,\n    tearDown: Callable[..., None] = NoOp,\n    namegen: Callable[..., str] = _default_namegen,\n    tags: tuple[str, ...] = (),\n    **iterables: Iterable,\n) -&gt; Callable[[Callable], BenchmarkFamily]:\n    \"\"\"\n    Define a family of benchmarks over a cartesian product of one or more iterables.\n\n    The resulting benchmarks can either be completely (i.e., the resulting function takes no\n    more arguments) or incompletely parametrized. In the latter case, the remaining free\n    parameters need to be passed in the call to `nnbench.run()`.\n\n    Parameters\n    ----------\n    setUp: Callable[..., None]\n        A setup hook to run before each of the benchmarks.\n    tearDown: Callable[..., None]\n        A teardown hook to run after each of the benchmarks.\n    namegen: Callable[..., str]\n        A function taking the benchmark function and given parameters that generates a unique\n        custom name for the benchmark. The default name generated is the benchmark function's name\n        followed by the keyword arguments in ``key=value`` format separated by underscores.\n    tags: tuple[str, ...]\n        Additional tags to attach for bookkeeping and selective filtering during runs.\n    **iterables: Iterable\n        The iterables parametrizing the benchmarks.\n\n    Returns\n    -------\n    Callable[[Callable], BenchmarkFamily]\n        A parametrized decorator returning the benchmark family.\n    \"\"\"\n\n    def decorator(fn: Callable) -&gt; BenchmarkFamily:\n        names, values = iterables.keys(), iterables.values()\n\n        # NB: This line forces the exhaustion of all input iterables by nature of the\n        # cartesian product (values need to be persisted to be accessed multiple times).\n        parameters = (dict(zip(names, vals)) for vals in itertools.product(*values))\n\n        return BenchmarkFamily(\n            fn,\n            parameters,\n            name=namegen,\n            setUp=setUp,\n            tearDown=tearDown,\n            tags=tags,\n        )\n\n    return decorator\n</code></pre>"},{"location":"reference/nnbench/fixtures/","title":"fixtures","text":"<p>Collect values ('fixtures') by name for benchmark runs from certain files, similarly to pytest and its <code>conftest.py</code>.</p>"},{"location":"reference/nnbench/fixtures/#nnbench.fixtures.FixtureManager","title":"FixtureManager","text":"<p>A lean class responsible for resolving parameter values (aka 'fixtures') of benchmarks from provider functions.</p> <p>To resolve a benchmark parameter (in <code>FixtureManager.resolve()</code>), the class does the following:</p> <pre><code>1. Obtain the path to the file containing the benchmark, as\nthe ``__file__`` attribute of the benchmark function's origin module.\n\n2. Look for a `conf.py` file in the same directory.\n\n3. Import the `conf.py` module, look for a function named the same as\nthe benchmark parameter.\n\n4. If necessary, resolve any named inputs to the function **within**\nthe module scope.\n\n5. If no function member is found, and the benchmark file is not in `root`,\nfall back to the parent directory, repeat steps 2-5, until `root` is reached.\n\n6. If no `conf.py` contains any function matching the name, throw an\nerror.\n</code></pre> Source code in <code>src/nnbench/fixtures.py</code> <pre><code>class FixtureManager:\n    \"\"\"\n    A lean class responsible for resolving parameter values (aka 'fixtures')\n    of benchmarks from provider functions.\n\n    To resolve a benchmark parameter (in ``FixtureManager.resolve()``), the class\n    does the following:\n\n        1. Obtain the path to the file containing the benchmark, as\n        the ``__file__`` attribute of the benchmark function's origin module.\n\n        2. Look for a `conf.py` file in the same directory.\n\n        3. Import the `conf.py` module, look for a function named the same as\n        the benchmark parameter.\n\n        4. If necessary, resolve any named inputs to the function **within**\n        the module scope.\n\n        5. If no function member is found, and the benchmark file is not in `root`,\n        fall back to the parent directory, repeat steps 2-5, until `root` is reached.\n\n        6. If no `conf.py` contains any function matching the name, throw an\n        error.\n    \"\"\"\n\n    def __init__(self, root: str | os.PathLike[str]) -&gt; None:\n        self.root = Path(root)\n        self.cache: dict[Path, dict[str, Any]] = {}\n        \"\"\"\n        Cache architecture:\n        key: directory\n        value: key-value mapping of fixture name -&gt; fixture value within directory.\n        \"\"\"\n\n    def collect(self, mod: ModuleType, names: Iterable[str]) -&gt; dict[str, Any]:\n        \"\"\"\n        Given a module containing fixtures (contents of a ``conf.py`` file imported\n        as a module), and a list of required fixture names (for a\n        selected benchmark), collect values, computing transitive closures in the\n        process (i.e., all inputs required to compute the set of fixtures).\n\n        Parameters\n        ----------\n        mod: ModuleType\n            The module to import fixture values from.\n        names: Iterable[str]\n            Names of fixture values to compute and use in the invoking benchmark.\n\n        Returns\n        -------\n        dict[str, Any]\n            The mapping of fixture names to their values.\n        \"\"\"\n        res: dict[str, Any] = {}\n        for name in names:\n            fixture_cand = getattr(mod, name, None)\n            if fixture_cand is None:\n                continue\n            else:\n                closure, interfaces = get_transitive_closure(mod, name)\n                # easy case first, fixture without arguments - just call the function.\n                if len(closure) == 1:\n                    (fn,) = closure\n                    res[name] = fn()\n                else:\n                    # compute the closure in reverse to get the fixture value.\n                    # the last fixture takes no arguments, otherwise this would\n                    # be an infinite loop.\n                    idx = -1\n                    _temp_res: dict[str, Any] = {}\n                    for iface in reversed(interfaces):\n                        iface_names = iface.names\n                        # each fixture can take values in the respective closure,\n                        # but only values that have already been computed.\n                        kwargs = {k: v for k, v in _temp_res.items() if k in iface_names}\n                        fn = closure[idx]\n                        _temp_res[iface.funcname] = fn(**kwargs)\n                        idx -= 1\n                    assert name in _temp_res, f\"internal error computing fixture {name!r}\"\n                    res[name] = _temp_res[name]\n        return res\n\n    def resolve(self, bm: Benchmark) -&gt; dict[str, Any]:\n        \"\"\"\n        Resolve fixture values for a benchmark.\n\n        Fixtures will be resolved only for benchmark inputs that do not have a\n        default value in place in the interface.\n\n        Fixtures need to be functions in a ``conf.py`` module in the benchmark\n        directory structure, and must *exactly* match input parameters by name.\n\n        Parameters\n        ----------\n        bm: Benchmark\n            The benchmark to resolve fixtures for.\n\n        Returns\n        -------\n        dict[str, Any]\n            The mapping of fixture values to use for the given benchmark.\n        \"\"\"\n        fixturevals: dict[str, Any] = {}\n        # first, get the candidate fixture names, aka the benchmark param names.\n        # We limit ourselves to names that do not have a default value.\n        names = [\n            n\n            for n, d in zip(bm.interface.names, bm.interface.defaults)\n            if d is inspect.Parameter.empty\n        ]\n        nameset, fixtureset = set(names), set()\n        # then, load the benchmark function's origin module,\n        # should be fast as it's a sys.modules lookup.\n        # Each user module loaded via spec_from_file_location *should* have its\n        # __file__ attribute set, so inspect.getsourcefile can find it.\n        bm_origin_module = inspect.getmodule(bm.fn)\n        sourcefile = inspect.getsourcefile(bm_origin_module)\n\n        if sourcefile is None:\n            raise ValueError(\n                \"during fixture collection: \"\n                f\"could not locate origin module for benchmark {bm.name!r}()\"\n            )\n\n        # then, look for a `conf.py` file in the benchmark file's directory,\n        # and all parents up to \"root\" (i.e., the path in `nnbench run &lt;path&gt;`)\n        bm_file = Path(sourcefile)\n        for p in bm_file.parents:\n            conf_candidate = p / \"conf.py\"\n            if conf_candidate.exists():\n                bm_dir_cache: dict[str, Any] | None = self.cache.get(p, None)\n                if bm_dir_cache is None:\n                    mod = import_file_as_module(conf_candidate)\n                    # contains fixture values for the benchmark that could be resolved\n                    # on the current directory level.\n                    res = self.collect(mod, names)\n                    # hydrate the directory cache with the found fixture values.\n                    # some might be missing, so we could have to continue traversal.\n                    self.cache[p] = res\n                    fixturevals.update(res)\n                else:\n                    # at this point, the cache entry might have other fixture values\n                    # that this benchmark may not consume, so we need to filter.\n                    fixturevals.update({k: v for k, v in bm_dir_cache.items() if k in names})\n\n                fixtureset |= set(fixturevals)\n\n            if p == self.root or nameset == fixtureset:\n                break\n\n        # TODO: This should not throw an error, inline the typecheck into before benchmark\n        # execution, then handle it there.\n        # if fixtureset &lt; nameset:\n        #     missing, *_ = nameset - fixtureset\n        #     raise RuntimeError(f\"could not locate fixture {missing!r} for benchmark {bm.name!r}\")\n\n        return fixturevals\n</code></pre>"},{"location":"reference/nnbench/fixtures/#nnbench.fixtures.FixtureManager.root","title":"root  <code>instance-attribute</code>","text":"<pre><code>root = Path(root)\n</code></pre>"},{"location":"reference/nnbench/fixtures/#nnbench.fixtures.FixtureManager.cache","title":"cache  <code>instance-attribute</code>","text":"<pre><code>cache: dict[Path, dict[str, Any]] = {}\n</code></pre> <p>Cache architecture: key: directory value: key-value mapping of fixture name -&gt; fixture value within directory.</p>"},{"location":"reference/nnbench/fixtures/#nnbench.fixtures.FixtureManager.collect","title":"collect","text":"<pre><code>collect(mod: ModuleType, names: Iterable[str]) -&gt; dict[str, Any]\n</code></pre> <p>Given a module containing fixtures (contents of a <code>conf.py</code> file imported as a module), and a list of required fixture names (for a selected benchmark), collect values, computing transitive closures in the process (i.e., all inputs required to compute the set of fixtures).</p> PARAMETER DESCRIPTION <code>mod</code> <p>The module to import fixture values from.</p> <p> TYPE: <code>ModuleType</code> </p> <code>names</code> <p>Names of fixture values to compute and use in the invoking benchmark.</p> <p> TYPE: <code>Iterable[str]</code> </p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>The mapping of fixture names to their values.</p> Source code in <code>src/nnbench/fixtures.py</code> <pre><code>def collect(self, mod: ModuleType, names: Iterable[str]) -&gt; dict[str, Any]:\n    \"\"\"\n    Given a module containing fixtures (contents of a ``conf.py`` file imported\n    as a module), and a list of required fixture names (for a\n    selected benchmark), collect values, computing transitive closures in the\n    process (i.e., all inputs required to compute the set of fixtures).\n\n    Parameters\n    ----------\n    mod: ModuleType\n        The module to import fixture values from.\n    names: Iterable[str]\n        Names of fixture values to compute and use in the invoking benchmark.\n\n    Returns\n    -------\n    dict[str, Any]\n        The mapping of fixture names to their values.\n    \"\"\"\n    res: dict[str, Any] = {}\n    for name in names:\n        fixture_cand = getattr(mod, name, None)\n        if fixture_cand is None:\n            continue\n        else:\n            closure, interfaces = get_transitive_closure(mod, name)\n            # easy case first, fixture without arguments - just call the function.\n            if len(closure) == 1:\n                (fn,) = closure\n                res[name] = fn()\n            else:\n                # compute the closure in reverse to get the fixture value.\n                # the last fixture takes no arguments, otherwise this would\n                # be an infinite loop.\n                idx = -1\n                _temp_res: dict[str, Any] = {}\n                for iface in reversed(interfaces):\n                    iface_names = iface.names\n                    # each fixture can take values in the respective closure,\n                    # but only values that have already been computed.\n                    kwargs = {k: v for k, v in _temp_res.items() if k in iface_names}\n                    fn = closure[idx]\n                    _temp_res[iface.funcname] = fn(**kwargs)\n                    idx -= 1\n                assert name in _temp_res, f\"internal error computing fixture {name!r}\"\n                res[name] = _temp_res[name]\n    return res\n</code></pre>"},{"location":"reference/nnbench/fixtures/#nnbench.fixtures.FixtureManager.resolve","title":"resolve","text":"<pre><code>resolve(bm: Benchmark) -&gt; dict[str, Any]\n</code></pre> <p>Resolve fixture values for a benchmark.</p> <p>Fixtures will be resolved only for benchmark inputs that do not have a default value in place in the interface.</p> <p>Fixtures need to be functions in a <code>conf.py</code> module in the benchmark directory structure, and must exactly match input parameters by name.</p> PARAMETER DESCRIPTION <code>bm</code> <p>The benchmark to resolve fixtures for.</p> <p> TYPE: <code>Benchmark</code> </p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>The mapping of fixture values to use for the given benchmark.</p> Source code in <code>src/nnbench/fixtures.py</code> <pre><code>def resolve(self, bm: Benchmark) -&gt; dict[str, Any]:\n    \"\"\"\n    Resolve fixture values for a benchmark.\n\n    Fixtures will be resolved only for benchmark inputs that do not have a\n    default value in place in the interface.\n\n    Fixtures need to be functions in a ``conf.py`` module in the benchmark\n    directory structure, and must *exactly* match input parameters by name.\n\n    Parameters\n    ----------\n    bm: Benchmark\n        The benchmark to resolve fixtures for.\n\n    Returns\n    -------\n    dict[str, Any]\n        The mapping of fixture values to use for the given benchmark.\n    \"\"\"\n    fixturevals: dict[str, Any] = {}\n    # first, get the candidate fixture names, aka the benchmark param names.\n    # We limit ourselves to names that do not have a default value.\n    names = [\n        n\n        for n, d in zip(bm.interface.names, bm.interface.defaults)\n        if d is inspect.Parameter.empty\n    ]\n    nameset, fixtureset = set(names), set()\n    # then, load the benchmark function's origin module,\n    # should be fast as it's a sys.modules lookup.\n    # Each user module loaded via spec_from_file_location *should* have its\n    # __file__ attribute set, so inspect.getsourcefile can find it.\n    bm_origin_module = inspect.getmodule(bm.fn)\n    sourcefile = inspect.getsourcefile(bm_origin_module)\n\n    if sourcefile is None:\n        raise ValueError(\n            \"during fixture collection: \"\n            f\"could not locate origin module for benchmark {bm.name!r}()\"\n        )\n\n    # then, look for a `conf.py` file in the benchmark file's directory,\n    # and all parents up to \"root\" (i.e., the path in `nnbench run &lt;path&gt;`)\n    bm_file = Path(sourcefile)\n    for p in bm_file.parents:\n        conf_candidate = p / \"conf.py\"\n        if conf_candidate.exists():\n            bm_dir_cache: dict[str, Any] | None = self.cache.get(p, None)\n            if bm_dir_cache is None:\n                mod = import_file_as_module(conf_candidate)\n                # contains fixture values for the benchmark that could be resolved\n                # on the current directory level.\n                res = self.collect(mod, names)\n                # hydrate the directory cache with the found fixture values.\n                # some might be missing, so we could have to continue traversal.\n                self.cache[p] = res\n                fixturevals.update(res)\n            else:\n                # at this point, the cache entry might have other fixture values\n                # that this benchmark may not consume, so we need to filter.\n                fixturevals.update({k: v for k, v in bm_dir_cache.items() if k in names})\n\n            fixtureset |= set(fixturevals)\n\n        if p == self.root or nameset == fixtureset:\n            break\n\n    # TODO: This should not throw an error, inline the typecheck into before benchmark\n    # execution, then handle it there.\n    # if fixtureset &lt; nameset:\n    #     missing, *_ = nameset - fixtureset\n    #     raise RuntimeError(f\"could not locate fixture {missing!r} for benchmark {bm.name!r}\")\n\n    return fixturevals\n</code></pre>"},{"location":"reference/nnbench/fixtures/#nnbench.fixtures.get_transitive_closure","title":"get_transitive_closure","text":"<pre><code>get_transitive_closure(mod: ModuleType, name: str) -&gt; tuple[list[Callable], list[Interface]]\n</code></pre> Source code in <code>src/nnbench/fixtures.py</code> <pre><code>def get_transitive_closure(mod: ModuleType, name: str) -&gt; tuple[list[Callable], list[Interface]]:\n    fixture = getattr(mod, name)\n    if not callable(fixture):\n        raise ValueError(f\"fixture input {name!r} needs to be a callable\")\n    closure: list[Callable] = [fixture]\n    interfaces: list[Interface] = []\n\n    def recursive_closure_collection(fn, _closure):\n        _if = Interface.from_callable(fn, {})\n        interfaces.append(_if)\n        # if the fixture itself takes arguments,\n        # resolve all of them within the module.\n        for closure_name in _if.names:\n            _closure_obj = getattr(mod, closure_name, None)\n            if _closure_obj is None:\n                raise ImportError(f\"fixture {name!r}: missing closure value {closure_name!r}\")\n            if not callable(_closure_obj):\n                raise ValueError(f\"input {name!r} to fixture {fn} needs to be a callable\")\n            _closure.append(_closure_obj)\n            recursive_closure_collection(_closure_obj, _closure)\n\n    recursive_closure_collection(fixture, closure)\n    return closure, interfaces\n</code></pre>"},{"location":"reference/nnbench/runner/","title":"runner","text":"<p>The abstract benchmark runner interface, which can be overridden for custom benchmark workloads.</p>"},{"location":"reference/nnbench/runner/#nnbench.runner.Benchmarkable","title":"Benchmarkable  <code>module-attribute</code>","text":"<pre><code>Benchmarkable = Benchmark | BenchmarkFamily\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger('nnbench.runner')\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.jsonify","title":"jsonify","text":"<pre><code>jsonify(params: dict[str, Any], repr_hooks: dict[type, Callable] | None = None) -&gt; dict[str, Any]\n</code></pre> <p>Construct a JSON representation of benchmark parameters.</p> <p>This is necessary to break reference cycles from the parameters to the records, which prevent garbage collection of memory-intensive values.</p> PARAMETER DESCRIPTION <code>params</code> <p>Benchmark parameters to compute a JSON representation of.</p> <p> TYPE: <code>dict[str, Any]</code> </p> <code>repr_hooks</code> <p>A dictionary mapping parameter types to functions returning a JSON representation of an instance of the type. Allows fine-grained control to achieve lossless, reproducible serialization of input parameter information.</p> <p> TYPE: <code>dict[type, Callable] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>A JSON-serializable representation of the benchmark input parameters.</p> Source code in <code>src/nnbench/runner.py</code> <pre><code>def jsonify(\n    params: dict[str, Any], repr_hooks: dict[type, Callable] | None = None\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Construct a JSON representation of benchmark parameters.\n\n    This is necessary to break reference cycles from the parameters to the records,\n    which prevent garbage collection of memory-intensive values.\n\n    Parameters\n    ----------\n    params: dict[str, Any]\n        Benchmark parameters to compute a JSON representation of.\n    repr_hooks: dict[type, Callable] | None\n        A dictionary mapping parameter types to functions returning a JSON representation\n        of an instance of the type. Allows fine-grained control to achieve lossless,\n        reproducible serialization of input parameter information.\n\n    Returns\n    -------\n    dict[str, Any]\n        A JSON-serializable representation of the benchmark input parameters.\n    \"\"\"\n    repr_hooks = repr_hooks or {}\n    natives = (float, int, str, bool, bytes, complex)\n    json_params: dict[str, Any] = {}\n\n    def _jsonify(val):\n        vtype = type(val)\n        if vtype in repr_hooks:\n            return repr_hooks[vtype](val)\n        if isinstance(val, natives):\n            return val\n        elif hasattr(val, \"to_json\"):\n            try:\n                return val.to_json()\n            except TypeError:\n                # if to_json() needs arguments, we're SOL.\n                pass\n\n        return repr(val)\n\n    for k, v in params.items():\n        if isinstance(v, tuple | list | set | frozenset):\n            container_type = type(v)\n            json_params[k] = container_type(map(_jsonify, v))\n        elif isinstance(v, dict):\n            json_params[k] = jsonify(v)\n        else:\n            json_params[k] = _jsonify(v)\n    return json_params\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.collect","title":"collect","text":"<pre><code>collect(\n    path_or_module: str | PathLike[str], tags: tuple[str, ...] = ()\n) -&gt; list[Benchmark | BenchmarkFamily]\n</code></pre> <p>Discover benchmarks in a module or source file.</p> PARAMETER DESCRIPTION <code>path_or_module</code> <p>Name or path of the module to discover benchmarks in. Can also be a directory, in which case benchmarks are collected from the Python files therein.</p> <p> TYPE: <code>str | PathLike[str]</code> </p> <code>tags</code> <p>Tags to filter for when collecting benchmarks. Only benchmarks containing either of these tags are collected.</p> <p> TYPE: <code>tuple[str, ...]</code> DEFAULT: <code>()</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the given path is not a Python file, directory, or module name.</p> Source code in <code>src/nnbench/runner.py</code> <pre><code>def collect(\n    path_or_module: str | os.PathLike[str], tags: tuple[str, ...] = ()\n) -&gt; list[Benchmark | BenchmarkFamily]:\n    # TODO: functools.cache this guy\n    \"\"\"\n    Discover benchmarks in a module or source file.\n\n    Parameters\n    ----------\n    path_or_module: str | os.PathLike[str]\n        Name or path of the module to discover benchmarks in. Can also be a directory,\n        in which case benchmarks are collected from the Python files therein.\n    tags: tuple[str, ...]\n        Tags to filter for when collecting benchmarks. Only benchmarks containing either of\n        these tags are collected.\n\n    Raises\n    ------\n    ValueError\n        If the given path is not a Python file, directory, or module name.\n    \"\"\"\n    benchmarks: list[Benchmark] = []\n    ppath = Path(path_or_module)\n    if ppath.is_dir():\n        pythonpaths = all_python_files(ppath)\n        for py in pythonpaths:\n            benchmarks.extend(collect(py, tags))\n        return benchmarks\n    elif ppath.is_file():\n        logger.debug(f\"Collecting benchmarks from file {ppath}.\")\n        module = import_file_as_module(path_or_module)\n    elif exists_module(path_or_module):\n        module = sys.modules[str(path_or_module)]\n    else:\n        raise ValueError(\n            f\"expected a module name, Python file, or directory, got {str(path_or_module)!r}\"\n        )\n\n    # iterate through the module dict members to register\n    for k, v in module.__dict__.items():\n        if k.startswith(\"__\") and k.endswith(\"__\"):\n            # dunder names are ignored.\n            continue\n        elif isinstance(v, Benchmarkable):\n            if not tags or set(tags) &amp; set(v.tags):\n                benchmarks.append(v)\n        elif isinstance(v, list | tuple | set | frozenset):\n            for bm in v:\n                if isinstance(bm, Benchmarkable):\n                    if not tags or set(tags) &amp; set(bm.tags):\n                        benchmarks.append(bm)\n    return benchmarks\n</code></pre>"},{"location":"reference/nnbench/runner/#nnbench.runner.run","title":"run","text":"<pre><code>run(\n    benchmarks: Benchmark | BenchmarkFamily | Iterable[Benchmark | BenchmarkFamily],\n    name: str | None = None,\n    params: dict[str, Any] | Parameters | None = None,\n    context: Context | Iterable[ContextProvider] = (),\n    jsonifier: Callable[[dict[str, Any]], dict[str, Any]] = jsonify,\n) -&gt; BenchmarkRecord\n</code></pre> <p>Run a previously collected benchmark workload.</p> PARAMETER DESCRIPTION <code>benchmarks</code> <p>A benchmark, family of benchmarks, or collection of discovered benchmarks to run.</p> <p> TYPE: <code>Benchmark | BenchmarkFamily | Iterable[Benchmark | BenchmarkFamily]</code> </p> <code>name</code> <p>A name for the currently started run. If None, a name will be automatically generated.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>params</code> <p>Parameters to use for the benchmark run. Names have to match positional and keyword argument names of the benchmark functions.</p> <p> TYPE: <code>dict[str, Any] | Parameters | None</code> DEFAULT: <code>None</code> </p> <code>context</code> <p>Additional context to log with the benchmarks in the output JSON record. Useful for obtaining environment information and configuration, like CPU/GPU hardware info, ML model metadata, and more.</p> <p> TYPE: <code>Context | Iterable[ContextProvider]</code> DEFAULT: <code>()</code> </p> <code>jsonifier</code> <p>A function constructing a string representation from the input parameters. Defaults to <code>nnbench.runner.jsonify_params()</code>. Must produce a dictionary containing only JSON-serializable values.</p> <p> TYPE: <code>Callable[[dict[str, Any]], dict[str, Any]]</code> DEFAULT: <code>jsonify</code> </p> RETURNS DESCRIPTION <code>BenchmarkRecord</code> <p>A JSON output representing the benchmark results. Has three top-level keys, \"name\" giving the benchmark run name, \"context\" holding the context information, and \"benchmarks\", holding an array with the benchmark results.</p> Source code in <code>src/nnbench/runner.py</code> <pre><code>def run(\n    benchmarks: Benchmark | BenchmarkFamily | Iterable[Benchmark | BenchmarkFamily],\n    name: str | None = None,\n    params: dict[str, Any] | Parameters | None = None,\n    context: Context | Iterable[ContextProvider] = (),\n    jsonifier: Callable[[dict[str, Any]], dict[str, Any]] = jsonify,\n) -&gt; BenchmarkRecord:\n    \"\"\"\n    Run a previously collected benchmark workload.\n\n    Parameters\n    ----------\n    benchmarks: Benchmark | BenchmarkFamily | Iterable[Benchmark | BenchmarkFamily]\n        A benchmark, family of benchmarks, or collection of discovered benchmarks to run.\n    name: str | None\n        A name for the currently started run. If None, a name will be automatically generated.\n    params: dict[str, Any] | Parameters | None\n        Parameters to use for the benchmark run. Names have to match positional and keyword\n        argument names of the benchmark functions.\n    context: Iterable[ContextProvider]\n        Additional context to log with the benchmarks in the output JSON record. Useful for\n        obtaining environment information and configuration, like CPU/GPU hardware info,\n        ML model metadata, and more.\n    jsonifier: Callable[[dict[str, Any], dict[str, Any]]]\n        A function constructing a string representation from the input parameters.\n        Defaults to ``nnbench.runner.jsonify_params()``. Must produce a dictionary containing\n        only JSON-serializable values.\n\n    Returns\n    -------\n    BenchmarkRecord\n        A JSON output representing the benchmark results. Has three top-level keys,\n        \"name\" giving the benchmark run name, \"context\" holding the context information,\n        and \"benchmarks\", holding an array with the benchmark results.\n    \"\"\"\n\n    _run = name or \"nnbench-\" + platform.node() + \"-\" + uuid.uuid1().hex[:8]\n\n    family_sizes: dict[str, Any] = collections.defaultdict(int)\n    family_indices: dict[str, Any] = collections.defaultdict(int)\n\n    if isinstance(context, dict):\n        ctx = context\n    else:\n        ctx = dict()\n        for provider in context:\n            val = provider()\n            duplicates = set(ctx.keys()) &amp; set(val.keys())\n            if duplicates:\n                dupe, *_ = duplicates\n                raise ValueError(f\"got multiple values for context key {dupe!r}\")\n            ctx.update(val)\n\n    if isinstance(benchmarks, Benchmarkable):\n        benchmarks = [benchmarks]\n\n    if isinstance(params, Parameters):\n        dparams = asdict(params)\n    else:\n        dparams = params or {}\n\n    results: list[dict[str, Any]] = []\n\n    for benchmark in collapse(benchmarks):\n        bm_family = benchmark.interface.funcname\n        state = State(\n            name=benchmark.name,\n            family=bm_family,\n            family_size=family_sizes[bm_family],\n            family_index=family_indices[bm_family],\n        )\n        family_indices[bm_family] += 1\n\n        # Assemble benchmark parameters. First grab all defaults from the interface,\n        bmparams = {\n            name: val\n            for name, _, val in benchmark.interface.variables\n            if val is not inspect.Parameter.empty\n        }\n        # ... then hydrate with the appropriate subset of input parameters.\n        bmparams |= {k: v for k, v in dparams.items() if k in benchmark.interface.names}\n        # If any arguments are still unresolved, go look them up as fixtures.\n        if set(bmparams) &lt; set(benchmark.interface.names):\n            # TODO: This breaks for a module name (like __main__).\n            # Since that only means that we cannot resolve fixtures when benchmarking\n            # a module name (which makes sense), and we can always pass extra\n            # parameters in the module case, fixing this is not as urgent.\n            mod = benchmark.__module__\n            file = sys.modules[mod].__file__\n            p = Path(file).parent\n            fm = FixtureManager(p)\n            bmparams |= fm.resolve(benchmark)\n\n        res: dict[str, Any] = {\n            \"name\": benchmark.name,\n            \"function\": qualname(benchmark.fn),\n            \"description\": benchmark.fn.__doc__ or \"\",\n            \"date\": datetime.now().isoformat(timespec=\"seconds\"),\n            \"error_occurred\": False,\n            \"error_message\": \"\",\n            \"parameters\": jsonifier(bmparams),\n        }\n        try:\n            benchmark.setUp(state, bmparams)\n            with timer(res):\n                res[\"value\"] = benchmark.fn(**bmparams)\n        except Exception as e:\n            res[\"error_occurred\"] = True\n            res[\"error_message\"] = str(e)\n        finally:\n            benchmark.tearDown(state, bmparams)\n            results.append(res)\n\n    return BenchmarkRecord(\n        run=_run,\n        context=ctx,\n        benchmarks=results,\n    )\n</code></pre>"},{"location":"reference/nnbench/types/","title":"types","text":"<p>Types for benchmarks and records holding results of a run.</p>"},{"location":"reference/nnbench/types/#nnbench.types.T","title":"T  <code>module-attribute</code>","text":"<pre><code>T = TypeVar('T')\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Variable","title":"Variable  <code>module-attribute</code>","text":"<pre><code>Variable = tuple[str, type, Any]\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.State","title":"State  <code>dataclass</code>","text":"<p>A dataclass holding some basic information about a benchmark and its hierarchy inside its family (i.e. a series of the same benchmark for different parameters).</p> <p>For benchmarks registered with <code>@nnbench.benchmark</code>, meaning no parametrization, each benchmark constitutes its own family, and <code>family_size == 1</code> holds true.</p> Source code in <code>src/nnbench/types.py</code> <pre><code>@dataclass(frozen=True)\nclass State:\n    \"\"\"\n    A dataclass holding some basic information about a benchmark and its hierarchy\n    inside its *family* (i.e. a series of the same benchmark for different parameters).\n\n    For benchmarks registered with ``@nnbench.benchmark``, meaning no parametrization,\n    each benchmark constitutes its own family, and ``family_size == 1`` holds true.\n    \"\"\"\n\n    name: str\n    family: str\n    family_size: int\n    family_index: int\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.State.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.State.family","title":"family  <code>instance-attribute</code>","text":"<pre><code>family: str\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.State.family_size","title":"family_size  <code>instance-attribute</code>","text":"<pre><code>family_size: int\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.State.family_index","title":"family_index  <code>instance-attribute</code>","text":"<pre><code>family_index: int\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.BenchmarkRecord","title":"BenchmarkRecord  <code>dataclass</code>","text":"<p>A dataclass representing the result of a benchmark run, i.e. the return value of a call to <code>nnbench.run()</code>.</p> Source code in <code>src/nnbench/types.py</code> <pre><code>@dataclass(frozen=True)\nclass BenchmarkRecord:\n    \"\"\"\n    A dataclass representing the result of a benchmark run, i.e. the return value\n    of a call to ``nnbench.run()``.\n    \"\"\"\n\n    run: str\n    \"\"\"A name describing the run.\"\"\"\n    context: dict[str, Any]\n    \"\"\"A map of key-value pairs describing context information around the benchmark run.\"\"\"\n    benchmarks: list[dict[str, Any]]\n    \"\"\"The list of benchmark results, each given as a Python dictionary.\"\"\"\n\n    def to_json(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Export a benchmark record to JSON.\n\n        Returns\n        -------\n        dict[str, Any]\n            A JSON representation of the benchmark record.\n        \"\"\"\n        return asdict(self)\n\n    def to_list(self) -&gt; list[dict[str, Any]]:\n        \"\"\"\n        Export a benchmark record to a list of individual results,\n        each with the benchmark run name and context inlined.\n        \"\"\"\n        results = []\n        for b in self.benchmarks:\n            bc = copy.deepcopy(b)\n            bc[\"context\"] = self.context\n            bc[\"run\"] = self.run\n            results.append(bc)\n        return results\n\n    @classmethod\n    def expand(cls, bms: dict[str, Any] | list[dict[str, Any]]) -&gt; Self:\n        \"\"\"\n        Expand a list of deserialized JSON-like objects into a benchmark record.\n        This is equivalent to extracting the context given by the method it was\n        serialized with, and then returning the rest of the data as is.\n\n        Parameters\n        ----------\n        bms: dict[str, Any] | list[dict[str, Any]]\n            The deserialized benchmark record or list of records to expand into a record.\n\n        Returns\n        -------\n        BenchmarkRecord\n            The resulting record, with the context and run name extracted.\n        \"\"\"\n        context: dict[str, Any]\n        if isinstance(bms, dict):\n            if \"benchmarks\" not in bms.keys():\n                raise ValueError(f\"no benchmark data found in struct {bms}\")\n\n            benchmarks = bms[\"benchmarks\"]\n            context = bms.get(\"context\", {})\n            run = bms.get(\"run\", \"\")\n        else:\n            run = \"\"\n            context = {}\n            benchmarks = bms\n            for b in benchmarks:\n                # TODO(nicholasjng): This does not do the right thing if the list contains\n                #  data from multiple benchmark runs, e.g. from a DB query.\n                if \"run\" in b:\n                    run = b.pop(\"run\")\n                if \"context\" in b:\n                    # TODO: Log context key/value disagreements\n                    context |= b.pop(\"context\", {})\n        return cls(run=run, benchmarks=benchmarks, context=context)\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.BenchmarkRecord.run","title":"run  <code>instance-attribute</code>","text":"<pre><code>run: str\n</code></pre> <p>A name describing the run.</p>"},{"location":"reference/nnbench/types/#nnbench.types.BenchmarkRecord.context","title":"context  <code>instance-attribute</code>","text":"<pre><code>context: dict[str, Any]\n</code></pre> <p>A map of key-value pairs describing context information around the benchmark run.</p>"},{"location":"reference/nnbench/types/#nnbench.types.BenchmarkRecord.benchmarks","title":"benchmarks  <code>instance-attribute</code>","text":"<pre><code>benchmarks: list[dict[str, Any]]\n</code></pre> <p>The list of benchmark results, each given as a Python dictionary.</p>"},{"location":"reference/nnbench/types/#nnbench.types.BenchmarkRecord.to_json","title":"to_json","text":"<pre><code>to_json() -&gt; dict[str, Any]\n</code></pre> <p>Export a benchmark record to JSON.</p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>A JSON representation of the benchmark record.</p> Source code in <code>src/nnbench/types.py</code> <pre><code>def to_json(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Export a benchmark record to JSON.\n\n    Returns\n    -------\n    dict[str, Any]\n        A JSON representation of the benchmark record.\n    \"\"\"\n    return asdict(self)\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.BenchmarkRecord.to_list","title":"to_list","text":"<pre><code>to_list() -&gt; list[dict[str, Any]]\n</code></pre> <p>Export a benchmark record to a list of individual results, each with the benchmark run name and context inlined.</p> Source code in <code>src/nnbench/types.py</code> <pre><code>def to_list(self) -&gt; list[dict[str, Any]]:\n    \"\"\"\n    Export a benchmark record to a list of individual results,\n    each with the benchmark run name and context inlined.\n    \"\"\"\n    results = []\n    for b in self.benchmarks:\n        bc = copy.deepcopy(b)\n        bc[\"context\"] = self.context\n        bc[\"run\"] = self.run\n        results.append(bc)\n    return results\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.BenchmarkRecord.expand","title":"expand  <code>classmethod</code>","text":"<pre><code>expand(bms: dict[str, Any] | list[dict[str, Any]]) -&gt; Self\n</code></pre> <p>Expand a list of deserialized JSON-like objects into a benchmark record. This is equivalent to extracting the context given by the method it was serialized with, and then returning the rest of the data as is.</p> PARAMETER DESCRIPTION <code>bms</code> <p>The deserialized benchmark record or list of records to expand into a record.</p> <p> TYPE: <code>dict[str, Any] | list[dict[str, Any]]</code> </p> RETURNS DESCRIPTION <code>BenchmarkRecord</code> <p>The resulting record, with the context and run name extracted.</p> Source code in <code>src/nnbench/types.py</code> <pre><code>@classmethod\ndef expand(cls, bms: dict[str, Any] | list[dict[str, Any]]) -&gt; Self:\n    \"\"\"\n    Expand a list of deserialized JSON-like objects into a benchmark record.\n    This is equivalent to extracting the context given by the method it was\n    serialized with, and then returning the rest of the data as is.\n\n    Parameters\n    ----------\n    bms: dict[str, Any] | list[dict[str, Any]]\n        The deserialized benchmark record or list of records to expand into a record.\n\n    Returns\n    -------\n    BenchmarkRecord\n        The resulting record, with the context and run name extracted.\n    \"\"\"\n    context: dict[str, Any]\n    if isinstance(bms, dict):\n        if \"benchmarks\" not in bms.keys():\n            raise ValueError(f\"no benchmark data found in struct {bms}\")\n\n        benchmarks = bms[\"benchmarks\"]\n        context = bms.get(\"context\", {})\n        run = bms.get(\"run\", \"\")\n    else:\n        run = \"\"\n        context = {}\n        benchmarks = bms\n        for b in benchmarks:\n            # TODO(nicholasjng): This does not do the right thing if the list contains\n            #  data from multiple benchmark runs, e.g. from a DB query.\n            if \"run\" in b:\n                run = b.pop(\"run\")\n            if \"context\" in b:\n                # TODO: Log context key/value disagreements\n                context |= b.pop(\"context\", {})\n    return cls(run=run, benchmarks=benchmarks, context=context)\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Parameters","title":"Parameters  <code>dataclass</code>","text":"<p>A dataclass designed to hold benchmark parameters.</p> <p>This class is not functional on its own, and needs to be subclassed according to your benchmarking workloads.</p> <p>The main advantage over passing parameters as a dictionary are static analysis and type safety for your benchmarking code.</p> Source code in <code>src/nnbench/types.py</code> <pre><code>@dataclass(init=False, frozen=True)\nclass Parameters:\n    \"\"\"\n    A dataclass designed to hold benchmark parameters.\n\n    This class is not functional on its own, and needs to be subclassed\n    according to your benchmarking workloads.\n\n    The main advantage over passing parameters as a dictionary are static analysis\n    and type safety for your benchmarking code.\n    \"\"\"\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Interface","title":"Interface  <code>dataclass</code>","text":"<p>Data model representing a function's interface.</p> <p>An instance of this class is created using the <code>Interface.from_callable()</code> class method.</p> Source code in <code>src/nnbench/types.py</code> <pre><code>@dataclass(frozen=True)\nclass Interface:\n    \"\"\"\n    Data model representing a function's interface.\n\n    An instance of this class is created using the ``Interface.from_callable()``\n    class method.\n    \"\"\"\n\n    funcname: str\n    \"\"\"Name of the function.\"\"\"\n    names: tuple[str, ...]\n    \"\"\"Names of the function parameters.\"\"\"\n    types: tuple[type, ...]\n    \"\"\"Type hints of the function parameters.\"\"\"\n    defaults: tuple\n    \"\"\"The function parameters' default values, or inspect.Parameter.empty if a parameter has no default.\"\"\"\n    variables: tuple[Variable, ...]\n    \"\"\"A tuple of tuples, where each inner tuple contains the parameter name, type, and default value.\"\"\"\n    returntype: type\n    \"\"\"The function's return type annotation, or NoneType if left untyped.\"\"\"\n\n    @classmethod\n    def from_callable(cls, fn: Callable, defaults: dict[str, Any]) -&gt; Self:\n        \"\"\"\n        Creates an interface instance from the given callable.\n\n        Wraps the information given by ``inspect.signature()``, with the option to\n        supply a ``defaults`` map and overwrite any default set in the function's\n        signature.\n        \"\"\"\n        # Set `follow_wrapped=False` to get the partially filled interfaces.\n        # Otherwise we get missing value errors for parameters supplied in benchmark decorators.\n        sig = inspect.signature(fn, follow_wrapped=False)\n        ret = sig.return_annotation\n        _defaults = {k: defaults.get(k, v.default) for k, v in sig.parameters.items()}\n        # defaults are the signature parameters, then the partial parametrization.\n        return cls(\n            fn.__name__,\n            tuple(sig.parameters.keys()),\n            tuple(p.annotation for p in sig.parameters.values()),\n            tuple(_defaults.values()),\n            tuple((k, v.annotation, _defaults[k]) for k, v in sig.parameters.items()),\n            type(ret) if ret is None else ret,\n        )\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Interface.funcname","title":"funcname  <code>instance-attribute</code>","text":"<pre><code>funcname: str\n</code></pre> <p>Name of the function.</p>"},{"location":"reference/nnbench/types/#nnbench.types.Interface.names","title":"names  <code>instance-attribute</code>","text":"<pre><code>names: tuple[str, ...]\n</code></pre> <p>Names of the function parameters.</p>"},{"location":"reference/nnbench/types/#nnbench.types.Interface.types","title":"types  <code>instance-attribute</code>","text":"<pre><code>types: tuple[type, ...]\n</code></pre> <p>Type hints of the function parameters.</p>"},{"location":"reference/nnbench/types/#nnbench.types.Interface.defaults","title":"defaults  <code>instance-attribute</code>","text":"<pre><code>defaults: tuple\n</code></pre> <p>The function parameters' default values, or inspect.Parameter.empty if a parameter has no default.</p>"},{"location":"reference/nnbench/types/#nnbench.types.Interface.variables","title":"variables  <code>instance-attribute</code>","text":"<pre><code>variables: tuple[Variable, ...]\n</code></pre> <p>A tuple of tuples, where each inner tuple contains the parameter name, type, and default value.</p>"},{"location":"reference/nnbench/types/#nnbench.types.Interface.returntype","title":"returntype  <code>instance-attribute</code>","text":"<pre><code>returntype: type\n</code></pre> <p>The function's return type annotation, or NoneType if left untyped.</p>"},{"location":"reference/nnbench/types/#nnbench.types.Interface.from_callable","title":"from_callable  <code>classmethod</code>","text":"<pre><code>from_callable(fn: Callable, defaults: dict[str, Any]) -&gt; Self\n</code></pre> <p>Creates an interface instance from the given callable.</p> <p>Wraps the information given by <code>inspect.signature()</code>, with the option to supply a <code>defaults</code> map and overwrite any default set in the function's signature.</p> Source code in <code>src/nnbench/types.py</code> <pre><code>@classmethod\ndef from_callable(cls, fn: Callable, defaults: dict[str, Any]) -&gt; Self:\n    \"\"\"\n    Creates an interface instance from the given callable.\n\n    Wraps the information given by ``inspect.signature()``, with the option to\n    supply a ``defaults`` map and overwrite any default set in the function's\n    signature.\n    \"\"\"\n    # Set `follow_wrapped=False` to get the partially filled interfaces.\n    # Otherwise we get missing value errors for parameters supplied in benchmark decorators.\n    sig = inspect.signature(fn, follow_wrapped=False)\n    ret = sig.return_annotation\n    _defaults = {k: defaults.get(k, v.default) for k, v in sig.parameters.items()}\n    # defaults are the signature parameters, then the partial parametrization.\n    return cls(\n        fn.__name__,\n        tuple(sig.parameters.keys()),\n        tuple(p.annotation for p in sig.parameters.values()),\n        tuple(_defaults.values()),\n        tuple((k, v.annotation, _defaults[k]) for k, v in sig.parameters.items()),\n        type(ret) if ret is None else ret,\n    )\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Benchmark","title":"Benchmark  <code>dataclass</code>","text":"<p>Data model representing a benchmark. Subclass this to define your own custom benchmark.</p> Source code in <code>src/nnbench/types.py</code> <pre><code>@dataclass(frozen=True)\nclass Benchmark:\n    \"\"\"\n    Data model representing a benchmark. Subclass this to define your own custom benchmark.\n    \"\"\"\n\n    fn: Callable[..., Any]\n    \"\"\"The function defining the benchmark.\"\"\"\n    name: str = \"\"\n    \"\"\"A name to display for the given benchmark. If not given, a name will be constructed from the function name and given parameters.\"\"\"\n    params: dict[str, Any] = field(default_factory=dict)\n    \"\"\"A partial parametrization to apply to the benchmark function. Internal only, you should not need to set this yourself.\"\"\"\n    setUp: Callable[[State, Mapping[str, Any]], None] = field(repr=False, default=NoOp)\n    \"\"\"A setup hook run before the benchmark. Must take all members of ``params`` as inputs.\"\"\"\n    tearDown: Callable[[State, Mapping[str, Any]], None] = field(repr=False, default=NoOp)\n    \"\"\"A teardown hook run after the benchmark. Must take all members of ``params`` as inputs.\"\"\"\n    tags: tuple[str, ...] = field(repr=False, default=())\n    \"\"\"Additional tags to attach for bookkeeping and selective filtering during runs.\"\"\"\n    interface: Interface = field(init=False, repr=False)\n    \"\"\"Benchmark interface, constructed from the given function. Implementation detail.\"\"\"\n\n    def __post_init__(self):\n        if not self.name:\n            super().__setattr__(\"name\", self.fn.__name__)\n        super().__setattr__(\"interface\", Interface.from_callable(self.fn, self.params))\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.Benchmark.fn","title":"fn  <code>instance-attribute</code>","text":"<pre><code>fn: Callable[..., Any]\n</code></pre> <p>The function defining the benchmark.</p>"},{"location":"reference/nnbench/types/#nnbench.types.Benchmark.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str = ''\n</code></pre> <p>A name to display for the given benchmark. If not given, a name will be constructed from the function name and given parameters.</p>"},{"location":"reference/nnbench/types/#nnbench.types.Benchmark.params","title":"params  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>params: dict[str, Any] = field(default_factory=dict)\n</code></pre> <p>A partial parametrization to apply to the benchmark function. Internal only, you should not need to set this yourself.</p>"},{"location":"reference/nnbench/types/#nnbench.types.Benchmark.setUp","title":"setUp  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>setUp: Callable[[State, Mapping[str, Any]], None] = field(repr=False, default=NoOp)\n</code></pre> <p>A setup hook run before the benchmark. Must take all members of <code>params</code> as inputs.</p>"},{"location":"reference/nnbench/types/#nnbench.types.Benchmark.tearDown","title":"tearDown  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tearDown: Callable[[State, Mapping[str, Any]], None] = field(repr=False, default=NoOp)\n</code></pre> <p>A teardown hook run after the benchmark. Must take all members of <code>params</code> as inputs.</p>"},{"location":"reference/nnbench/types/#nnbench.types.Benchmark.tags","title":"tags  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tags: tuple[str, ...] = field(repr=False, default=())\n</code></pre> <p>Additional tags to attach for bookkeeping and selective filtering during runs.</p>"},{"location":"reference/nnbench/types/#nnbench.types.Benchmark.interface","title":"interface  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>interface: Interface = field(init=False, repr=False)\n</code></pre> <p>Benchmark interface, constructed from the given function. Implementation detail.</p>"},{"location":"reference/nnbench/types/#nnbench.types.BenchmarkFamily","title":"BenchmarkFamily","text":"<p>               Bases: <code>Iterable[Benchmark]</code></p> Source code in <code>src/nnbench/types.py</code> <pre><code>class BenchmarkFamily(Iterable[Benchmark]):\n    def __init__(\n        self,\n        fn: Callable[..., Any],\n        params: Iterable[dict[str, Any]],\n        name: str | Callable[..., str],\n        setUp: Callable[..., None] = NoOp,\n        tearDown: Callable[..., None] = NoOp,\n        tags: tuple[str, ...] = (),\n    ):\n        self.fn = fn\n        self.params = params\n        if isinstance(name, str):\n            # if name is a str, we assume it's an f-string that should be\n            # interpolated with the necessary parameters.\n            self.name = lambda _fn, **kwargs: name.format(**kwargs)\n        else:\n            self.name = name\n        self.setUp = setUp\n        self.tearDown = tearDown\n        self.tags = tags\n\n    def __iter__(self):\n        \"\"\"\n        Dispatch benchmarks lazily, creating a name from the arguments as dictated\n        by ``self.name``.\n        \"\"\"\n        for p in self.params:\n            yield Benchmark(\n                fn=self.fn,\n                params=p,\n                name=self.name(self.fn, **p),\n                setUp=self.setUp,\n                tearDown=self.tearDown,\n                tags=self.tags,\n            )\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.BenchmarkFamily.fn","title":"fn  <code>instance-attribute</code>","text":"<pre><code>fn = fn\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.BenchmarkFamily.params","title":"params  <code>instance-attribute</code>","text":"<pre><code>params = params\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.BenchmarkFamily.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = lambda _fn, **kwargs: format(**kwargs)\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.BenchmarkFamily.setUp","title":"setUp  <code>instance-attribute</code>","text":"<pre><code>setUp = setUp\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.BenchmarkFamily.tearDown","title":"tearDown  <code>instance-attribute</code>","text":"<pre><code>tearDown = tearDown\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.BenchmarkFamily.tags","title":"tags  <code>instance-attribute</code>","text":"<pre><code>tags = tags\n</code></pre>"},{"location":"reference/nnbench/types/#nnbench.types.NoOp","title":"NoOp","text":"<pre><code>NoOp(state: State, params: Mapping[str, Any] = MappingProxyType({})) -&gt; None\n</code></pre> <p>A no-op setup/teardown callback that does nothing.</p> Source code in <code>src/nnbench/types.py</code> <pre><code>def NoOp(state: State, params: Mapping[str, Any] = MappingProxyType({})) -&gt; None:\n    \"\"\"A no-op setup/teardown callback that does nothing.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/nnbench/util/","title":"util","text":"<p>Various utilities related to benchmark collection, filtering, and more.</p>"},{"location":"reference/nnbench/util/#nnbench.util.T","title":"T  <code>module-attribute</code>","text":"<pre><code>T = TypeVar('T')\n</code></pre>"},{"location":"reference/nnbench/util/#nnbench.util.collapse","title":"collapse","text":"<pre><code>collapse(_its: Iterable[T | Iterable[T]]) -&gt; Generator[T, None, None]\n</code></pre> Source code in <code>src/nnbench/util.py</code> <pre><code>def collapse(_its: Iterable[T | Iterable[T]]) -&gt; Generator[T, None, None]:\n    for _it in _its:\n        if isinstance(_it, Iterable):\n            yield from _it\n        else:\n            yield _it\n</code></pre>"},{"location":"reference/nnbench/util/#nnbench.util.flatten","title":"flatten","text":"<pre><code>flatten(d: dict[str, Any], sep: str = '.', prefix: str = '') -&gt; dict\n</code></pre> <p>Given a nested dictionary and a separator, returns another dictionary of depth 1, containing values under nested keys joined by the separator.</p> PARAMETER DESCRIPTION <code>d</code> <p>A dictionary to be flattened. All nested dictionaries must contain string keys only.</p> <p> TYPE: <code>dict[str, Any]</code> </p> <code>sep</code> <p>The separator string to join keys on.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'.'</code> </p> <code>prefix</code> <p>A prefix to apply to keys when calling <code>flatten()</code> recursively. You shouldn't need to pass this yourself.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>The flattened dictionary.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; flatten({\"a\": 1, \"b\": {\"c\": 2}})\n{\"a\": 1, \"b.c\": 2}\n</code></pre> Source code in <code>src/nnbench/util.py</code> <pre><code>def flatten(d: dict[str, Any], sep: str = \".\", prefix: str = \"\") -&gt; dict:\n    \"\"\"\n    Given a nested dictionary and a separator, returns another dictionary\n    of depth 1, containing values under nested keys joined by the separator.\n\n    Parameters\n    ----------\n    d: dict[str, Any]\n        A dictionary to be flattened. All nested dictionaries must contain\n        string keys only.\n    sep: str\n        The separator string to join keys on.\n    prefix: str\n        A prefix to apply to keys when calling ``flatten()`` recursively.\n        You shouldn't need to pass this yourself.\n\n    Returns\n    -------\n    dict[str, Any]\n        The flattened dictionary.\n\n    Examples\n    --------\n    &gt;&gt;&gt; flatten({\"a\": 1, \"b\": {\"c\": 2}})\n    {\"a\": 1, \"b.c\": 2}\n    \"\"\"\n    d_flat = {}\n    for k, v in d.items():\n        new_key = prefix + sep + k if prefix else k\n        if isinstance(v, dict):\n            d_flat.update(flatten(v, sep=sep, prefix=new_key))\n        else:\n            d_flat[new_key] = v\n    return d_flat\n</code></pre>"},{"location":"reference/nnbench/util/#nnbench.util.unflatten","title":"unflatten","text":"<pre><code>unflatten(d: dict[str, Any], sep: str = '.') -&gt; dict[str, Any]\n</code></pre> <p>Unflatten a previously flattened dictionary.</p> <p>Any key that does not contain the separator is passed through unchanged.</p> <p>This is, as the name suggests, the inverse operation to <code>nnbench.util.flatten()</code>.</p> PARAMETER DESCRIPTION <code>d</code> <p>The dictionary to unflatten.</p> <p> TYPE: <code>dict[str, Any]</code> </p> <code>sep</code> <p>The separator to split keys on, introducing dictionary nesting.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'.'</code> </p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; unflatten({\"a\": 1, \"b.c\": 2})\n{\"a\": 1, \"b\": {\"c\": 2}}\n</code></pre> <pre><code>&gt;&gt;&gt; d = {\"a\": 1, \"b\": {\"c\": 2}}\n&gt;&gt;&gt; unflatten(flatten(d)) == d\nTrue\n</code></pre> Source code in <code>src/nnbench/util.py</code> <pre><code>def unflatten(d: dict[str, Any], sep: str = \".\") -&gt; dict[str, Any]:\n    \"\"\"\n    Unflatten a previously flattened dictionary.\n\n    Any key that does not contain the separator is passed through unchanged.\n\n    This is, as the name suggests, the inverse operation to ``nnbench.util.flatten()``.\n\n    Parameters\n    ----------\n    d: dict[str, Any]\n        The dictionary to unflatten.\n    sep: str\n        The separator to split keys on, introducing dictionary nesting.\n\n    Returns\n    -------\n    dict[str, Any]\n\n    Examples\n    --------\n    &gt;&gt;&gt; unflatten({\"a\": 1, \"b.c\": 2})\n    {\"a\": 1, \"b\": {\"c\": 2}}\n\n    &gt;&gt;&gt; d = {\"a\": 1, \"b\": {\"c\": 2}}\n    &gt;&gt;&gt; unflatten(flatten(d)) == d\n    True\n    \"\"\"\n    sorted_keys = sorted(d.keys())\n    unflattened = {}\n    for prefix, keys in itertools.groupby(sorted_keys, key=lambda key: key.split(sep, 1)[0]):\n        key_group = list(keys)\n        if len(key_group) == 1 and sep not in key_group[0]:\n            unflattened[prefix] = d[prefix]\n        else:\n            nested_dict = {key.split(sep, 1)[1]: d[key] for key in key_group}\n            unflattened[prefix] = unflatten(nested_dict, sep=sep)\n    return unflattened\n</code></pre>"},{"location":"reference/nnbench/util/#nnbench.util.exists_module","title":"exists_module","text":"<pre><code>exists_module(name: str | PathLike[str]) -&gt; bool\n</code></pre> <p>Checks if the current interpreter has an available Python module named <code>name</code>.</p> Source code in <code>src/nnbench/util.py</code> <pre><code>def exists_module(name: str | os.PathLike[str]) -&gt; bool:\n    \"\"\"Checks if the current interpreter has an available Python module named `name`.\"\"\"\n    name = str(name)\n    if name in sys.modules:\n        return True\n\n    root, *parts = name.split(\".\")\n\n    for part in parts:\n        spec = importlib.util.find_spec(root)\n        if spec is None:\n            return False\n        root += f\".{part}\"\n\n    return importlib.util.find_spec(name) is not None\n</code></pre>"},{"location":"reference/nnbench/util/#nnbench.util.modulename","title":"modulename","text":"<pre><code>modulename(file: str | PathLike[str]) -&gt; str\n</code></pre> <p>Convert a file name to its corresponding Python module name.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; modulename(\"path/to/my/file.py\")\n\"path.to.my.module\"\n</code></pre> Source code in <code>src/nnbench/util.py</code> <pre><code>def modulename(file: str | os.PathLike[str]) -&gt; str:\n    \"\"\"\n    Convert a file name to its corresponding Python module name.\n\n    Examples\n    --------\n    &gt;&gt;&gt; modulename(\"path/to/my/file.py\")\n    \"path.to.my.module\"\n    \"\"\"\n    fpath = Path(file).with_suffix(\"\")\n    if len(fpath.parts) == 1:\n        return str(fpath)\n\n    filename = fpath.as_posix()\n    return filename.replace(\"/\", \".\")\n</code></pre>"},{"location":"reference/nnbench/util/#nnbench.util.import_file_as_module","title":"import_file_as_module","text":"<pre><code>import_file_as_module(file: str | PathLike[str]) -&gt; ModuleType\n</code></pre> <p>Import a Python file as a module using importlib.</p> <p>Raises an error if the given path is not a Python file, or if the module spec could not be constructed.</p> PARAMETER DESCRIPTION <code>file</code> <p>The file to import as a Python module.</p> <p> TYPE: <code>str | PathLike[str]</code> </p> RETURNS DESCRIPTION <code>ModuleType</code> <p>The imported module, with its file location set as <code>__file__</code>.</p> Source code in <code>src/nnbench/util.py</code> <pre><code>def import_file_as_module(file: str | os.PathLike[str]) -&gt; ModuleType:\n    \"\"\"\n    Import a Python file as a module using importlib.\n\n    Raises an error if the given path is not a Python file, or if the\n    module spec could not be constructed.\n\n    Parameters\n    ----------\n    file: str | os.PathLike[str]\n        The file to import as a Python module.\n\n    Returns\n    -------\n    ModuleType\n        The imported module, with its file location set as ``__file__``.\n\n    \"\"\"\n    fpath = Path(file)\n    if not fpath.is_file() or fpath.suffix != \".py\":\n        raise ValueError(f\"path {str(file)!r} is not a Python file\")\n\n    # TODO: Recomputing this map in a loop can be expensive if many modules are loaded.\n    modmap = {m.__file__: m for m in sys.modules.values() if getattr(m, \"__file__\", None)}\n    spath = str(fpath)\n    if spath in modmap:\n        # if the module under \"file\" has already been loaded, return it,\n        # otherwise we get nasty type errors in collection.\n        return modmap[spath]\n\n    modname = modulename(fpath)\n    if modname in sys.modules:\n        # return already loaded module\n        return sys.modules[modname]\n\n    spec: ModuleSpec | None = importlib.util.spec_from_file_location(modname, fpath)\n    if spec is None:\n        raise RuntimeError(f\"could not import module {fpath}\")\n\n    module = importlib.util.module_from_spec(spec)\n    sys.modules[modname] = module\n    spec.loader.exec_module(module)\n    return module\n</code></pre>"},{"location":"reference/nnbench/util/#nnbench.util.all_python_files","title":"all_python_files","text":"<pre><code>all_python_files(_dir: str | PathLike[str]) -&gt; Generator[Path, None, None]\n</code></pre> Source code in <code>src/nnbench/util.py</code> <pre><code>def all_python_files(_dir: str | os.PathLike[str]) -&gt; Generator[Path, None, None]:\n    if sys.version_info &gt;= (3, 12):\n        pathgen = Path(_dir).walk(top_down=True)\n    else:\n        pathgen = os.walk(_dir, topdown=True)\n\n    for root, dirs, files in pathgen:\n        proot = Path(root)\n        # do not descend into potentially large __pycache__ dirs.\n        if \"__pycache__\" in dirs:\n            dirs.remove(\"__pycache__\")\n        for file in files:\n            fp = proot / file\n            if fp.suffix == \".py\":\n                yield fp\n</code></pre>"},{"location":"reference/nnbench/util/#nnbench.util.qualname","title":"qualname","text":"<pre><code>qualname(fn: Callable) -&gt; str\n</code></pre> Source code in <code>src/nnbench/util.py</code> <pre><code>def qualname(fn: Callable) -&gt; str:\n    if fn.__name__ == fn.__qualname__:\n        return fn.__name__\n    return f\"{fn.__qualname__}.{fn.__name__}\"\n</code></pre>"},{"location":"reference/nnbench/util/#nnbench.util.timer","title":"timer","text":"<pre><code>timer(bm: dict[str, Any]) -&gt; Generator[None, None, None]\n</code></pre> Source code in <code>src/nnbench/util.py</code> <pre><code>@contextlib.contextmanager\ndef timer(bm: dict[str, Any]) -&gt; Generator[None, None, None]:\n    start = time.perf_counter_ns()\n    try:\n        yield\n    finally:\n        end = time.perf_counter_ns()\n        bm[\"time_ns\"] = end - start\n</code></pre>"},{"location":"reference/nnbench/reporter/","title":"reporter","text":"<p>An interface for displaying, writing, or streaming benchmark results to files, databases, or web services.</p>"},{"location":"reference/nnbench/reporter/#nnbench.reporter.get_io_implementation","title":"get_io_implementation","text":"<pre><code>get_io_implementation(uri: str | PathLike[str]) -&gt; BenchmarkFileIO | BenchmarkServiceIO\n</code></pre> Source code in <code>src/nnbench/reporter/__init__.py</code> <pre><code>def get_io_implementation(uri: str | os.PathLike[str]) -&gt; BenchmarkFileIO | BenchmarkServiceIO:\n    import sys\n\n    if uri is sys.stdout:\n        proto = \"stdout\"\n    else:\n        from .util import get_protocol\n\n        proto = get_protocol(uri)\n    try:\n        return _io_implementations[proto]()\n    except KeyError:\n        raise ValueError(f\"unsupported benchmark IO protocol {proto!r}\") from None\n</code></pre>"},{"location":"reference/nnbench/reporter/#nnbench.reporter.register_io_implementation","title":"register_io_implementation","text":"<pre><code>register_io_implementation(name: str, klass: type, clobber: bool = False) -&gt; None\n</code></pre> Source code in <code>src/nnbench/reporter/__init__.py</code> <pre><code>def register_io_implementation(name: str, klass: type, clobber: bool = False) -&gt; None:\n    if name in _io_implementations and not clobber:\n        raise RuntimeError(\n            f\"benchmark IO {name!r} is already registered \"\n            f\"(to force registration, rerun with clobber=True)\"\n        )\n    _io_implementations[name] = klass\n</code></pre>"},{"location":"reference/nnbench/reporter/console/","title":"console","text":""},{"location":"reference/nnbench/reporter/console/#nnbench.reporter.console.ConsoleReporter","title":"ConsoleReporter","text":"<p>               Bases: <code>BenchmarkFileIO</code></p> <p>The base interface for a console reporter class.</p> <p>Wraps a <code>rich.Console()</code> to display values in a rich-text table.</p> Source code in <code>src/nnbench/reporter/console.py</code> <pre><code>class ConsoleReporter(BenchmarkFileIO):\n    \"\"\"\n    The base interface for a console reporter class.\n\n    Wraps a ``rich.Console()`` to display values in a rich-text table.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Initialize a console reporter.\n\n        Parameters\n        ----------\n        *args: Any\n            Positional arguments, unused.\n        **kwargs: Any\n            Keyword arguments, forwarded directly to ``rich.Console()``.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        # TODO: Add context manager to register live console prints\n        self.console = Console(**kwargs)\n\n    def read(self, fp: str | os.PathLike[str], options: dict[str, Any]) -&gt; BenchmarkRecord:\n        raise NotImplementedError\n\n    def write(\n        self,\n        record: BenchmarkRecord,\n        outfile: str | os.PathLike[str] = None,\n        options: dict[str, Any] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Display a benchmark record in the console as a rich-text table.\n\n        Gives a summary of all present context values directly above the table,\n        as a pretty-printed JSON record.\n\n        By default, displays only the benchmark name, value, execution wall time,\n        and parameters.\n\n        Parameters\n        ----------\n        record: BenchmarkRecord\n            The benchmark record to display.\n        outfile: str | os.PathLike[str]\n            For compatibility with the `BenchmarkFileIO` interface, unused.\n        options: dict[str, Any]\n            Display options used to format the resulting table.\n        \"\"\"\n        del outfile\n        t = Table()\n\n        rows: list[list[str]] = []\n        columns: list[str] = [\"Benchmark\", \"Value\", \"Wall time (ns)\", \"Parameters\"]\n\n        # print context values\n        print(\"Context values:\")\n        print(json.dumps(record.context, indent=4))\n\n        for bm in record.benchmarks:\n            row = [bm[\"name\"], get_value_by_name(bm), str(bm[\"time_ns\"]), str(bm[\"parameters\"])]\n            rows.append(row)\n\n        for column in columns:\n            t.add_column(column)\n        for row in rows:\n            t.add_row(*row)\n\n        self.console.print(t, overflow=\"ellipsis\")\n</code></pre>"},{"location":"reference/nnbench/reporter/console/#nnbench.reporter.console.ConsoleReporter.console","title":"console  <code>instance-attribute</code>","text":"<pre><code>console = Console(**kwargs)\n</code></pre>"},{"location":"reference/nnbench/reporter/console/#nnbench.reporter.console.ConsoleReporter.read","title":"read","text":"<pre><code>read(fp: str | PathLike[str], options: dict[str, Any]) -&gt; BenchmarkRecord\n</code></pre> Source code in <code>src/nnbench/reporter/console.py</code> <pre><code>def read(self, fp: str | os.PathLike[str], options: dict[str, Any]) -&gt; BenchmarkRecord:\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/nnbench/reporter/console/#nnbench.reporter.console.ConsoleReporter.write","title":"write","text":"<pre><code>write(\n    record: BenchmarkRecord,\n    outfile: str | PathLike[str] = None,\n    options: dict[str, Any] | None = None,\n) -&gt; None\n</code></pre> <p>Display a benchmark record in the console as a rich-text table.</p> <p>Gives a summary of all present context values directly above the table, as a pretty-printed JSON record.</p> <p>By default, displays only the benchmark name, value, execution wall time, and parameters.</p> PARAMETER DESCRIPTION <code>record</code> <p>The benchmark record to display.</p> <p> TYPE: <code>BenchmarkRecord</code> </p> <code>outfile</code> <p>For compatibility with the <code>BenchmarkFileIO</code> interface, unused.</p> <p> TYPE: <code>str | PathLike[str]</code> DEFAULT: <code>None</code> </p> <code>options</code> <p>Display options used to format the resulting table.</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/nnbench/reporter/console.py</code> <pre><code>def write(\n    self,\n    record: BenchmarkRecord,\n    outfile: str | os.PathLike[str] = None,\n    options: dict[str, Any] | None = None,\n) -&gt; None:\n    \"\"\"\n    Display a benchmark record in the console as a rich-text table.\n\n    Gives a summary of all present context values directly above the table,\n    as a pretty-printed JSON record.\n\n    By default, displays only the benchmark name, value, execution wall time,\n    and parameters.\n\n    Parameters\n    ----------\n    record: BenchmarkRecord\n        The benchmark record to display.\n    outfile: str | os.PathLike[str]\n        For compatibility with the `BenchmarkFileIO` interface, unused.\n    options: dict[str, Any]\n        Display options used to format the resulting table.\n    \"\"\"\n    del outfile\n    t = Table()\n\n    rows: list[list[str]] = []\n    columns: list[str] = [\"Benchmark\", \"Value\", \"Wall time (ns)\", \"Parameters\"]\n\n    # print context values\n    print(\"Context values:\")\n    print(json.dumps(record.context, indent=4))\n\n    for bm in record.benchmarks:\n        row = [bm[\"name\"], get_value_by_name(bm), str(bm[\"time_ns\"]), str(bm[\"parameters\"])]\n        rows.append(row)\n\n    for column in columns:\n        t.add_column(column)\n    for row in rows:\n        t.add_row(*row)\n\n    self.console.print(t, overflow=\"ellipsis\")\n</code></pre>"},{"location":"reference/nnbench/reporter/console/#nnbench.reporter.console.get_value_by_name","title":"get_value_by_name","text":"<pre><code>get_value_by_name(result: dict[str, Any]) -&gt; str\n</code></pre> Source code in <code>src/nnbench/reporter/console.py</code> <pre><code>def get_value_by_name(result: dict[str, Any]) -&gt; str:\n    if result.get(\"error_occurred\", False):\n        errmsg = result.get(\"error_message\", \"&lt;unknown&gt;\")\n        return \"[red]ERROR: [/red]\" + errmsg\n    return str(result.get(\"value\", _MISSING))\n</code></pre>"},{"location":"reference/nnbench/reporter/file/","title":"file","text":""},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.BenchmarkFileIO","title":"BenchmarkFileIO","text":"<p>               Bases: <code>Protocol</code></p> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>class BenchmarkFileIO(Protocol):\n    def read(self, fp: str | os.PathLike[str], options: dict[str, Any]) -&gt; BenchmarkRecord: ...\n\n    def write(\n        self, record: BenchmarkRecord, fp: str | os.PathLike[str], options: dict[str, Any]\n    ) -&gt; None: ...\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.BenchmarkFileIO.read","title":"read","text":"<pre><code>read(fp: str | PathLike[str], options: dict[str, Any]) -&gt; BenchmarkRecord\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def read(self, fp: str | os.PathLike[str], options: dict[str, Any]) -&gt; BenchmarkRecord: ...\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.BenchmarkFileIO.write","title":"write","text":"<pre><code>write(record: BenchmarkRecord, fp: str | PathLike[str], options: dict[str, Any]) -&gt; None\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def write(\n    self, record: BenchmarkRecord, fp: str | os.PathLike[str], options: dict[str, Any]\n) -&gt; None: ...\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.YAMLFileIO","title":"YAMLFileIO","text":"<p>               Bases: <code>BenchmarkFileIO</code></p> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>class YAMLFileIO(BenchmarkFileIO):\n    extensions = (\".yaml\", \".yml\")\n\n    def __init__(self):\n        try:\n            import yaml\n\n            self.yaml = yaml\n        except ImportError:\n            raise ModuleNotFoundError(\"`pyyaml` is not installed\")\n\n    def read(self, fp: str | os.PathLike[str], options: dict[str, Any]) -&gt; BenchmarkRecord:\n        del options\n        with make_file_descriptor(fp, mode=\"r\") as fd:\n            bms = self.yaml.safe_load(fd)\n        return BenchmarkRecord.expand(bms)\n\n    def write(\n        self, record: BenchmarkRecord, fp: str | os.PathLike[str], options: dict[str, Any]\n    ) -&gt; None:\n        with make_file_descriptor(fp, mode=\"w\") as fd:\n            self.yaml.safe_dump(record.to_json(), fd, **options)\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.YAMLFileIO.extensions","title":"extensions  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>extensions = ('.yaml', '.yml')\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.YAMLFileIO.yaml","title":"yaml  <code>instance-attribute</code>","text":"<pre><code>yaml = yaml\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.YAMLFileIO.read","title":"read","text":"<pre><code>read(fp: str | PathLike[str], options: dict[str, Any]) -&gt; BenchmarkRecord\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def read(self, fp: str | os.PathLike[str], options: dict[str, Any]) -&gt; BenchmarkRecord:\n    del options\n    with make_file_descriptor(fp, mode=\"r\") as fd:\n        bms = self.yaml.safe_load(fd)\n    return BenchmarkRecord.expand(bms)\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.YAMLFileIO.write","title":"write","text":"<pre><code>write(record: BenchmarkRecord, fp: str | PathLike[str], options: dict[str, Any]) -&gt; None\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def write(\n    self, record: BenchmarkRecord, fp: str | os.PathLike[str], options: dict[str, Any]\n) -&gt; None:\n    with make_file_descriptor(fp, mode=\"w\") as fd:\n        self.yaml.safe_dump(record.to_json(), fd, **options)\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.JSONFileIO","title":"JSONFileIO","text":"<p>               Bases: <code>BenchmarkFileIO</code></p> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>class JSONFileIO(BenchmarkFileIO):\n    extensions = (\".json\", \".ndjson\")\n\n    def __init__(self):\n        import json\n\n        self.json = json\n\n    def read(self, fp: str | os.PathLike[str], options: dict[str, Any]) -&gt; BenchmarkRecord:\n        newline_delimited = Path(fp).suffix == \".ndjson\"\n        benchmarks: list[dict[str, Any]]\n        with make_file_descriptor(fp, mode=\"r\") as fd:\n            if newline_delimited:\n                benchmarks = [self.json.loads(line, **options) for line in fd]\n            else:\n                benchmarks = self.json.load(fd, **options)\n            return BenchmarkRecord.expand(benchmarks)\n\n    def write(\n        self, record: BenchmarkRecord, fp: str | os.PathLike[str], options: dict[str, Any]\n    ) -&gt; None:\n        newline_delimited = Path(fp).suffix == \".ndjson\"\n        with make_file_descriptor(fp, mode=\"w\") as fd:\n            if newline_delimited:\n                bms = record.to_list()\n                fd.write(\"\\n\".join([self.json.dumps(b, **options) for b in bms]))\n            else:\n                self.json.dump(record.to_json(), fd, **options)\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.JSONFileIO.extensions","title":"extensions  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>extensions = ('.json', '.ndjson')\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.JSONFileIO.json","title":"json  <code>instance-attribute</code>","text":"<pre><code>json = json\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.JSONFileIO.read","title":"read","text":"<pre><code>read(fp: str | PathLike[str], options: dict[str, Any]) -&gt; BenchmarkRecord\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def read(self, fp: str | os.PathLike[str], options: dict[str, Any]) -&gt; BenchmarkRecord:\n    newline_delimited = Path(fp).suffix == \".ndjson\"\n    benchmarks: list[dict[str, Any]]\n    with make_file_descriptor(fp, mode=\"r\") as fd:\n        if newline_delimited:\n            benchmarks = [self.json.loads(line, **options) for line in fd]\n        else:\n            benchmarks = self.json.load(fd, **options)\n        return BenchmarkRecord.expand(benchmarks)\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.JSONFileIO.write","title":"write","text":"<pre><code>write(record: BenchmarkRecord, fp: str | PathLike[str], options: dict[str, Any]) -&gt; None\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def write(\n    self, record: BenchmarkRecord, fp: str | os.PathLike[str], options: dict[str, Any]\n) -&gt; None:\n    newline_delimited = Path(fp).suffix == \".ndjson\"\n    with make_file_descriptor(fp, mode=\"w\") as fd:\n        if newline_delimited:\n            bms = record.to_list()\n            fd.write(\"\\n\".join([self.json.dumps(b, **options) for b in bms]))\n        else:\n            self.json.dump(record.to_json(), fd, **options)\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.CSVFileIO","title":"CSVFileIO","text":"<p>               Bases: <code>BenchmarkFileIO</code></p> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>class CSVFileIO(BenchmarkFileIO):\n    extensions = (\".csv\",)\n\n    def __init__(self):\n        import csv\n\n        self.csv = csv\n\n    def read(self, fp: str | os.PathLike[str], options: dict[str, Any]) -&gt; BenchmarkRecord:\n        import json\n\n        with make_file_descriptor(fp, mode=\"r\") as fd:\n            reader = self.csv.DictReader(fd, **options)\n            benchmarks: list[dict[str, Any]] = []\n            # csv.DictReader has no appropriate type hint for __next__,\n            # so we supply one ourselves.\n            bm: dict[str, Any]\n            for bm in reader:\n                benchmarks.append(bm)\n                # it can happen that the context is inlined as a stringified JSON object\n                # (e.g. in CSV), so we optionally JSON-load the context.\n                if \"context\" in bm:\n                    strctx: str = bm[\"context\"]\n                    # TODO: This does not play nicely with doublequote, maybe re.sub?\n                    strctx = strctx.replace(\"'\", '\"')\n                    bm[\"context\"] = json.loads(strctx)\n            return BenchmarkRecord.expand(benchmarks)\n\n    def write(\n        self, record: BenchmarkRecord, fp: str | os.PathLike[str], options: dict[str, Any]\n    ) -&gt; None:\n        bm = record.to_list()\n        with make_file_descriptor(fp, mode=\"w\") as fd:\n            writer = self.csv.DictWriter(fd, fieldnames=bm[0].keys(), **options)\n            writer.writeheader()\n\n            for b in bm:\n                writer.writerow(b)\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.CSVFileIO.extensions","title":"extensions  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>extensions = ('.csv',)\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.CSVFileIO.csv","title":"csv  <code>instance-attribute</code>","text":"<pre><code>csv = csv\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.CSVFileIO.read","title":"read","text":"<pre><code>read(fp: str | PathLike[str], options: dict[str, Any]) -&gt; BenchmarkRecord\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def read(self, fp: str | os.PathLike[str], options: dict[str, Any]) -&gt; BenchmarkRecord:\n    import json\n\n    with make_file_descriptor(fp, mode=\"r\") as fd:\n        reader = self.csv.DictReader(fd, **options)\n        benchmarks: list[dict[str, Any]] = []\n        # csv.DictReader has no appropriate type hint for __next__,\n        # so we supply one ourselves.\n        bm: dict[str, Any]\n        for bm in reader:\n            benchmarks.append(bm)\n            # it can happen that the context is inlined as a stringified JSON object\n            # (e.g. in CSV), so we optionally JSON-load the context.\n            if \"context\" in bm:\n                strctx: str = bm[\"context\"]\n                # TODO: This does not play nicely with doublequote, maybe re.sub?\n                strctx = strctx.replace(\"'\", '\"')\n                bm[\"context\"] = json.loads(strctx)\n        return BenchmarkRecord.expand(benchmarks)\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.CSVFileIO.write","title":"write","text":"<pre><code>write(record: BenchmarkRecord, fp: str | PathLike[str], options: dict[str, Any]) -&gt; None\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def write(\n    self, record: BenchmarkRecord, fp: str | os.PathLike[str], options: dict[str, Any]\n) -&gt; None:\n    bm = record.to_list()\n    with make_file_descriptor(fp, mode=\"w\") as fd:\n        writer = self.csv.DictWriter(fd, fieldnames=bm[0].keys(), **options)\n        writer.writeheader()\n\n        for b in bm:\n            writer.writerow(b)\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.ParquetFileIO","title":"ParquetFileIO","text":"<p>               Bases: <code>BenchmarkFileIO</code></p> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>class ParquetFileIO(BenchmarkFileIO):\n    extensions = (\".parquet\", \".pq\")\n\n    def __init__(self):\n        import pyarrow.parquet as pq\n\n        self.pyarrow_parquet = pq\n\n    def read(self, fp: str | os.PathLike[str], options: dict[str, Any]) -&gt; BenchmarkRecord:\n        table = self.pyarrow_parquet.read_table(str(fp), **options)\n        benchmarks: list[dict[str, Any]] = table.to_pylist()\n        return BenchmarkRecord.expand(benchmarks)\n\n    def write(\n        self, record: BenchmarkRecord, fp: str | os.PathLike[str], options: dict[str, Any]\n    ) -&gt; None:\n        from pyarrow import Table\n\n        table = Table.from_pylist(record.to_list())\n        self.pyarrow_parquet.write_table(table, str(fp), **options)\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.ParquetFileIO.extensions","title":"extensions  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>extensions = ('.parquet', '.pq')\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.ParquetFileIO.pyarrow_parquet","title":"pyarrow_parquet  <code>instance-attribute</code>","text":"<pre><code>pyarrow_parquet = parquet\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.ParquetFileIO.read","title":"read","text":"<pre><code>read(fp: str | PathLike[str], options: dict[str, Any]) -&gt; BenchmarkRecord\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def read(self, fp: str | os.PathLike[str], options: dict[str, Any]) -&gt; BenchmarkRecord:\n    table = self.pyarrow_parquet.read_table(str(fp), **options)\n    benchmarks: list[dict[str, Any]] = table.to_pylist()\n    return BenchmarkRecord.expand(benchmarks)\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.ParquetFileIO.write","title":"write","text":"<pre><code>write(record: BenchmarkRecord, fp: str | PathLike[str], options: dict[str, Any]) -&gt; None\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def write(\n    self, record: BenchmarkRecord, fp: str | os.PathLike[str], options: dict[str, Any]\n) -&gt; None:\n    from pyarrow import Table\n\n    table = Table.from_pylist(record.to_list())\n    self.pyarrow_parquet.write_table(table, str(fp), **options)\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.FileReporter","title":"FileReporter","text":"Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>class FileReporter:\n    def read(\n        self,\n        file: str | os.PathLike[str],\n        options: dict[str, Any] | None = None,\n    ) -&gt; BenchmarkRecord:\n        \"\"\"\n        Reads a benchmark record from the given file path.\n\n        The file IO is chosen based on the extension in the ``file`` pathname.\n\n        Parameters\n        ----------\n        file: str | os.PathLike[str]\n            The file name, or object, to read from.\n        options: dict[str, Any] | None\n            Options to pass to the respective file IO implementation.\n\n        Returns\n        -------\n        BenchmarkRecord\n            The benchmark record contained in the file.\n\n        Raises\n        ------\n        ValueError\n            If the extension of the given file is not supported.\n        \"\"\"\n\n        file_io = get_file_io_class(file)\n        return file_io.read(file, options or {})\n\n    def write(\n        self,\n        record: BenchmarkRecord,\n        file: str | os.PathLike[str],\n        options: dict[str, Any] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Writes a benchmark record to the given file path.\n\n        The file IO is chosen based on the extension found on the ``file`` path.\n\n        Parameters\n        ----------\n        record: BenchmarkRecord\n            The record to write to the database.\n        file: str | os.PathLike[str]\n            The file name, or object, to write to.\n        options: dict[str, Any] | None\n            Options to pass to the respective file IO implementation.\n\n        Raises\n        ------\n        ValueError\n            If the extension of the given file is not supported.\n        \"\"\"\n        file_io = get_file_io_class(file)\n        file_io.write(record, file, options or {})\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.FileReporter.read","title":"read","text":"<pre><code>read(file: str | PathLike[str], options: dict[str, Any] | None = None) -&gt; BenchmarkRecord\n</code></pre> <p>Reads a benchmark record from the given file path.</p> <p>The file IO is chosen based on the extension in the <code>file</code> pathname.</p> PARAMETER DESCRIPTION <code>file</code> <p>The file name, or object, to read from.</p> <p> TYPE: <code>str | PathLike[str]</code> </p> <code>options</code> <p>Options to pass to the respective file IO implementation.</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>BenchmarkRecord</code> <p>The benchmark record contained in the file.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the extension of the given file is not supported.</p> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def read(\n    self,\n    file: str | os.PathLike[str],\n    options: dict[str, Any] | None = None,\n) -&gt; BenchmarkRecord:\n    \"\"\"\n    Reads a benchmark record from the given file path.\n\n    The file IO is chosen based on the extension in the ``file`` pathname.\n\n    Parameters\n    ----------\n    file: str | os.PathLike[str]\n        The file name, or object, to read from.\n    options: dict[str, Any] | None\n        Options to pass to the respective file IO implementation.\n\n    Returns\n    -------\n    BenchmarkRecord\n        The benchmark record contained in the file.\n\n    Raises\n    ------\n    ValueError\n        If the extension of the given file is not supported.\n    \"\"\"\n\n    file_io = get_file_io_class(file)\n    return file_io.read(file, options or {})\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.FileReporter.write","title":"write","text":"<pre><code>write(\n    record: BenchmarkRecord, file: str | PathLike[str], options: dict[str, Any] | None = None\n) -&gt; None\n</code></pre> <p>Writes a benchmark record to the given file path.</p> <p>The file IO is chosen based on the extension found on the <code>file</code> path.</p> PARAMETER DESCRIPTION <code>record</code> <p>The record to write to the database.</p> <p> TYPE: <code>BenchmarkRecord</code> </p> <code>file</code> <p>The file name, or object, to write to.</p> <p> TYPE: <code>str | PathLike[str]</code> </p> <code>options</code> <p>Options to pass to the respective file IO implementation.</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the extension of the given file is not supported.</p> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def write(\n    self,\n    record: BenchmarkRecord,\n    file: str | os.PathLike[str],\n    options: dict[str, Any] | None = None,\n) -&gt; None:\n    \"\"\"\n    Writes a benchmark record to the given file path.\n\n    The file IO is chosen based on the extension found on the ``file`` path.\n\n    Parameters\n    ----------\n    record: BenchmarkRecord\n        The record to write to the database.\n    file: str | os.PathLike[str]\n        The file name, or object, to write to.\n    options: dict[str, Any] | None\n        Options to pass to the respective file IO implementation.\n\n    Raises\n    ------\n    ValueError\n        If the extension of the given file is not supported.\n    \"\"\"\n    file_io = get_file_io_class(file)\n    file_io.write(record, file, options or {})\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.make_file_descriptor","title":"make_file_descriptor","text":"<pre><code>make_file_descriptor(\n    file: str | PathLike[str] | IO,\n    mode: Literal[\"r\", \"w\", \"a\", \"x\", \"rb\", \"wb\", \"ab\", \"xb\"],\n    **open_kwargs: Any\n) -&gt; IO\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def make_file_descriptor(\n    file: str | os.PathLike[str] | IO,\n    mode: Literal[\"r\", \"w\", \"a\", \"x\", \"rb\", \"wb\", \"ab\", \"xb\"],\n    **open_kwargs: Any,\n) -&gt; IO:\n    if hasattr(file, \"read\") or hasattr(file, \"write\"):\n        return cast(IO, file)\n    elif isinstance(file, str | os.PathLike):\n        protocol = get_protocol(file)\n        fd: IO\n        if protocol == \"file\":\n            fd = open(file, mode, **open_kwargs)\n        else:\n            try:\n                import fsspec\n            except ImportError:\n                raise RuntimeError(\"non-local URIs require the fsspec package\")\n            fs = fsspec.filesystem(protocol)\n            fd = fs.open(file, mode, **open_kwargs)\n        return fd\n    raise TypeError(\"filename must be a str, bytes, file or PathLike object\")\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.get_file_io_class","title":"get_file_io_class","text":"<pre><code>get_file_io_class(file: str | PathLike[str]) -&gt; BenchmarkFileIO\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def get_file_io_class(file: str | os.PathLike[str]) -&gt; BenchmarkFileIO:\n    ext = get_extension(file)\n    try:\n        return _file_io_mapping[ext]()\n    except KeyError:\n        raise ValueError(f\"unsupported benchmark file format {ext!r}\") from None\n</code></pre>"},{"location":"reference/nnbench/reporter/file/#nnbench.reporter.file.register_file_io_class","title":"register_file_io_class","text":"<pre><code>register_file_io_class(name: str, klass: type[BenchmarkFileIO], clobber: bool = False) -&gt; None\n</code></pre> Source code in <code>src/nnbench/reporter/file.py</code> <pre><code>def register_file_io_class(name: str, klass: type[BenchmarkFileIO], clobber: bool = False) -&gt; None:\n    if name in _file_io_mapping and not clobber:\n        raise RuntimeError(\n            f\"driver {name!r} is already registered \"\n            f\"(to force registration, rerun with clobber=True)\"\n        )\n    _file_io_mapping[name] = klass\n</code></pre>"},{"location":"reference/nnbench/reporter/service/","title":"service","text":""},{"location":"reference/nnbench/reporter/service/#nnbench.reporter.service.BenchmarkServiceIO","title":"BenchmarkServiceIO","text":"<p>               Bases: <code>Protocol</code></p> Source code in <code>src/nnbench/reporter/service.py</code> <pre><code>class BenchmarkServiceIO(Protocol):\n    def read(self, uri: str | os.PathLike[str], options: dict[str, Any]) -&gt; BenchmarkRecord: ...\n\n    def write(\n        self, record: BenchmarkRecord, uri: str | os.PathLike[str], options: dict[str, Any]\n    ) -&gt; None: ...\n</code></pre>"},{"location":"reference/nnbench/reporter/service/#nnbench.reporter.service.BenchmarkServiceIO.read","title":"read","text":"<pre><code>read(uri: str | PathLike[str], options: dict[str, Any]) -&gt; BenchmarkRecord\n</code></pre> Source code in <code>src/nnbench/reporter/service.py</code> <pre><code>def read(self, uri: str | os.PathLike[str], options: dict[str, Any]) -&gt; BenchmarkRecord: ...\n</code></pre>"},{"location":"reference/nnbench/reporter/service/#nnbench.reporter.service.BenchmarkServiceIO.write","title":"write","text":"<pre><code>write(record: BenchmarkRecord, uri: str | PathLike[str], options: dict[str, Any]) -&gt; None\n</code></pre> Source code in <code>src/nnbench/reporter/service.py</code> <pre><code>def write(\n    self, record: BenchmarkRecord, uri: str | os.PathLike[str], options: dict[str, Any]\n) -&gt; None: ...\n</code></pre>"},{"location":"reference/nnbench/reporter/service/#nnbench.reporter.service.MLFlowIO","title":"MLFlowIO","text":"<p>               Bases: <code>BenchmarkServiceIO</code></p> Source code in <code>src/nnbench/reporter/service.py</code> <pre><code>class MLFlowIO(BenchmarkServiceIO):\n    def __init__(self):\n        import mlflow\n\n        self.mlflow = mlflow\n        self.stack = ExitStack()\n\n    @staticmethod\n    def strip_protocol(uri: str | os.PathLike[str]) -&gt; str:\n        s = str(uri)\n        if s.startswith(\"mlflow://\"):\n            return s[9:]\n        return s\n\n    def get_or_create_run(self, run_name: str, nested: bool = False) -&gt; \"ActiveRun\":\n        existing_runs = self.mlflow.search_runs(\n            filter_string=f\"attributes.`run_name`={run_name!r}\", output_format=\"list\"\n        )\n        if existing_runs:\n            run_id = existing_runs[0].info.run_id\n            return self.mlflow.start_run(run_id=run_id, nested=nested)\n        else:\n            return self.mlflow.start_run(run_name=run_name, nested=nested)\n\n    def read(self, uri: str | os.PathLike[str], options: dict[str, Any]) -&gt; BenchmarkRecord:\n        raise NotImplementedError\n\n    def write(\n        self, record: BenchmarkRecord, uri: str | os.PathLike[str], options: dict[str, Any]\n    ) -&gt; None:\n        uri = self.strip_protocol(uri)\n        try:\n            experiment, run_name, *subruns = Path(uri).parts\n        except ValueError:\n            raise ValueError(f\"expected URI of form &lt;experiment&gt;/&lt;run&gt;[/&lt;subrun&gt;]..., got {uri!r}\")\n\n        # setting experiment removes the need for passing the `experiment_id` kwarg\n        # in the subsequent API calls.\n        self.mlflow.set_experiment(experiment_name=experiment)\n\n        run = self.stack.enter_context(self.get_or_create_run(run_name=run_name))\n        for s in subruns:\n            # reassignment ensures that we log into the max-depth subrun specified.\n            run = self.stack.enter_context(self.get_or_create_run(run_name=s, nested=True))\n\n        self.mlflow.log_dict(record.context, \"context.json\", run_id=run.info.run_id)\n        for bm in record.benchmarks:\n            name, value = bm[\"name\"], bm[\"value\"]\n            timestamp = int(datetime.fromisoformat(bm[\"date\"]).timestamp())\n            self.mlflow.log_metric(name, value, timestamp=timestamp, run_id=run.info.run_id)\n</code></pre>"},{"location":"reference/nnbench/reporter/service/#nnbench.reporter.service.MLFlowIO.mlflow","title":"mlflow  <code>instance-attribute</code>","text":"<pre><code>mlflow = mlflow\n</code></pre>"},{"location":"reference/nnbench/reporter/service/#nnbench.reporter.service.MLFlowIO.stack","title":"stack  <code>instance-attribute</code>","text":"<pre><code>stack = ExitStack()\n</code></pre>"},{"location":"reference/nnbench/reporter/service/#nnbench.reporter.service.MLFlowIO.strip_protocol","title":"strip_protocol  <code>staticmethod</code>","text":"<pre><code>strip_protocol(uri: str | PathLike[str]) -&gt; str\n</code></pre> Source code in <code>src/nnbench/reporter/service.py</code> <pre><code>@staticmethod\ndef strip_protocol(uri: str | os.PathLike[str]) -&gt; str:\n    s = str(uri)\n    if s.startswith(\"mlflow://\"):\n        return s[9:]\n    return s\n</code></pre>"},{"location":"reference/nnbench/reporter/service/#nnbench.reporter.service.MLFlowIO.get_or_create_run","title":"get_or_create_run","text":"<pre><code>get_or_create_run(run_name: str, nested: bool = False) -&gt; ActiveRun\n</code></pre> Source code in <code>src/nnbench/reporter/service.py</code> <pre><code>def get_or_create_run(self, run_name: str, nested: bool = False) -&gt; \"ActiveRun\":\n    existing_runs = self.mlflow.search_runs(\n        filter_string=f\"attributes.`run_name`={run_name!r}\", output_format=\"list\"\n    )\n    if existing_runs:\n        run_id = existing_runs[0].info.run_id\n        return self.mlflow.start_run(run_id=run_id, nested=nested)\n    else:\n        return self.mlflow.start_run(run_name=run_name, nested=nested)\n</code></pre>"},{"location":"reference/nnbench/reporter/service/#nnbench.reporter.service.MLFlowIO.read","title":"read","text":"<pre><code>read(uri: str | PathLike[str], options: dict[str, Any]) -&gt; BenchmarkRecord\n</code></pre> Source code in <code>src/nnbench/reporter/service.py</code> <pre><code>def read(self, uri: str | os.PathLike[str], options: dict[str, Any]) -&gt; BenchmarkRecord:\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/nnbench/reporter/service/#nnbench.reporter.service.MLFlowIO.write","title":"write","text":"<pre><code>write(record: BenchmarkRecord, uri: str | PathLike[str], options: dict[str, Any]) -&gt; None\n</code></pre> Source code in <code>src/nnbench/reporter/service.py</code> <pre><code>def write(\n    self, record: BenchmarkRecord, uri: str | os.PathLike[str], options: dict[str, Any]\n) -&gt; None:\n    uri = self.strip_protocol(uri)\n    try:\n        experiment, run_name, *subruns = Path(uri).parts\n    except ValueError:\n        raise ValueError(f\"expected URI of form &lt;experiment&gt;/&lt;run&gt;[/&lt;subrun&gt;]..., got {uri!r}\")\n\n    # setting experiment removes the need for passing the `experiment_id` kwarg\n    # in the subsequent API calls.\n    self.mlflow.set_experiment(experiment_name=experiment)\n\n    run = self.stack.enter_context(self.get_or_create_run(run_name=run_name))\n    for s in subruns:\n        # reassignment ensures that we log into the max-depth subrun specified.\n        run = self.stack.enter_context(self.get_or_create_run(run_name=s, nested=True))\n\n    self.mlflow.log_dict(record.context, \"context.json\", run_id=run.info.run_id)\n    for bm in record.benchmarks:\n        name, value = bm[\"name\"], bm[\"value\"]\n        timestamp = int(datetime.fromisoformat(bm[\"date\"]).timestamp())\n        self.mlflow.log_metric(name, value, timestamp=timestamp, run_id=run.info.run_id)\n</code></pre>"},{"location":"reference/nnbench/reporter/service/#nnbench.reporter.service.get_service_io_class","title":"get_service_io_class","text":"<pre><code>get_service_io_class(uri: str) -&gt; BenchmarkServiceIO\n</code></pre> Source code in <code>src/nnbench/reporter/service.py</code> <pre><code>def get_service_io_class(uri: str) -&gt; BenchmarkServiceIO:\n    proto = get_protocol(uri)\n    try:\n        return _service_io_mapping[proto]()\n    except KeyError:\n        raise ValueError(f\"unsupported benchmark IO: {proto!r}\") from None\n</code></pre>"},{"location":"reference/nnbench/reporter/service/#nnbench.reporter.service.register_file_io_class","title":"register_file_io_class","text":"<pre><code>register_file_io_class(name: str, klass: type[BenchmarkServiceIO], clobber: bool = False) -&gt; None\n</code></pre> Source code in <code>src/nnbench/reporter/service.py</code> <pre><code>def register_file_io_class(\n    name: str, klass: type[BenchmarkServiceIO], clobber: bool = False\n) -&gt; None:\n    if name in _service_io_mapping and not clobber:\n        raise RuntimeError(\n            f\"IO {name!r} is already registered (to force registration, rerun with clobber=True)\"\n        )\n    _service_io_mapping[name] = klass\n</code></pre>"},{"location":"reference/nnbench/reporter/util/","title":"util","text":""},{"location":"reference/nnbench/reporter/util/#nnbench.reporter.util.nullcols","title":"nullcols","text":"<pre><code>nullcols(_benchmarks: list[dict[str, Any]]) -&gt; set[str]\n</code></pre> <p>Extracts columns that only contain false-ish data from a list of benchmarks.</p> <p>Since this data is most often not interesting, the result of this can be used to filter out these columns from the benchmark dictionaries.</p> PARAMETER DESCRIPTION <code>_benchmarks</code> <p>The benchmarks to filter.</p> <p> TYPE: <code>list[dict[str, Any]]</code> </p> RETURNS DESCRIPTION <code>set[str]</code> <p>Set of the columns (key names) that only contain false-ish values across all benchmarks.</p> Source code in <code>src/nnbench/reporter/util.py</code> <pre><code>def nullcols(_benchmarks: list[dict[str, Any]]) -&gt; set[str]:\n    \"\"\"\n    Extracts columns that only contain false-ish data from a list of benchmarks.\n\n    Since this data is most often not interesting, the result of this\n    can be used to filter out these columns from the benchmark dictionaries.\n\n    Parameters\n    ----------\n    _benchmarks: list[dict[str, Any]]\n        The benchmarks to filter.\n\n    Returns\n    -------\n    set[str]\n        Set of the columns (key names) that only contain false-ish values\n        across all benchmarks.\n    \"\"\"\n    nulls: dict[str, bool] = collections.defaultdict(bool)\n    for bm in _benchmarks:\n        for k, v in bm.items():\n            nulls[k] = nulls[k] or bool(v)\n    return set(k for k, v in nulls.items() if not v)\n</code></pre>"},{"location":"reference/nnbench/reporter/util/#nnbench.reporter.util.get_protocol","title":"get_protocol","text":"<pre><code>get_protocol(url: str | PathLike[str]) -&gt; str\n</code></pre> Source code in <code>src/nnbench/reporter/util.py</code> <pre><code>def get_protocol(url: str | os.PathLike[str]) -&gt; str:\n    url = str(url)\n    parts = re.split(r\"(::|://)\", url, maxsplit=1)\n    if len(parts) &gt; 1:\n        return parts[0]\n    return \"file\"\n</code></pre>"},{"location":"reference/nnbench/reporter/util/#nnbench.reporter.util.get_extension","title":"get_extension","text":"<pre><code>get_extension(f: str | PathLike[str] | IO) -&gt; str\n</code></pre> <p>Given a path or file-like object, returns file extension (can be the empty string, if the file has no extension).</p> Source code in <code>src/nnbench/reporter/util.py</code> <pre><code>def get_extension(f: str | os.PathLike[str] | IO) -&gt; str:\n    \"\"\"\n    Given a path or file-like object, returns file extension\n    (can be the empty string, if the file has no extension).\n    \"\"\"\n    if isinstance(f, str | os.PathLike):\n        return Path(f).suffix\n    else:\n        return Path(f.name).suffix\n</code></pre>"},{"location":"tutorials/","title":"Examples","text":"<p>This page showcases some examples of applications for nnbench. Click any of the links below for inspiration on how to use nnbench in your projects.</p> <ul> <li>Integrating nnbench into an existing ML pipeline</li> <li>Integrating nnbench with workflow orchestrators</li> <li>Using a streamlit web app to dispatch benchmarks</li> <li>Analyzing benchmark results at scale with duckDB</li> <li>Streaming benchmark results to a cloud database (Google BigQuery)</li> <li>How to benchmark pre-trained HuggingFace models</li> </ul>"},{"location":"tutorials/bq/","title":"Streaming benchmarks to a cloud database","text":"<p>Once you obtain the results of your benchmarks, you will most likely want to store them somewhere. Whether that is in storage as flat files, on a server, or in a database, <code>nnbench</code> allows you to write records anywhere, provided the destination supports JSON.</p> <p>This is a small guide containing a snippet on how to stream benchmark results to a Google Cloud BigQuery table.</p>"},{"location":"tutorials/bq/#the-benchmarks","title":"The benchmarks","text":"<p>Configure your benchmarks as normal, for example by separating them into a Python file. The following is a very simple example benchmark setup.</p> <pre><code>import nnbench\n\n\n@nnbench.benchmark\ndef prod(a: int, b: int) -&gt; int:\n    return a * b\n\n\n@nnbench.benchmark\ndef sum(a: int, b: int) -&gt; int:\n    return a + b\n</code></pre>"},{"location":"tutorials/bq/#setting-up-a-bigquery-client","title":"Setting up a BigQuery client","text":"<p>In order to authenticate with BigQuery, follow the official Google Cloud documentation. In this case, we rely on Application Default Credentials (ADC), which can be configured with the <code>gcloud</code> CLI.</p> <p>To interact with BigQuery from Python, the <code>google-cloud-bigquery</code> package has to be installed. You can do this e.g. using pip via <code>pip install --upgrade google-cloud-bigquery</code>.</p>"},{"location":"tutorials/bq/#creating-a-table","title":"Creating a table","text":"<p>Within your configured project, proceed by creating a destination table to write the benchmarks to. Consider the BigQuery Python documentation on tables for how to create a table programmatically.</p> <p>Note</p> <p>If the configured dataset does not exist, you will have to create it as well, either programmatically via the <code>bigquery.Client.create_dataset</code> API or in the Google Cloud console.</p>"},{"location":"tutorials/bq/#using-bigquerys-schema-auto-detection","title":"Using BigQuery's schema auto-detection","text":"<p>In order to skip tedious schema inference by hand, we can use BigQuery's schema auto-detection from JSON records. All we have to do is configure a BigQuery load job to auto-detect the schema from the Python dictionaries in memory:</p> <pre><code>    job_config = bigquery.LoadJobConfig(\n        autodetect=True, source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n    )\n</code></pre> <p>After that, write and stream the compacted benchmark record directly to your destination table. In this example, we decide to flatten the benchmark context to be able to extract scalar context values directly from the result table using raw SQL queries. Note that you have to use a custom separator (an underscore <code>\"_\"</code> in this case) for the context data, since BigQuery does not allow dots in column names.</p> <pre><code>    load_job.result()\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Tip</p> <p>If you would like to save the context dictionary as a struct instead, use <code>mode = \"inline\"</code> in the call to <code>BenchmarkRecord.compact()</code>.</p> <p>And that's all! To check that the records appear as expected, you can now query the data e.g. like so:</p> <pre><code># check that the insert worked.\nquery = f'SELECT name, value, time_ns, git_commit AS commit FROM {table_id}'\nr = client.query(query)\nfor row in r.result():\n    print(r)\n</code></pre>"},{"location":"tutorials/bq/#recap-and-the-full-source-code","title":"Recap and the full source code","text":"<p>In this tutorial, we</p> <p>1) defined and ran a benchmark workload using <code>nnbench</code>. 2) configured a Google Cloud BigQuery client and a load job to insert benchmark records into a table, and 3) inserted the records into the destination table.</p> <p>The full source code for this tutorial is included below, and also in the nnbench repository.</p> <pre><code>from google.cloud import bigquery\n\nimport nnbench\nfrom nnbench.context import GitEnvironmentInfo\n\n\ndef main():\n    client = bigquery.Client()\n\n    # TODO: Fill these out with your appropriate resource names.\n    table_id = \"&lt;PROJECT&gt;.&lt;DATASET&gt;.&lt;TABLE&gt;\"\n\n    job_config = bigquery.LoadJobConfig(\n        autodetect=True, source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n    )\n\n    benchmarks = nnbench.collect(\"benchmarks.py\")\n    res = nnbench.run(benchmarks, params={\"a\": 1, \"b\": 1}, context=(GitEnvironmentInfo(),))\n\n    load_job = client.load_table_from_json(res.to_json(), table_id, job_config=job_config)\n    load_job.result()\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"tutorials/duckdb/","title":"Querying benchmark results at scale with duckDB","text":"<p>For a powerful way to query, filter, and visualize benchmark records, duckdb is a great choice. This page contains a quick tutorial for analyzing benchmark results with duckDB.</p>"},{"location":"tutorials/duckdb/#prerequisites-and-installation","title":"Prerequisites and installation","text":"<p>To use duckdb, install the duckdb Python package by running <code>pip install --upgrade duckdb</code>. In this tutorial, we are going to be using the in-memory database only, but you can easily persist SQL views of records on disk as well.</p>"},{"location":"tutorials/duckdb/#writing-and-ingesting-benchmark-records","title":"Writing and ingesting benchmark records","text":"<p>We consider the following easy benchmark example:</p> <pre><code>import nnbench\n\n\n@nnbench.benchmark\ndef prod(a: int, b: int) -&gt; int:\n    return a * b\n\n\n@nnbench.benchmark\ndef sum(a: int, b: int) -&gt; int:\n    return a + b\n</code></pre> <p>Running both of these benchmarks produces a benchmark record, which we can save to disk using the <code>FileIO</code> class.</p> <pre><code>import nnbench\nfrom nnbench.context import GitEnvironmentInfo\nfrom nnbench.reporter.file import FileReporter\n\nbenchmarks = nnbench.collect(\"benchmarks.py\")\nrecord = nnbench.run(benchmarks, params={\"a\": 1, \"b\": 1}, context=(GitEnvironmentInfo(),))\n\nfile_reporter = FileReporter()\nfile_reporter.write(record, \"record.json\", driver=\"ndjson\")\n</code></pre> <p>This writes a newline-delimited JSON file as <code>record.json</code> into the current directory. We choose this format because it is ideal for duckdb to work with.</p> <p>Now, we can easily ingest the record into a duckDB database:</p> <pre><code>import duckdb\n\nduckdb.sql(\n    \"\"\"\n    SELECT name, value FROM read_ndjson_auto('record.json')\n    \"\"\"\n).show()\n\n# ----- prints: -----\n# \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n# \u2502  name   \u2502 value \u2502\n# \u2502 varchar \u2502 int64 \u2502\n# \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n# \u2502 prod    \u2502     1 \u2502\n# \u2502 sum     \u2502     2 \u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"tutorials/duckdb/#querying-metadata-directly-in-sql-by-flattening-the-context-struct","title":"Querying metadata directly in SQL by flattening the context struct","text":"<p>By default, the benchmark context struct, which holds additional information about the benchmark runs, is inlined into the raw dictionary before saving it to a file. This is not ideal for some SQL implementations, where you might not be able to filter records easily by interpreting the serialized <code>context</code> struct.</p> <p>To improve, you can pass <code>ctxmode=\"flatten\"</code> to the <code>FileIO.write()</code> method to flatten the context and inline all nested values instead. This comes at the expense of an inflated schema, i.e. more columns in the database.</p> <pre><code>fio = FileIO()\nfio.write(record, \"record.json\", driver=\"ndjson\", ctxmode=\"flatten\")\n</code></pre> <p>In the example above, we used the <code>GitEnvironmentInfo</code> context provider to log some information on the git environment we ran our benchmarks in. In flat mode, this includes the <code>git.commit</code> and <code>git.repository</code> values, telling us at which commit and in which repository the benchmarks were run, respectively.</p> <p>To log this information in a duckDB view, we run the following on a flat-context NDJSON record:</p> <pre><code>duckdb.sql(\n    \"\"\"\n    SELECT name, value, \\\"git.commit\\\", \\\"git.repository\\\" FROM read_ndjson_auto('record.json')\n    \"\"\"\n).show()\n\n#   ---------------------------------- prints ------------------------------------------\n# \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n# \u2502  name   \u2502 value \u2502                git.commit                \u2502    git.repository     \u2502\n# \u2502 varchar \u2502 int64 \u2502                 varchar                  \u2502        varchar        \u2502\n# \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n# \u2502 prod    \u2502     1 \u2502 0d47d7bcd2d2c13b69796355fe9d4ef5f50b1edb \u2502 aai-institute/nnbench \u2502\n# \u2502 sum     \u2502     2 \u2502 0d47d7bcd2d2c13b69796355fe9d4ef5f50b1edb \u2502 aai-institute/nnbench \u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>This method is great to select a subset of context values when the full context metadata structure is not required.</p> <p>Tip</p> <p>In duckDB specifically, this is equivalent to dotted access of the \"context\" column if <code>ctxmode=\"inline\"</code>. This means that the following also works to obtain the git commit mentioned above: <pre><code>duckdb.sql(\n    \"\"\"\n    SELECT name, value, context.git.commit AS \\\"git.commit\\\" FROM read_ndjson_auto('record.json')\n    \"\"\"\n).show()\n</code></pre></p>"},{"location":"tutorials/huggingface/","title":"Benchmarking HuggingFace models on a dataset","text":"<p>There is a high likelihood that you, at some point, find yourself wanting to benchmark previously trained models. This guide shows you how to do it for a HuggingFace model with nnbench.</p>"},{"location":"tutorials/huggingface/#example-named-entity-recognition","title":"Example: Named Entity Recognition","text":"<p>We start with a small tangent about the example setup that we will use in this guide. If you are only interested in the application of nnbench, you can skip this section.</p> <p>There are lots of reasons why you could want to retrieve saved models for benchmarking.  Among them these are reviewing the work of colleagues, comparing model performance to an existing benchmark, or dealing with models that require significant compute such that in-place retraining is impractical.</p> <p>For this example, we look at a named entity recognition (NER) model that is based on the pre-trained encoder-decoder transformer BERT from HuggingFace. The model is trained on the CoNLLpp dataset which consists of sentences from news stories where words were tagged with Person, Organization, Location, or Miscellaneous if they referred to entities.  Words are assigned an out-of-entity label if they do not represent an entity.</p>"},{"location":"tutorials/huggingface/#model-training","title":"Model Training","text":"<p>You find the code to train the model in the nnbench repository. If you want to skip running the training script but still want to reproduce this example, you can take any BERT model fine tuned for NER with the CoNLL dataset family. You find many on the Huggingface model hub, for example this one. You need to download the <code>model.safetensors</code>, <code>config.json</code>, <code>tokenizer_config.json</code>, and <code>tokenizer.json</code> files. If you want to train your own model, continue below.</p> <p>There is some necessary preprocessing and data wrangling to train the model.  We will not go into the details here, but if you are interested in a more thorough walkthrough, look into this resource by Huggingface which served as the basis for this example.</p> <p>It is not feasible to train the model on a CPU. If you do not have access to a GPU, you can use free GPU instances on Google Colab. When opening a new Colab notebook, make sure to select a GPU instance in the upper right corner. Then, you can upload the <code>training.py</code>. You can ignore any warnings about the data not being persisted.</p> <p>Next, install the necessary dependencies: <code>!pip install datasets transformers[torch]</code>.  Google Colab comes with some dependencies already installed in the environment. Hence, if you are working with a different GPU instance, make sure to install everything from the <code>pyproject.toml</code> in the <code>examples/artifact_benchmarking</code> folder.</p> <p>Finally, you can execute the <code>training.py</code> with <code>!python training.py</code>. This will train two BERT models (\"bert-base-uncased\" and \"distilbert-base-uncased\") which we can compare using nnbench.  If you want, you can adapt the training script to train other models by editing the tuples in the <code>tokenizers_and_models</code> list at the bottom of the training script.  The training of the models takes around 10 minutes.</p> <p>Once it is done, download the respective files and save them to your disk. They should be the same mentioned above. We will need the paths to the files for benchmarking later.</p>"},{"location":"tutorials/huggingface/#the-benchmarks","title":"The benchmarks","text":"<p>The benchmarking code is found in the <code>examples/huggingface/benchmark.py</code>. We calculate precision, recall, accuracy, and f1 scores for the whole test set and specific labels. Additionally, we obtain information about the model such as its memory footprint and inference time.</p> <p>We are not walking through the whole file but instead point out certain design choices as an inspiration to you.  If you are interested in a more detailed walkthrough on how to set up benchmarks, you can find it here.</p> <p>Notable design choices in this benchmark are that we factored out the evaluation loop as it is necessary for all evaluation metrics. We cache it using the <code>functools.cache</code> decorator so the evaluation loop runs only once per benchmark run instead of once per metric which greatly reduces runtime.</p> <p>We also use <code>nnbench.parametrize</code> to get the per-class metrics. As the parametrization method needs the same arguments for each benchmark, we use Python's builtin <code>functools.partial</code> to fill the arguments.</p> <pre><code>parametrize_label = partial(\n    nnbench.parametrize,\n    [\"O\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"B-MISC\", \"I-MISC\"],\n    tags=(\"metric\", \"per-class\"),\n)()\n</code></pre> <p>After this, the benchmarking code is actually very simple, as in most of the other examples. You find it in the nnbench repository in <code>examples/huggingface/runner.py</code>.</p>"},{"location":"tutorials/mnist/","title":"Integrating nnbench into an existing ML pipeline","text":"<p>Thanks to nnbench's modularity, we can easily integrate it into existing ML experiment code.</p> <p>As an example, we use an MNIST pipeline written for the popular ML framework JAX. While the actual data sourcing and training code is interesting on its own, we focus solely on the nnbench application part. You can find the full example code in the nnbench repository.</p>"},{"location":"tutorials/mnist/#defining-and-organizing-benchmarks","title":"Defining and organizing benchmarks","text":"<p>To properly structure our project, we avoid mixing training pipeline code and benchmark code by placing all benchmarks in a standalone file, similarly to how you might structure unit tests for your code.</p> <pre><code>import jax\nimport jax.numpy as jnp\nfrom mnist import ArrayMapping, ConvNet\n\nimport nnbench\n\n\n@nnbench.benchmark\ndef accuracy(params: ArrayMapping, data: ArrayMapping) -&gt; float:\n    x_test, y_test = data[\"x_test\"], data[\"y_test\"]\n\n    cn = ConvNet()\n    y_pred = cn.apply({\"params\": params}, x_test)\n    return jnp.mean(jnp.argmax(y_pred, -1) == y_test).item()\n\n\n@nnbench.benchmark(name=\"Model size (MB)\")\ndef modelsize(params: ArrayMapping) -&gt; float:\n    nbytes = sum(x.size * x.dtype.itemsize for x in jax.tree_util.tree_leaves(params))\n    return nbytes / 1e6\n</code></pre> <p>This definition is short and sweet, and contains a few important details:</p> <ul> <li>Both functions are given the <code>@nnbench.benchmark</code> decorator - this allows us to find and collect them before starting the benchmark run.</li> <li>The <code>modelsize</code> benchmark is given a custom name (<code>\"Model size (MB)\"</code>), indicating that the resulting number is the combined size of the model weights in megabytes. This is done for display purposes, to improve interpretability when reporting results.</li> <li>The <code>params</code> argument is the same in both benchmarks, both in name and type. This is important, since it ensures that both benchmarks will be run with the same model weights.</li> </ul> <p>That's all - now we can shift over to our main pipeline code and see what is necessary to execute the benchmarks and visualize the results.</p>"},{"location":"tutorials/mnist/#setting-up-a-benchmark-run-and-parameters","title":"Setting up a benchmark run and parameters","text":"<p>After finishing the benchmark setup, we only need a few more lines to augment our pipeline.</p> <p>We assume that the benchmark file is located in the same folder as the training pipeline - thus, we can specify our parent directory as the place in which to search for benchmarks:</p> <pre><code>HERE = Path(__file__).parent\n</code></pre> <p>Next, we can define a custom subclass of <code>nnbench.Parameters</code> to hold our benchmark parameters. Benchmark parameters are a set of variables used as inputs to the benchmark functions collected during the benchmark run.</p> <p>Since our benchmarks above are parametrized by the model weights (named <code>params</code> in the function signatures) and the MNIST data split (called <code>data</code>), we define our parameters to take exactly these two values.</p> <pre><code>@dataclass(frozen=True)\nclass MNISTTestParameters(nnbench.Parameters):\n    params: Mapping[str, jax.Array]\n    data: ArrayMapping\n</code></pre> <p>And that's it! After we implement all training code, we just run nnbench directly after training in our top-level pipeline function:</p> <pre><code>def mnist_jax():\n    \"\"\"Load MNIST data and train a simple ConvNet model.\"\"\"\n    mnist = load_mnist()\n    mnist = preprocess(mnist)\n    state, data = train(mnist)\n\n    # the nnbench portion.\n    benchmarks = nnbench.collect(HERE)\n    reporter = FileReporter()\n    params = MNISTTestParameters(params=state.params, data=data)\n    result = nnbench.run(benchmarks, params=params)\n</code></pre> <p>We use the <code>BenchmarkReporter</code> to print the results directly to the terminal in a table. Notice how by we can reuse the training artifacts in nnbench as parameters to obtain results right after training!</p> <p>The output might look like this:</p> <pre><code>name               value\n---------------  -------\naccuracy         0.9712\nModel size (MB)  3.29783\n</code></pre> <p>This can be improved in a number of ways - for example by enriching it with metadata about the model architecture, the used GPU, etc. For more information on how to supply context to benchmarks, check the user guide section.</p>"},{"location":"tutorials/prefect/","title":"Integrating nnbench with Prefect","text":"<p>If you have more complex workflows it is sensible to use a workflow orchestration tool to manage them.  Benchmarking with nnbench can be integrated with orchestrators. We will present an example integration with Prefect. We will explain the orchestration concepts in a high level and link to the corresponding parts of the  Prefect docs. The full example code can be found in the nnbench repository.</p> <p>In this example we want to orchestrate the training and benchmarking of a linear regression model.</p>"},{"location":"tutorials/prefect/#project-structure","title":"Project Structure","text":""},{"location":"tutorials/prefect/#defining-the-training-tasks-and-workflows","title":"Defining the training tasks and workflows","text":"<p>We recommend to separate the training and benchmarking logic. </p> <pre><code>import numpy as np\nfrom prefect import flow, task\nfrom sklearn import base\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n\n@task\ndef make_regression_data(\n    random_state: int, n_samples: int = 100, n_features: int = 1, noise: float = 0.2\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    X, y = make_regression(\n        n_samples=n_samples, n_features=n_features, noise=noise, random_state=random_state\n    )\n    return X, y\n\n\n@task\ndef make_train_test_split(\n    X: np.ndarray, y: np.ndarray, random_state: int, test_size: float = 0.2\n) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=test_size, random_state=random_state\n    )\n    return X_train, y_train, X_test, y_test\n\n\n@task\ndef train_linear_regression(X: np.ndarray, y: np.ndarray) -&gt; base.BaseEstimator:\n    model = LinearRegression()\n    model.fit(X, y)\n    return model\n\n\n@flow\ndef prepare_regression_data(\n    random_state: int = 42, n_samples: int = 100, n_features: int = 1, noise: float = 0.2\n) -&gt; tuple[np.ndarray, ...]:\n    X, y = make_regression_data(\n        random_state=random_state, n_samples=n_samples, n_features=n_features, noise=noise\n    )\n    X_train, y_train, X_test, y_test = make_train_test_split(X=X, y=y, random_state=random_state)\n    return X_train, y_train, X_test, y_test\n\n\n@flow\nasync def prepare_regressor_and_test_data(\n    data_params: dict[str, int | float] | None = None,\n) -&gt; tuple[base.BaseEstimator, np.ndarray, np.ndarray]:\n    if data_params is None:\n        data_params = {}\n    X_train, y_train, X_test, y_test = prepare_regression_data(**data_params)\n    model = train_linear_regression(X=X_train, y=y_train)\n    return model, X_test, y_test\n</code></pre> <p>The <code>training.py</code> file contains functions to generate synthetic data for our regression model, facilitate a train-test-split, and finally train the regression model. We have applied Prefect's <code>@task</code> decorator.. which marks the contained logic as a discrete unit of work for Prefect.  Two other functions prepare the regression data and train the estimator.  They are labeled with the <code>@flow</code> decorator. that labels the function as a workflow that can depend on other flows or tasks. The <code>prepare_regressor_and_test_data</code> function returns the model and test data so that we can use it in our benchmarks.</p>"},{"location":"tutorials/prefect/#defining-benchmarks","title":"Defining Benchmarks","text":"<p>The benchmarks are in the <code>benchmark.py</code> file. We have two functions to calculate the mean absolute error and the mean squared error. These benchmarks are tagged to indicate they are metrics. Another two benchmarks calculate calculate information about the model, namely the inference time and size of the model. The last two functions serve to investigate the test dataset.</p> <pre><code>import pickle\nimport sys\nimport time\n\nimport numpy as np\nfrom sklearn import base, metrics\n\nimport nnbench\n\n\n@nnbench.benchmark(tags=(\"metric\",))\ndef mae(model: base.BaseEstimator, X_test: np.ndarray, y_test: np.ndarray) -&gt; float:\n    y_pred = model.predict(X_test)\n    return metrics.mean_absolute_error(y_true=y_test, y_pred=y_pred)\n\n\n@nnbench.benchmark(tags=(\"metric\",))\ndef mse(model: base.BaseEstimator, X_test: np.ndarray, y_test: np.ndarray) -&gt; float:\n    y_pred = model.predict(X_test)\n    return metrics.mean_squared_error(y_true=y_test, y_pred=y_pred)\n\n\n@nnbench.benchmark(name=\"Model size (bytes)\", tags=(\"model-meta\",))\ndef modelsize(model: base.BaseEstimator) -&gt; int:\n    model_bytes = pickle.dumps(model)\n    return sys.getsizeof(model_bytes)\n\n\n@nnbench.benchmark(name=\"Inference time (s)\", tags=(\"model-meta\",))\ndef inference_time(model: base.BaseEstimator, X: np.ndarray, n_iter: int = 100) -&gt; float:\n    start = time.perf_counter()\n    for i in range(n_iter):\n        _ = model.predict(X)\n    end = time.perf_counter()\n    return (end - start) / n_iter\n</code></pre> <p>We did not apply any Prefect decorators here, as we will assign <code>@task</code>s - Prefects smallest unit of work - to run a benchmark family.</p>"},{"location":"tutorials/prefect/#defining-benchmark-runners","title":"Defining Benchmark runners.","text":"<p>In the <code>runners.py</code> file, we define the logic to run our benchmarks. The runner collects the benchmarks from the specified file.  We can filter by tags and use this to define two separate tasks, one to run the metrics and the other to run the metadata benchmarks. We have applied the <code>@task</code> decorator to these functions.</p> <pre><code>@task\ndef run_metric_benchmarks(\n    model: base.BaseEstimator, X_test: np.ndarray, y_test: np.ndarray\n) -&gt; nnbench.BenchmarkRecord:\n    benchmarks = nnbench.collect(os.path.join(dir_path, \"benchmark.py\"), tags=(\"metric\",))\n    results = nnbench.run(\n        benchmarks,\n        params={\"model\": model, \"X_test\": X_test, \"y_test\": y_test},\n    )\n    return results\n\n\n@task\ndef run_metadata_benchmarks(model: base.BaseEstimator, X: np.ndarray) -&gt; nnbench.BenchmarkRecord:\n    benchmarks = nnbench.collect(os.path.join(dir_path, \"benchmark.py\"), tags=(\"model-meta\",))\n    result = nnbench.run(\n        benchmarks,\n        params={\"model\": model, \"X\": X},\n    )\n    return result\n\n\n@flow(persist_result=True)\n</code></pre> <p>We have also defined a basic reporter that we will use to save the benchmark results with Prefect's artifact storage machinery.</p> <p><pre><code>        self.logger = get_run_logger()\n\n    async def write(\n        self,\n        record: nnbench.BenchmarkRecord,\n        key: str,\n        description: str = \"Benchmark and Context\",\n    ) -&gt; None:\n        await create_table_artifact(\n            key=key,\n            table=record.to_json(),\n            description=description,\n</code></pre> In a real-world scenario, we would report to a database and use a dedicated frontend to look at the benchmark results. But logging will suffice as we are only discussing integration with orchestrators here.</p> <p>A final compound flow executes the model training, obtains the test set and supplies it to the benchmarks we defined earlier.</p> <pre><code>) -&gt; tuple[nnbench.BenchmarkRecord, ...]:\n    if data_params is None:\n        data_params = {}\n\n    reporter = PrefectReporter()\n\n    regressor_and_test_data: tuple[\n        base.BaseEstimator, np.ndarray, np.ndarray\n    ] = await training.prepare_regressor_and_test_data(data_params=data_params)  # type: ignore\n\n    model = regressor_and_test_data[0]\n    X_test = regressor_and_test_data[1]\n    y_test = regressor_and_test_data[2]\n\n    metadata_results: nnbench.BenchmarkRecord = run_metadata_benchmarks(model=model, X=X_test)\n\n    metadata_results.context.update(data_params)\n    metadata_results.context.update(context.PythonInfo()())\n\n    await reporter.write(\n        record=metadata_results, key=\"model-attributes\", description=\"Model Attributes\"\n    )\n\n    metric_results: nnbench.BenchmarkRecord = run_metric_benchmarks(\n        model=model, X_test=X_test, y_test=y_test\n    )\n\n    metric_results.context.update(data_params)\n    metric_results.context.update(context.PythonInfo()())\n    await reporter.write(metric_results, key=\"model-performance\", description=\"Model Performance\")\n    return metadata_results, metric_results\n\n\nif __name__ == \"__main__\":\n</code></pre> <p>The final lines in the <code>runner.py</code> serve the <code>train_and_benchmark</code> function to make it available to Prefect for execution.</p> <pre><code>\n</code></pre>"},{"location":"tutorials/prefect/#running-prefect","title":"Running Prefect","text":"<p>To run Prefect we have to do several things. First, we have to make sure it is installed. You can use <code>pip install -U prefect</code>. Then we have to run a Prefect server using <code>prefect server start</code>. We make our benchmark flows available by executing it, <code>python runner.py</code>. This enables us to now order an execution with the following command: <code>prefect deployment run 'train-and-benchmark/benchmark-runner'</code>. The command should also be displayed in the output of the <code>runner.py</code> execution.</p> <p>Now we can visit the local Prefect dashboard. By default it is on <code>localhost:4200</code>.  Here we see the executed tasks and workflows.</p> <p></p> <p>If we navigate to the \"Flow Runs\" tab we see more details of the flow runs.</p> <p></p> <p>In the \"Deployments\" tab you see all deployed flows. Currently, there is only our <code>train_and_benchmark</code> flow under the <code>benchmark-runner</code> name. We can trigger a custom execution of workflows in the menu behind the three dots.</p> <p></p> <p>You find the results of the benchmarks when visiting the \"Artifacts\" tab or by navigating to the \"Artifacts\" section of a specific flow execution.</p> <p>As you can see, the nnbench is easily integrated with workflow orchestrators by simply registering the execution of a benchmark runner as a task in the orchestrator.</p> <p>For more functionality of Prefect, you can check out their documentation. </p>"},{"location":"tutorials/streamlit/","title":"Streamlit","text":""},{"location":"tutorials/streamlit/#integrating-nnbench-with-streamlit-and-prefect","title":"Integrating nnbench with Streamlit and Prefect","text":"<p>In a project you may want to execute benchmarks or investigate their results with a dedicated frontend. There exist several frameworks that can help you setting up a frontend quickly. For example Streamlit, Gradio, Dash, or you could roll your own implementation using a backend framework such as Flask. In this guide we will use Streamlit and integrate it with the orchestration setup we've developed with Prefect. That guide is a prerequisite for this one.  The full example code can be found in the nnbench repository.</p>"},{"location":"tutorials/streamlit/#the-streamlit-ui","title":"The Streamlit UI","text":"<p>The Streamlit UI is launched by executing  <pre><code>streamlit run streamlit_example.py\n</code></pre> and initially looks like this: </p> <p>The user interface is assembled in the final part of <code>streamlit_example.py</code>.</p> <pre><code>\n</code></pre> <p>The user inputs are generated via the custom <code>setup_ui()</code> function which then processes the input values once the \"Run Benchmark\" button is clicked.</p> <pre><code>\n</code></pre> <p>We use a session state to keep track of all the benchmarks we ran in the current session which then are displayed within expander elements at the bottom.</p> <pre><code>\n</code></pre>"},{"location":"tutorials/streamlit/#integrating-prefect-with-streamlit","title":"Integrating Prefect with Streamlit","text":"<p>To integrate Streamlit with Prefect, we have to do some initial housekeeping. Namely, we specify the URL for the <code>PreFectClient</code> as well as the storage location of run artifacts where we retrieve the benchmark results from.</p> <pre><code>\n</code></pre> <p>In this example there is no direct integration of Streamlit with nnbench, but all interactions are passing through Prefect to make use of its orchestration benefits such as caching of tasks. Another thing to note is that we are working with local instances for easier reproducibility of this example. Adapting it to work with a remote orchestration server and object storage should be straightforward.</p> <p>The main interaction of the Streamlit frontend with Prefect takes place in the <code>run_bms</code> and <code>get_bm_artifacts</code> functions.</p> <p>The former searches for a Prefect deployment <code>\"train-and-benchmark/benchmark-runner\"</code> and executes it with the benchmark parameters specified by the user. It returns the <code>storage_key</code>, which we use to retrieve the persisted benchmark results.</p> <pre><code>\n</code></pre> <p>The <code>get_bm_artifacts</code> function gets a storage key and retrieves the corresponding results. As the results are stored in raw bytes, we have some logic to reconstruct the <code>nnbench.types.BenchmarkRecord</code> object. We transform the data into Pandas <code>DataFrame</code>s, which are later processed by Streamlit to display the results in tables.</p> <pre><code>\n</code></pre>"},{"location":"tutorials/streamlit/#running-the-example","title":"Running the example","text":"<p>To run the example, we have to do several things.  First, we need to start Prefect using <code>prefect server start</code> in the command line.  Next, we need to make the <code>\"train-and-benchmark/benchmark-runner\"</code> deployment available. We do so by running the corresponding Python file, <code>python runner.py</code>. You find that file in the <code>examples/prefect/src</code> directory.  If you are recreating this example on your machine, make sure you have the full contents of the <code>prefect</code> directory available in addition to the <code>streamlit_example.py</code>. For more information, you can look into the Prefect Guide.</p> <p>Now that Prefect is set up, you can launch a local instance of Streamlit with <code>streamlit run streamlit_example.py</code>.</p> <p>For more information on how to work with Streamlit, visit their docs.</p>"}]}